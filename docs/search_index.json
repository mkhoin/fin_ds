[
["index.html", "금융 데이터 사이언스 Welcome", " 금융 데이터 사이언스 이현열 2019-11-19 Welcome R을 이용한 금융 데이터 사이언스 과정 페이지 입니다. 현재 머신러닝 파트만 업로드 중입니다. "],
["머신러닝이란.html", "Chapter 1 머신러닝이란? 1.1 지도학습(Supervised Learning) 1.2 비지도학습(Unsupervised Learning) 1.3 딥러닝 / 강화학습(Reinforcement Learning) 1.4 사용 패키지", " Chapter 1 머신러닝이란? 머신러닝이란 데이터에서 패턴을 찾아 새로운 데이터의 결과값을 예측하는 방법이며 크게 지도학습, 비지도학습, 강화학습으로 나눌수 있습니다. 1.1 지도학습(Supervised Learning) 지도학습은 정답을 알려주며 학습시키는 것입니다. 예를들어 고양이 사진을 준 후(input data), 이 사진은 고양이(label data)라는 것를 알려준 후, 이러한 패턴을 바탕으로 새로운 데이터의 결과값을 예측하는 방식입니다. 따라서 기계가 정답을 잘 맞췄는지 아닌지 쉽게 알 수 있다. 지도학습에는 크게 분류(classification)과 회귀(regression)가 있습니다. 분류(classification): 두 가지 혹은 여러 값 중 하나로 분류하는 것입니다. 예를 들어 특정 이메일이 스팸인가 아닌가 혹은 개인의 신용등급을 분류하는 방식입니다. 회귀(regression): 어떤 데이터들의 특징(feature)을 토대로 값을 예측하는 것입니다. 1.2 비지도학습(Unsupervised Learning) 정답(label)을 따로 알려주지 않은 상태에서, 비슷한 데이터들을 군집화 하는 것입니다. 예를들어 고양이, 병아리, 기린, 호랑이 사진을 비지도학습 시킬 경우, 각 사진이 무슨 동물인지 정답(label)을 알려주지 않았기 때문에 이 동물이 ’무엇’이라고 기계가 정의는 할 수 없지만 비슷한 단위로 군집화 해준다.다리가 4개인 고양이와 호랑이를 한 분류로 묶고, 다리가 4개지만 목이 긴 기린은 다른 분류로, 다리가 얇고 몸통이 둥그런 병아리는 또 다른 분류로 나누어 놓을 것입니다. 실무에서는 지도학습에서의 적절한 feature를 찾아내기 위한 전처리 방법으로 비지도 학습을 쓰기도 합니다. 1.3 딥러닝 / 강화학습(Reinforcement Learning) 상과 벌이라는 보상(reward)을 주며 상을 최대화하고 벌을 최소화 하도록 강화 학습하는 방식입니다. 알파고가 이 방법으로 학습 되었고, 주로 게임에서 최적의 동작을 찾는데 쓰는 학습 방식입니다. 1.4 사용 패키지 본 과정에서 사용되는 패키지는 다음과 같이 설치할 수 있습니다. (업데이트 중) pkg = c(&#39;alr3&#39;, &#39;caret&#39;, &#39;ISLR&#39;, &#39;MASS&#39;, &#39;InformationValue&#39;, &#39;leaps&#39;, &#39;car&#39;, &#39;corrplot&#39;, &#39;lmtest&#39;, &#39;bestglm&#39;, &#39;ElemStatLearn&#39;) new.pkg = pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg)) { install.packages(new.pkg, dependencies = TRUE)} "],
["회귀분석.html", "Chapter 2 회귀분석 2.1 상관관계 이해하기 2.2 회귀의 이해 2.3 단변량 회귀분석 2.4 다변량 회귀분석 2.5 다른 고려사항", " Chapter 2 회귀분석 2.1 상관관계 이해하기 먼저 R에서 제공하는 기본 데이터를 불러옵니다. data(anscombe) attach(anscombe) head(anscombe) ## x1 x2 x3 x4 y1 y2 y3 y4 ## 1 10 10 10 8 8.04 9.14 7.46 6.58 ## 2 8 8 8 8 6.95 8.14 6.77 5.76 ## 3 13 13 13 8 7.58 8.74 12.74 7.71 ## 4 9 9 9 8 8.81 8.77 7.11 8.84 ## 5 11 11 11 8 8.33 9.26 7.81 8.47 ## 6 14 14 14 8 9.96 8.10 8.84 7.04 각 변수의 상관관계를 살펴보도록 합니다. cor(x1, y1) ## [1] 0.8164 cor(x2, y2) ## [1] 0.8162 둘 간의 상관관계는 0.8164로 동일합니다. 이를 그림으로 확인해보도록 합니다. par(mfrow = c(2, 2)) plot(x1, y1, main = &#39;Plot 1&#39;) plot(x2, y2, main = &#39;Plot 2&#39;) plot(x3, y3, main = &#39;Plot 3&#39;) plot(x4, y4, main = &#39;Plot 4&#39;) Plot 1은 선형관계를, Plot 2는 곡선 모양을, Plot 3은 특이점이, Plot 4는 특이점 하나만이 상관관계가 있는것 처럼 보입니다. 이처럼 상관관계에만 전적으로 의존하면 제대로 된 결과를 확인할 수 없습니다. 2.2 회귀의 이해 회귀분석의 식은 다음과 같이 나타납니다. \\(y = a + bx\\) \\(y\\): 종속변수 \\(x\\): 독립변수 \\(b\\): 기울기. \\(x\\)가 증가할 때마다 직선이 얼마나 올라가는지를 명시 \\(a\\): 절편. 직선이 세로 \\(y\\)축과 교차하는 지점을 명시 2.2.1 보통 최소 제곱(OLS) 추정 OLS 회귀의 목표는 다음 방정식을 최소화하는 작업입니다. \\(\\sum(y_i - \\hat{y_i})^2 = \\sum{e_i}^2\\) 즉 실제 값과 예측 값의 차로 \\(e\\)(오차)를 정의됩니다. \\(\\bar{y} = a + b\\bar{x}\\)에서 다음식이 유도됩니다. \\(a = \\bar{y} - b\\bar{x}\\) \\(b = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2}\\) \\(Var(x) = \\frac{\\sum(x_i - \\bar{x})^2}{n}\\) \\(Cov(x,y) = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{n}\\) 따라서 b는 다음과 같이 나타낼 수 있습니다. \\(b = \\frac{Cov(x,y)}{Var(x)}\\) R에서 해당 계수는 lm() 함수를 이용해 손쉽게 추정할 수 있습니다. 2.3 단변량 회귀분석 2.3.1 챌린저 호 데이터 미국 우주왕복선 챌린저가 로켓 부스터 고장으로 분해되면서 일곱 명의 승무원이 사망했으며, 잠재 요인으로 발사 온도가 의심되었습니다. 로켓 연결 부분의 밀봉을 담당하는 패킹용 고무 오링이 40°F 미만에서는 테스트되지 않았었고, 발사일의 날씨가 평소와 달리 매우 춥고 영하(31°F)인 상태였기 때문입니다. 다음 데이터는 온도에 따른 오링의 손상여부 테스트 데이터입니다. challenger = read.csv(&#39;http://www.math.usu.edu/~symanzik/teaching/2009_stat6560/RDataAndScripts/sharif_abbass_project1_challenger.csv&#39;) plot(challenger$temperature, challenger$r, xlab = &#39;Temp&#39;, ylab = &#39;Damage&#39;, pch = 1) abline(v = 65) 고온에서 발사될 때 오링의 손상 이벤트가 적어지는 경향이 있습니다. 회귀분석을 통해 둘간의 관계를 살펴보도록 합니다. reg.challenger= lm(r ~ temperature, data = challenger) summary(reg.challenger) ## ## Call: ## lm(formula = r ~ temperature, data = challenger) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.5608 -0.3944 -0.0854 0.1056 1.8671 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.6984 1.2195 3.03 0.0063 ** ## temperature -0.0475 0.0174 -2.73 0.0127 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.577 on 21 degrees of freedom ## Multiple R-squared: 0.261, Adjusted R-squared: 0.226 ## F-statistic: 7.43 on 1 and 21 DF, p-value: 0.0127 temperature의 회귀계수가 -0.05로써 온도와 손상 이벤트 간에는 역의 관계가 있음이 보입니다. 당시 온도인 31°F를 대입하면 오링의 예상 손상 이벤트는 \\(3.69841 + 31 \\times (-0.04754) = 2.22467\\) 이 됩니다. 회귀분석 결과를 그림으로 확인해보도록 하겠습니다. plot(challenger$temperature, challenger$r, xlab = &#39;Temp&#39;, ylab = &#39;Damage&#39;, pch = 1) abline(reg.challenger, lwd = 3, col = &#39;red&#39;) 2.3.2 미국 와이오밍 주 용출량 예측 미국 와이오밍 주 스네이크 강 유역의 용출량을 예측변수, 해당 연도 눈의 강우량을 이용하여 예측합니다. 먼저 해당 데이터를 그림으로 나타내봅니다. library(alr3) data(snake) colnames(snake) = c(&#39;content&#39;, &#39;yield&#39;) head(snake) ## content yield ## 1 23.1 10.5 ## 2 32.8 16.7 ## 3 31.8 18.2 ## 4 32.0 17.0 ## 5 30.4 16.3 ## 6 24.0 10.5 plot(snake, xlab = &#39;water content of snow&#39;, ylab = &#39;water yield&#39;) 양 끝에 특이점 두개가 있습니다. 다음으로 lm() 함수를 이용해 단변량 회귀분석을 실행합니다. reg = lm(yield ~ content, data = snake) summary(reg) ## ## Call: ## lm(formula = yield ~ content, data = snake) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.179 -1.515 -0.362 1.628 3.197 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.7254 1.5488 0.47 0.65 ## content 0.4981 0.0495 10.06 0.000000046 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.74 on 15 degrees of freedom ## Multiple R-squared: 0.871, Adjusted R-squared: 0.862 ## F-statistic: 101 on 1 and 15 DF, p-value: 0.0000000463 content 변수가 유의미한 변수임이 확인됩니다. 다음으로 산포도에 회귀식을 그려보도록 하겠습니다. plot(snake, xlab = &#39;water content of snow&#39;, ylab = &#39;water yield&#39;) abline(reg, lwd = 3, col = &#39;red&#39;) 회귀분석의 가정은 다음과 같습니다. 선형성(linearity): 독립 변수(x)와 종속 변수(y) 사이에 선형적 관계 오류항의 비상관(non-correlation): 오류항 사이에 상관관계가 없음 등분산성(homoscedasticity): 오류항은 정규분포를 따르며 일정한 분산을 가짐. 이 가정을 위배되면 이분산성(heteroscedasticity) 비공선성(non-collinearity): 두 예측 변수 사이에도 선형적인 관계가 있으면 안됨 특이점의 부재(absence of outliers): 특이점이 있으면 추정값이 심하게 왜곡될 수 있음 회귀분석 결과에 plot() 함수를 입력하여 해당 가정을 확인할 수 있습니다. par(mfrow = c(2, 2)) plot(reg) car 패키지의 qqPlot() 함수를 통해 Q-Q 플롯의 신뢰구간을 확인할 수 있습니다. qqPlot(reg) ## [1] 7 10 2.4 다변량 회귀분석 2.4.1 다이아몬드 데이터 다이아몬드 가격에 영향을 미치는 요소에 대해 회귀분석을 실시하도록 합니다. library(caret) data(diamonds) head(diamonds) ## # A tibble: 6 x 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 종속변수로 price, 독립변수로 caret, depth, table 피처를 사용하도록 하겠습니다. caret: 다이아몬드 무게 depth: 깊이 비율, z / mean(x, y) table: 가장 넓은 부분의 너비 대비 다이아몬드 꼭대기의 너비 reg.diamonds = lm(price ~ carat + depth + table, data = diamonds) summary(reg.diamonds) ## ## Call: ## lm(formula = price ~ carat + depth + table, data = diamonds) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18288 -786 -33 527 12487 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13003.44 390.92 33.3 &lt;2e-16 *** ## carat 7858.77 14.15 555.4 &lt;2e-16 *** ## depth -151.24 4.82 -31.4 &lt;2e-16 *** ## table -104.47 3.14 -33.3 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1530 on 53936 degrees of freedom ## Multiple R-squared: 0.854, Adjusted R-squared: 0.854 ## F-statistic: 1.05e+05 on 3 and 53936 DF, p-value: &lt;2e-16 price와 carat은 양의 관계, depth와 table은 음의 관계가 있습니다. 2.4.2 캘리포니아 물 가용량 캘리포니아 오웬스 벨리의 여섯 지점에서 측정한 강설량을 토대로 물 가용량을 예측해보도록 하겠습니다. data(water) str(water) ## &#39;data.frame&#39;: 43 obs. of 8 variables: ## $ Year : int 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 ... ## $ APMAM : num 9.13 5.28 4.2 4.6 7.15 9.7 5.02 6.7 10.5 9.1 ... ## $ APSAB : num 3.58 4.82 3.77 4.46 4.99 5.65 1.45 7.44 5.85 6.13 ... ## $ APSLAKE: num 3.91 5.2 3.67 3.93 4.88 4.91 1.77 6.51 3.38 4.08 ... ## $ OPBPC : num 4.1 7.55 9.52 11.14 16.34 ... ## $ OPRC : num 7.43 11.11 12.2 15.15 20.05 ... ## $ OPSLAKE: num 6.47 10.26 11.35 11.13 22.81 ... ## $ BSAAM : int 54235 67567 66161 68094 107080 67594 65356 67909 92715 70024 ... Year는 불필요한 변수이므로 삭제해주도록 합니다. socal.water = water[, -1] head(socal.water) ## APMAM APSAB APSLAKE OPBPC OPRC OPSLAKE BSAAM ## 1 9.13 3.58 3.91 4.10 7.43 6.47 54235 ## 2 5.28 4.82 5.20 7.55 11.11 10.26 67567 ## 3 4.20 3.77 3.67 9.52 12.20 11.35 66161 ## 4 4.60 4.46 3.93 11.14 15.15 11.13 68094 ## 5 7.15 4.99 4.88 16.34 20.05 22.81 107080 ## 6 9.70 5.65 4.91 8.88 8.15 7.41 67594 각 변수들 간 상관관계를 살펴보도록 하겠습니다. library(corrplot) water.cor = cor(socal.water) print(water.cor) ## APMAM APSAB APSLAKE OPBPC OPRC OPSLAKE BSAAM ## APMAM 1.0000 0.82769 0.81608 0.12239 0.1544 0.10754 0.2386 ## APSAB 0.8277 1.00000 0.90030 0.03954 0.1056 0.02961 0.1833 ## APSLAKE 0.8161 0.90030 1.00000 0.09345 0.1064 0.10059 0.2493 ## OPBPC 0.1224 0.03954 0.09345 1.00000 0.8647 0.94335 0.8857 ## OPRC 0.1544 0.10564 0.10638 0.86471 1.0000 0.91914 0.9196 ## OPSLAKE 0.1075 0.02961 0.10059 0.94335 0.9191 1.00000 0.9384 ## BSAAM 0.2386 0.18329 0.24934 0.88575 0.9196 0.93844 1.0000 corrplot(water.cor) AP와 OP 변수들 간의 강한 상관관계가 존재하며, 다중 공선성 문제에 맞닥뜨릴 것이라는 사실을 알 수 있습니다. lm() 함수를 통해 회귀분석을 실시하며, 독립변수로 모든 변수를 입력하고자 할 때는 변수를 모두 입력하는 대신 y ~ . 형태로 입력이 가능합니다. reg = lm(BSAAM ~ ., data = socal.water) summary(reg) ## ## Call: ## lm(formula = BSAAM ~ ., data = socal.water) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12690 -4936 -1424 4173 18542 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 15944.7 4099.8 3.89 0.00042 *** ## APMAM -12.8 708.9 -0.02 0.98572 ## APSAB -664.4 1522.9 -0.44 0.66524 ## APSLAKE 2270.7 1341.3 1.69 0.09911 . ## OPBPC 69.7 461.7 0.15 0.88084 ## OPRC 1916.5 641.4 2.99 0.00503 ** ## OPSLAKE 2211.6 752.7 2.94 0.00573 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7560 on 36 degrees of freedom ## Multiple R-squared: 0.925, Adjusted R-squared: 0.912 ## F-statistic: 73.8 on 6 and 36 DF, p-value: &lt;2e-16 2.4.3 최적화를 통한 변수 선택 변수 선택에는 크게 두가지 방법이 있습니다. 단계적 전방 선택법(forward stepwise selection): 피처가 하나도 없는 모형에서 시작해, 피처를 한 번에 하나씩 더해 모든 피처가 포함될 때까지 계속한다. 잔차 제곱합(RSS)이 제일 작은 피처를 선택 단계적 후방 회귀분석(backward stepwise regression): 모형에 모든 피처를 더해 놓고 시작해 가장 덜 유용한 피처를 한 번에 하나씩 제거 두 방법 모두 편향된 회귀 계수를 생성할 수 있으므로, 최량 부분 집합 회귀 분석법(best subsets regression)을 실시힙합니다. 이는 가능한 모든 피처의 조합을 이용해 모형을 적합화합니다. leaps 패키지의 regsubsets() 함수를 통해 최량 부분 집합 회귀를 수행할 수 있습니다. library(leaps) reg.sub = regsubsets(BSAAM ~ ., data = socal.water) best.summary = summary(reg.sub) best.summary$rss ## [1] 3264010454 2600641788 2068947585 2057133378 2055849271 2055830733 which.min(best.summary$rss) ## [1] 6 피처가 6개 일때 RSS가 가장 낮음이 보입니다. 그러나 피처를 더하면 더할 수록 RSS는 감소하고 \\(R^2\\)는 증가하기 마련입니다. 따라서 피처 선택을 위해 여러 기준을 살펴봐야 합니다. \\(AIC = n \\times log(\\frac{RSS_p}{n}) + 2 \\times p\\) \\(p\\): 테스트하고 있는 모형의 피처 수 \\(C_p = \\frac{RSS_p}{MSE_f} - n + 2 \\times p\\) \\(MSE_t\\): 모든 피처를 포함한 모형의 평균 제곱 오차 \\(n\\): 표본 크기 \\(BIC = n \\times log \\frac{RSS_p}{n} + p \\times log(n)\\) \\(Adjusted\\ R^2 = 1 - \\frac{RSS}{n-p-1} / \\frac{R^2}{n-1}\\) 선형 모형에서 AIC와 Cp는 서로 비례하므로 Cp만 살펴보도록 하며, Cp는 leaps 패키지로 출력할 수 있습니다. plot(best.summary$cp, xlab = &#39;number of features&#39;, ylab = &#39;cp&#39;) 피처가 3개로 구성된 모형이 가장 작은 Cp 값을 가집니다. plot(reg.sub, scale = &#39;Cp&#39;) 가장 작은 Cp 값을 제공하는 피처를 나타내고 있으며 APSLAKE, OPRC, OPSLAKE가 이 모형에 포함된 피처들입니다. 위에서 선택된 피처만으로 다중 회귀분석을 실시하도록 하겠습니다. reg.best = lm(BSAAM ~ APSLAKE + OPRC + OPSLAKE, data = socal.water) summary(reg.best) ## ## Call: ## lm(formula = BSAAM ~ APSLAKE + OPRC + OPSLAKE, data = socal.water) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12964 -5140 -1252 4446 18649 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 15425 3638 4.24 0.00013 *** ## APSLAKE 1712 500 3.42 0.00148 ** ## OPRC 1798 568 3.17 0.00300 ** ## OPSLAKE 2390 447 5.35 0.0000042 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7280 on 39 degrees of freedom ## Multiple R-squared: 0.924, Adjusted R-squared: 0.919 ## F-statistic: 159 on 3 and 39 DF, p-value: &lt;2e-16 3개의 피처만으로 회귀분석한 \\(R^2\\)가 0.9185로써, 전체 피처로 회귀분석한 \\(R^2\\)인 0.9123 대비 증가합니다. par(mfrow = c(2, 2)) plot(reg.best) 2.4.4 Robustness Check 회귀분석의 가정이 맞는지 강건성 체크를 해보도록 하겠습니다. 2.4.4.1 다중공선성 다중공선성(multicollinearity) 여부를 조사하기 위해서는 분산 팽창 인자(VIF: Variance inflation factor) 통계량을 사용해야 합니다. VIF는 모든 피처가 들어 있는 전체 모형을 적합화할 때 계산된 특정한 피처 계수의 분산과 그 피처만 들어 있는 부분 모형으로 적합화했을 때의 계수 분산의 비율입니다. \\[VIF = 1 / (1 - R^2_i)\\] car 패키지의 vif() 함수를 통해 해당 값을 계산할 수 있습니다. vif(reg.best) ## APSLAKE OPRC OPSLAKE ## 1.011 6.453 6.445 OPRC과 OPSLAKE의 vif가 매우 높게 나오며, 이는 OPRC와 OPSLAKE 간 상관관계가 지나치게 높기 때문입니다. plot(socal.water$OPRC, socal.water$OPSLAKE, xlab = &#39;OPRC&#39;, ylab = &#39;OPSLAKE&#39;) 따라서 둘 중 하나의 변수를 탈락시키는 것이 좋습니다. best.summary$adjr2 ## [1] 0.8778 0.9002 0.9185 0.9169 0.9147 0.9123 변수가 2개인 경우 \\(R^2\\)는 0.900이며, 3개인 경우 \\(R^2\\)는 0.918여서 증가가 경미합니다. 변수 2개로만 이뤄진 모형의 가정을 점검합니다. fit.2 = lm(BSAAM ~ APSLAKE + OPSLAKE, data = socal.water) summary(fit.2) ## ## Call: ## lm(formula = BSAAM ~ APSLAKE + OPSLAKE, data = socal.water) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13336 -5893 -172 4220 19500 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 19145 3812 5.02 0.000011 *** ## APSLAKE 1769 554 3.19 0.0027 ** ## OPSLAKE 3690 196 18.83 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8060 on 40 degrees of freedom ## Multiple R-squared: 0.905, Adjusted R-squared: 0.9 ## F-statistic: 190 on 2 and 40 DF, p-value: &lt;2e-16 par(mfrow = c(2, 2)) plot(fit.2) vif(fit.2) ## APSLAKE OPSLAKE ## 1.01 1.01 2.4.4.2 등분산성 등분산성에 여부는 브루시-페이건(Breusch-Pagan, BP) 테스트를 통해 확인이 가능하며, lmtest 패키지의 bptest() 함수를 이용합니다. library(lmtest) bptest(fit.2) ## ## studentized Breusch-Pagan test ## ## data: fit.2 ## BP = 0.0046, df = 2, p-value = 1 BP 테스트의 귀무가설과 대립가설은 다음과 같습니다 귀무가설: “오차항은 등분산성을 띤다” 대립가설: “오차항은 이분산성을 띤다” p 값이 0.9977로 매우크므로 귀무가설을 기각할 근거가 부족해, 오차항은 등분산을 띤다는 것을 알 수 있습니다. 2.4.5 실제와 예측간의 차이 model$fitted.values에는 모델을 통해 나온 예측값이 있으므로, 실제 값과 차이를 살펴볼 수 있습니다. plot(fit.2$fitted.values, socal.water$BSAAM, xlab = &#39;predicted&#39;, ylab = &#39;actual&#39;, main = &#39;Predicted vs. Actual&#39;) ggplot을 이용 이용하면 더욱 깔끔하게 이를 나타낼 수 있다. library(ggplot2) library(magrittr) socal.water[&#39;Actual&#39;] = water$BSAAM socal.water$Forecast = predict(fit.2) socal.water %&gt;% ggplot(aes(x = Forecast, y = Actual)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se = FALSE) + labs(title = &#39;Forecast vs. Actuals&#39;) 2.5 다른 고려사항 2.5.1 질적 피처 질적 피처(qualitative feature)에서는 남성/여성 또는 나쁨/중간/좋음 등 2개나 그 이상의 단계를 정할 수 있습니다. 예를 들어 성별처럼 두 가지 단계를 갖는 피처가 있다면, 지표 혹은 더미 피처라는 변수를 만들어 임의로 단계 하나는 0, 다른 하나는 1로 줄 수 있습니다. 지표만을 이용해 모형을 만들어도 여전히 선형 모형은 기존 식과 같습니다. \\[Y = B_0 + B_1x + e\\] 피처가 남성일 때 0, 여성일 때 1로 할당할 경우, 남성의 기대값은 \\(y\\) 절편인 \\(B_0\\)이고, 여성의 기대값은 \\(B_0 + B_1x\\) 입니다. R 내에서 factor 형태로 된 피처를 사용할 경우 자동으로 질적 피처로 계산이 됩니다. 예제로 ISLR 패키지의 Carseats 데이터 세트를 사용하도록 합니다. library(ISLR) data(Carseats) str(Carseats) ## &#39;data.frame&#39;: 400 obs. of 11 variables: ## $ Sales : num 9.5 11.22 10.06 7.4 4.15 ... ## $ CompPrice : num 138 111 113 117 141 124 115 136 132 132 ... ## $ Income : num 73 48 35 100 64 113 105 81 110 113 ... ## $ Advertising: num 11 16 10 4 3 13 0 15 0 0 ... ## $ Population : num 276 260 269 466 340 501 45 425 108 131 ... ## $ Price : num 120 83 80 97 128 72 108 120 124 124 ... ## $ ShelveLoc : Factor w/ 3 levels &quot;Bad&quot;,&quot;Good&quot;,&quot;Medium&quot;: 1 2 3 3 1 1 3 2 3 3 ... ## $ Age : num 42 65 59 55 38 78 71 67 76 76 ... ## $ Education : num 17 10 12 14 13 16 15 10 10 17 ... ## $ Urban : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 1 2 2 1 1 ... ## $ US : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 1 2 1 2 1 2 ... 해당 데이터 중 정량적 피처인 광고(Advertising)과 질적 피처인 진열대 위치(ShelveLoc)만을 이용해 카시트(Carseats)의 판매량을 예측합니다. 이 중 진열대 위치는 Bad, Good, Medium 총 3개 level로 구성되어 있습니다. sales.fit = lm(Sales ~ Advertising + ShelveLoc, data = Carseats) summary(sales.fit) ## ## Call: ## lm(formula = Sales ~ Advertising + ShelveLoc, data = Carseats) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.648 -1.620 -0.048 1.531 6.410 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.8966 0.2521 19.43 &lt; 2e-16 *** ## Advertising 0.1007 0.0169 5.95 5.9e-09 *** ## ShelveLocGood 4.5769 0.3348 13.67 &lt; 2e-16 *** ## ShelveLocMedium 1.7514 0.2748 6.37 5.1e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.24 on 396 degrees of freedom ## Multiple R-squared: 0.373, Adjusted R-squared: 0.369 ## F-statistic: 78.6 on 3 and 396 DF, p-value: &lt;2e-16 진열대 위치가 좋은 경우(ShelveLocGood)는 위치가 나쁜 경우의 판매량인 Intercept 값인 4.89662 대비 4.57686이 더 높습니다. 2.5.2 상호작용 항 어떤 피처가 예측에 미치는 영향이 또 다른 피처에 종속적일 경우, 이 두 피처는 서로 상호작용한다고 말합니다. \\[Y = B_0 + B_1x + B_2 + B_1B_2x + e\\] MASS 패키지의 Boston 데이터 세트를 이용해 상호작용 회귀분석을 살펴보도록 하겠습니다. library(MASS) data(Boston) str(Boston) ## &#39;data.frame&#39;: 506 obs. of 14 variables: ## $ crim : num 0.00632 0.02731 0.02729 0.03237 0.06905 ... ## $ zn : num 18 0 0 0 0 0 12.5 12.5 12.5 12.5 ... ## $ indus : num 2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ... ## $ chas : int 0 0 0 0 0 0 0 0 0 0 ... ## $ nox : num 0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ... ## $ rm : num 6.58 6.42 7.18 7 7.15 ... ## $ age : num 65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ... ## $ dis : num 4.09 4.97 4.97 6.06 6.06 ... ## $ rad : int 1 2 2 3 3 3 5 5 5 5 ... ## $ tax : num 296 242 242 222 222 222 311 311 311 311 ... ## $ ptratio: num 15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ... ## $ black : num 397 397 393 395 397 ... ## $ lstat : num 4.98 9.14 4.03 2.94 5.33 ... ## $ medv : num 24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ... 이 중 사용할 피처의 설명은 다음과 같습니다. medv: 주택 가치의 중위값 lstat: 낮은 사회 경제적 지위를 갖는 가구의 백분율 age: 주택의 연령 lm() 함수에 \\(feature1 * feature2\\)를 쓰면, 각 피처뿐만 아니라 두 피처의 상호작용 항도 모형에 포함됩니다. value.fit = lm(medv ~ lstat * age, data = Boston) summary(value.fit) ## ## Call: ## lm(formula = medv ~ lstat * age, data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.81 -4.04 -1.33 2.08 27.55 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 36.088536 1.469835 24.55 &lt; 2e-16 *** ## lstat -1.392117 0.167456 -8.31 8.8e-16 *** ## age -0.000721 0.019879 -0.04 0.971 ## lstat:age 0.004156 0.001852 2.24 0.025 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.15 on 502 degrees of freedom ## Multiple R-squared: 0.556, Adjusted R-squared: 0.553 ## F-statistic: 209 on 3 and 502 DF, p-value: &lt;2e-16 lstat은 매우 예측력이 높은 피처이며, age는 예측력이 높지 않습니다. 그러나 이 두 피처는 유의한 상호작용을 보이며, medv를 설명하는 변수입니다. "],
["로지스틱-회귀.html", "Chapter 3 로지스틱 회귀 3.1 오즈비 3.2 로지스틱 회귀 3.3 입학 데이터 분석 3.4 위스콘신 유방암 데이터 3.5 교차검증을 포함한 로지스틱 회귀 3.6 BIC 기준 최적의 피처 선택 3.7 ROC", " Chapter 3 로지스틱 회귀 3.1 오즈비 오즈는 성공할 확률이 실패할 확률의 몇 배인지를 나타내는 것으로써, \\(Probability(Y) / 1 - (Probability(Y))\\) 공식을 통해 계산됩니다. 예를 들어, 브라질이 월드컵 경기에서 이길 확률이 20%라면, 오즈는 \\(0.2 / (1-0.2) = 0.25\\)가 되고, 1대 4의 승산입니다. 오즈를 확률로 역변환하려면 오즈를 \\(1 + (오즈)\\)로 나누며, 앞의 예에서는 \\(0.25 / (1+0.25) = 0.2\\), 즉 20%가 됩니다. 만일 독일이 우승할 오즈가 0.18, 브라질이 우승할 오즈가 0.25인 경우 둘 간의 오즈를 오즈비를 이용해 비교할 수 있습니다. 브라질이 독일 대비 월드컵에서 우승할 확률은 0.25 / 0.18 = 1.39 입니다. 3.2 로지스틱 회귀 결과가 이항 혹은 다항 범주일 경우, 관찰값이 출력 변수의 특정 범주에 속할 확률을 예측해야 합니다. 이를 위해 기존 OLS 선형 회귀를 사용할 경우 매우 큰 측정 오차가 생길 수 있으며 편향된 결과를 낳습니다. 분류 문제는 0과 1 사이의 값을 갖는 확률로 가장 잘 모형화할 수 있습니다. 로지스틱 회귀와 선형 회귀의 관계는 로지스틱 회귀의 종속 변수를 로그 오즈, 즉 \\(log(P(Y) / 1-P(Y))\\)로 표현하고 이 값이 \\(a + bX\\)와 같음을 밝힘으로써 보일 수 있습니다. 이를 정리하면 다음과 같습니다. \\[log(\\frac{P(Y)}{1-P(Y)}) = a + bX\\] \\[\\frac{P(Y)}{1-P(Y)} = e^{a + bX}\\] \\[ P(Y) = \\frac{e^{a + bX}}{1 + e^{a + bX}} \\] 위 \\(P(Y)\\) 를 그래프로 나타내면 다음과 같다. 즉 \\(x\\)에 따른 \\(y\\)의 확률이 0과 1 사이에 놓이게 됩니다. 3.3 입학 데이터 분석 GRE, GPA, RANK가 입학(admission)에 어떤 영향을 주는지 로지스틱 회귀분석을 통해 분석하도록 하겠습니다. admission = read.csv(&quot;https://stats.idre.ucla.edu/stat/data/binary.csv&quot;) head(admission) ## admit gre gpa rank ## 1 0 380 3.61 3 ## 2 1 660 3.67 3 ## 3 1 800 4.00 1 ## 4 1 640 3.19 4 ## 5 0 520 2.93 4 ## 6 1 760 3.00 2 glm() 함수를 이용하여 로지스틱 회귀분석을 실시합니다. ad.logit = glm(admit ~ ., family = binomial, data = admission) summary(ad.logit) ## ## Call: ## glm(formula = admit ~ ., family = binomial, data = admission) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.580 -0.885 -0.638 1.157 2.173 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.44955 1.13285 -3.05 0.0023 ** ## gre 0.00229 0.00109 2.10 0.0356 * ## gpa 0.77701 0.32748 2.37 0.0177 * ## rank -0.56003 0.12714 -4.40 0.000011 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 499.98 on 399 degrees of freedom ## Residual deviance: 459.44 on 396 degrees of freedom ## AIC: 467.4 ## ## Number of Fisher Scoring iterations: 4 모든 변수가 유의미한 결과를 보입니다. 로지스틱 회귀에서는 OLS와는 다르게 피처의 계수를 \\(X\\)가 한 단위 변화할 때 \\(Y\\)가 변화하는 양을 나타낸다고 해석할 수 없습니다. 로그 함수에서 \\(\\beta\\)라는 계수는 오즈비 \\(e^\\beta\\)로 변환해 해석해야 합니다. exp(coef(ad.logit)) ## (Intercept) gre gpa rank ## 0.03176 1.00230 2.17497 0.57119 오즈비는 피처가 한 단위 변했을 때 나타나는 결과의 오지로 해석할 수 있습니다. 만일 이 값이 1보다 크면 피처가 증가할 때 결과의 오즈도 증가하며, 1보다 작으면 피처가 증가할 때 결과의 오즈는 감소합니다. 위의 예에서 gre와 gpa는 로그 오즈를 증가시키지만, rank는 로그 오즈를 감소시킵니다. ad.probs = ad.logit$fitted.values ad.probs = ifelse(ad.probs &gt; 0.5, 1, 0) 회귀분석 결과의 fitted.values에는 확률이 저장되어 있으며, 해당 값이 0.5보다 크면 1, 그렇지 않으면 0로 변환해줍니다. 이를 실제 데이터와 비교해보도록 합니다. table(ad.probs, admission$admit) ## ## ad.probs 0 1 ## 0 253 98 ## 1 20 29 prop.table(table(ad.probs, admission$admit)) ## ## ad.probs 0 1 ## 0 0.6325 0.2450 ## 1 0.0500 0.0725 맞게 판단할 확률이 대략 70% 입니다. 3.4 위스콘신 유방암 데이터 위스콘신 유방암 데이터를 통해 종양이 양성 혹은 악성인지에 대해 예측해보도록 하겠습니다. 해당 데이터는 MASS 패키지의 biopsy 이름으로 저장되어 있습니다. 3.4.1 데이터 불러오기 및 편집 library(MASS) data(biopsy) str(biopsy) ## &#39;data.frame&#39;: 699 obs. of 11 variables: ## $ ID : chr &quot;1000025&quot; &quot;1002945&quot; &quot;1015425&quot; &quot;1016277&quot; ... ## $ V1 : int 5 5 3 6 4 8 1 2 2 4 ... ## $ V2 : int 1 4 1 8 1 10 1 1 1 2 ... ## $ V3 : int 1 4 1 8 1 10 1 2 1 1 ... ## $ V4 : int 1 5 1 1 3 8 1 1 1 1 ... ## $ V5 : int 2 7 2 3 2 7 2 2 2 2 ... ## $ V6 : int 1 10 2 4 1 10 10 1 1 1 ... ## $ V7 : int 3 3 3 3 3 9 3 3 1 2 ... ## $ V8 : int 1 2 1 7 1 7 1 1 1 1 ... ## $ V9 : int 1 1 1 1 1 1 1 1 5 1 ... ## $ class: Factor w/ 2 levels &quot;benign&quot;,&quot;malignant&quot;: 1 1 1 1 1 2 1 1 1 1 ... 각 피처는 다음과 같습니다. ID: 표본의 코드 번호 V1: 두께 V2: 세포 크기의 균일성 V3: 세포 모양의 균일성 V4: 한계 부착력 V5: 단일 상피세포 크기 V6: 나핵(16개의 관찰값 결측) V7: 특징 없는 염색질 V8: 정상 핵소체 V9: 분열 class: 종양의 진단의 결과, 양성 또는 악성. 우리가 예측하려는 결과 피처명이 입력되어 있지 않으므로, 이를 입력해주도록 합니다. biopsy$ID = NULL names(biopsy) = c(&#39;thick&#39;, &#39;u.size&#39;, &#39;u.shape&#39;, &#39;adhsn&#39;, &#39;s.size&#39;, &#39;nucl&#39;, &#39;chrom&#39;, &#39;n.nuc&#39;, &#39;mit&#39;, &#39;class&#39;) head(biopsy) ## thick u.size u.shape adhsn s.size nucl chrom n.nuc mit class ## 1 5 1 1 1 2 1 3 1 1 benign ## 2 5 4 4 5 7 10 3 2 1 benign ## 3 3 1 1 1 2 2 3 1 1 benign ## 4 6 8 8 1 3 4 3 7 1 benign ## 5 4 1 1 3 2 1 3 1 1 benign ## 6 8 10 10 8 7 10 9 7 1 malignant 다음으로 결측 관측치를 삭제 및 데이터를 변형해줍니다. sum(is.na(biopsy)) ## [1] 16 biopsy.v2 = na.omit(biopsy) y = ifelse(biopsy.v2$class == &#39;malignant&#39;, 1, 0) table(y) ## y ## 0 1 ## 444 239 총 16개의 na 데이터가 존재하며, na.omit() 함수를 통해 해당 데이터를 모두 지워주도록 합니다. 또한 예측변수 y에는 class가 malignant(악성)일 경우 1, 그렇지 않을 경우 0을 입력합니다. gather() 함수를 통해 테이블을 변경한 후, ggplot() 함수를 통해 각 class 별 피처들의 분포를 살펴보도록 합니다. library(ggplot2) library(dplyr) library(tidyr) library(magrittr) biop.m = biopsy.v2 %&gt;% gather(key, value, -class) biop.m %&gt;% ggplot(aes(x = class, y = value)) + geom_boxplot() + facet_wrap( ~ key) 다중공선성 확인을 위해 상관관계를 검사하도록 합니다. library(corrplot) bc = biopsy.v2 %&gt;% dplyr::select(-class) %&gt;% cor() corrplot.mixed(bc) u.size와 u.shape 간 상관관계가 0.91로 다중공선성 문제가 두드러져 보입니다. 3.4.2 데이터 나누기 기존에는 모든 데이터를 이용하여 모델을 훈련시켰습니다. 그러나 모델의 예측력을 평가하기 위해서는 모델링에 사용되지 않은 데이터와 평가하야 합니다. 이를 위해 트레이닝 및 테스트 세트로 나누도록 합니다. 일반적으로 트레이닝과 테스트 셋의 비율은 7:3 혹은 8:2로 합니다. set.seed(123) ind = sample(2, nrow(biopsy.v2), replace = TRUE, prob = c(0.7, 0.3)) train = biopsy.v2[ind==1, ] test = biopsy.v2[ind==2, ] prop.table(table(train$class)) ## ## benign malignant ## 0.6371 0.3629 prop.table(table(test$class)) ## ## benign malignant ## 0.6794 0.3206 sample() 을 통해 무작위 숫자를 7:3 비율로 생성한 후, train과 test 셋으로 나눠주도록 합니다. 그 후 각 데이터 셋의 종속변수의 비율을 확인해 7:3 비율과 비슷한지 확인합니다. 3.4.3 모형화 먼저 모든 입력 변수로 로지스틱 모형을 만든 후 점차 줄여 나가며 최량 부분 집합을 생성하도록 합니다. full.fit = glm(class ~ ., family = binomial, data = train) summary(full.fit) ## ## Call: ## glm(formula = class ~ ., family = binomial, data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.340 -0.139 -0.072 0.032 2.356 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -9.429 1.227 -7.68 1.6e-14 *** ## thick 0.525 0.160 3.28 0.00104 ** ## u.size -0.105 0.245 -0.43 0.66917 ## u.shape 0.280 0.253 1.11 0.26804 ## adhsn 0.309 0.174 1.78 0.07572 . ## s.size 0.287 0.207 1.38 0.16702 ## nucl 0.406 0.121 3.34 0.00083 *** ## chrom 0.274 0.217 1.26 0.20801 ## n.nuc 0.224 0.137 1.63 0.10213 ## mit 0.430 0.339 1.27 0.20540 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 620.989 on 473 degrees of freedom ## Residual deviance: 78.373 on 464 degrees of freedom ## AIC: 98.37 ## ## Number of Fisher Scoring iterations: 8 exp(coef(full.fit)) %&gt;% round(., 4) ## (Intercept) thick u.size u.shape adhsn s.size ## 0.0001 1.6909 0.9007 1.3228 1.3615 1.3319 ## nucl chrom n.nuc mit ## 1.5003 1.3148 1.2516 1.5367 위 예제에서 u.size를 제외한 모든 피처가 로그 오즈를 증가시킵니다. 다음으로 다중공선성을 확인합니다. library(car) vif(full.fit) ## thick u.size u.shape adhsn s.size nucl chrom n.nuc mit ## 1.235 3.249 2.830 1.302 1.636 1.373 1.523 1.343 1.060 위의 값들 중 어느 것도 통계값이 5보다 크지 않으므로 공선성을 크게 문제가 되지 안습니다. train.probs = full.fit$fitted.values head(train.probs) ## 1 3 6 7 9 10 ## 0.02053 0.01088 0.99993 0.08987 0.01379 0.00842 확률을 선택한 후, 해당 값이 0.5보다 클 경우 1, 아닐 경우 0으로 구분합니다. 그 후 train 데이터의 class와 비교하여 예측 정확도를 비교해보도록 한다. train.bi = ifelse(train.probs &gt; 0.5, 1, 0) %&gt;% as.factor() train.class = ifelse(train$class == &#39;malignant&#39;, 1, 0) %&gt;% as.factor() true.ratio = prop.table(table(train.bi, train.class)) print(true.ratio[1,1] + true.ratio[2,2]) ## [1] 0.9684 예측 정확도가 0.6203, 0.0169, 0.0148, 0.3481로 매우 높게 나타납니다. 3.4.3.1 혼돈 행렬(Confusion Matrix) 이해하기 혼돈 행렬은 예측 값이 실제 값과 일치하는지에 따라 여측을 범주화한 표입니다. 한 차원은 예측 값을 나타내고, 다른 차원은 실제값을 나타냅니다. 일반적으로 관심 있는 클래스를 positive 클래스, 다른 클래스들을 false 클래스라고 하며, 두 클래스의 관계는 네 종류의 범주 중 예측이 속하는 범주를 도표화한 2 X 2 혼동 행렬로 표현할 수 있습니다. 각 항목의 설명은 다음과 같습니다. True Positive: 관심 클래스로 정확하게 분류 True Negative: 관심 클래스가 아닌 클래스로 정확하게 분류 False Positive: 관심 클래스로 부정확학 분류 False Negative: 관심 클래스가 아닌 클래스로 부정확하게 분류 혼돈 행렬을 이용한 성능 측정에는 다음과 같은 값들이 있습니다. 정확도(Accuracy): \\(\\frac{TP + TN}{TP + TN + FP + FN}\\), True Positive과 True Negative의 횟수를 전체 예측 횟수로 나눈 값 오류율(Error rate): \\(\\frac{FP + FN}{TP + TN + FP + FN} = 1 - 정확도\\), 부정확한 분류된 예시 재현율(Recall): \\(\\frac{TP}{TP + FN}\\), True Postiive 개수를 전체 긍정 개수로 나누어 계산. 민감도(Sensitivity)로도 불림 특이도(Specificity, 참 부정률): \\(\\frac{TN}{TN + FP}\\), True Negative의 개수를 전체 부정으로 나누어 계산 정밀도(Precision, 긍정 예측 값): \\(\\frac{TP}{TP + FP}\\), 모델이 Positive로 예측할 때 예측이 얼마나 정확한지 여부 F 점수(F-score): \\(\\frac{2 \\times 정밀도 \\times 재현율}{재현율 + 정밀도} = \\frac{2 \\times TP}{2 \\times TP + FP+ FN}\\), 조화 평균을 이용하여 정밀도와 재현율을 결합 혼돈 행렬은 caret 패키지의 confusionMatrix(predict, truth) 함수를 이용해 계산할 수 있습니다. caret::confusionMatrix(train.bi, train.class) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 294 7 ## 1 8 165 ## ## Accuracy : 0.968 ## 95% CI : (0.948, 0.982) ## No Information Rate : 0.637 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.932 ## ## Mcnemar&#39;s Test P-Value : 1 ## ## Sensitivity : 0.974 ## Specificity : 0.959 ## Pos Pred Value : 0.977 ## Neg Pred Value : 0.954 ## Prevalence : 0.637 ## Detection Rate : 0.620 ## Detection Prevalence : 0.635 ## Balanced Accuracy : 0.966 ## ## &#39;Positive&#39; Class : 0 ## 정확도를 의미하는 Accuracy가 0.9684로 직접 계산한 값과 동일합니다. 3.4.4 테스트 셋에 적용 위 모형은 트레이닝 셋을 대상으로 만들어졌습니다. 따라서 모형에 포함되지 않은 데이터인 테스트 셋의 데이터를 대상으로 모델의 정확도를 구해보도록 합니다. test.probs = predict(full.fit, newdata = test, type = &#39;response&#39;) test.bi = ifelse(test.probs &gt; 0.5, 1, 0) %&gt;% as.factor() test.class = ifelse(test$class == &#39;malignant&#39;, 1, 0) %&gt;% as.factor() caret::confusionMatrix(test.bi, test.class) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 139 2 ## 1 3 65 ## ## Accuracy : 0.976 ## 95% CI : (0.945, 0.992) ## No Information Rate : 0.679 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.945 ## ## Mcnemar&#39;s Test P-Value : 1 ## ## Sensitivity : 0.979 ## Specificity : 0.970 ## Pos Pred Value : 0.986 ## Neg Pred Value : 0.956 ## Prevalence : 0.679 ## Detection Rate : 0.665 ## Detection Prevalence : 0.675 ## Balanced Accuracy : 0.975 ## ## &#39;Positive&#39; Class : 0 ## predict() 함수의 newdata 인자에 test를 입력하여 확률을 계산한 후, 혼돈 행렬을 구하도록 합니다. 정확도가 0.9761로써 역시나 뛰어난 성과를 보입니다. 3.5 교차검증을 포함한 로지스틱 회귀 K-폴드 교차검증은 데이터 세트를 같은 크기를 갖는 조각으로 K등분한 후, K-세트 중에 1개의 세트를 번갈아 제외하며 학습합니다. bestglm 패키지를 이용하여 교차 검증을 이용한 로지스틱 회귀분석을 실행할 수 있습니다. 해당 패키지를 이용하기 위해서 결과값을 0과 1로 코드화할 필요가 있으며, 만일 변수형이 팩터 형태로 남아 있으면 작동이 되지 않습니다. 또한 결과값인 \\(y\\)가 맨 마지막 컬럼에 위치해야 하며, 불필요한 컬럼은 삭제되어야 합니다. 이를 고려하여 새로운 데이터 테이블을 만들고, 교차 검증을 실시합니다. library(bestglm) df = train %&gt;% mutate(class = ifelse(class == &#39;malignant&#39;, 1, 0)) bestglm(df, IC = &#39;CV&#39;, CVArgs = list(Method = &#39;HTF&#39;, K = 10, REP = 1), family = binomial) ## CV(K = 10, REP = 1) ## BICq equivalent for q in (0.0000716797006619085, 0.273173435514231) ## Best Model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -7.8147 0.90996 -8.588 8.855e-18 ## thick 0.6188 0.14713 4.206 2.598e-05 ## u.size 0.6582 0.15295 4.303 1.683e-05 ## nucl 0.5726 0.09923 5.771 7.899e-09 K = 10 개를 대상으로 교차 검증을 수행한 결과 최적의 변수가 선택되었습니다. 해당 변수만을 이용하여 다시 로지스틱 회귀분석을 실시합니다. reduce.fit = glm(class ~ thick + u.size + nucl, family = binomial, data = train) summary(reduce.fit) ## ## Call: ## glm(formula = class ~ thick + u.size + nucl, family = binomial, ## data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.579 -0.181 -0.072 0.042 2.373 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -7.8147 0.9100 -8.59 &lt; 2e-16 *** ## thick 0.6188 0.1471 4.21 0.0000259816 *** ## u.size 0.6582 0.1530 4.30 0.0000168303 *** ## nucl 0.5726 0.0992 5.77 0.0000000079 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 620.989 on 473 degrees of freedom ## Residual deviance: 97.665 on 470 degrees of freedom ## AIC: 105.7 ## ## Number of Fisher Scoring iterations: 7 위 모델을 테스트 셋에 적용한 후, 혼돈 행렬을 이용해 측값과 실제 값을 비교해보도록 합니다. library(caret) test.cv.probs = predict(reduce.fit, newdata = test, type = &#39;response&#39;) test.cv.probs = ifelse(test.cv.probs &gt; 0.5, 1, 0) %&gt;% as.factor() test.class = ifelse(test$class == &#39;malignant&#39;, 1, 0) %&gt;% as.factor() caret::confusionMatrix(test.cv.probs, test.class) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 139 5 ## 1 3 62 ## ## Accuracy : 0.962 ## 95% CI : (0.926, 0.983) ## No Information Rate : 0.679 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.911 ## ## Mcnemar&#39;s Test P-Value : 0.724 ## ## Sensitivity : 0.979 ## Specificity : 0.925 ## Pos Pred Value : 0.965 ## Neg Pred Value : 0.954 ## Prevalence : 0.679 ## Detection Rate : 0.665 ## Detection Prevalence : 0.689 ## Balanced Accuracy : 0.952 ## ## &#39;Positive&#39; Class : 0 ## 모든 피처를 포함하는 모형에 비하면 정확도가 다소 떨어졌습니다. 3.6 BIC 기준 최적의 피처 선택 bestglm() 함수의 IC 인자를 변경하여 타 기준 최적 피처를 선택할 수 있으며, BIC 기준 최적의 피처를 선택하도로 하겠습니다. bestglm(df, IC = &#39;BIC&#39;, family = binomial) ## BIC ## BICq equivalent for q in (0.273173435514231, 0.577036596263764) ## Best Model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -8.6170 1.03155 -8.353 6.633e-17 ## thick 0.7114 0.14752 4.822 1.419e-06 ## adhsn 0.4538 0.15034 3.018 2.541e-03 ## nucl 0.5580 0.09848 5.666 1.462e-08 ## n.nuc 0.4291 0.11846 3.622 2.920e-04 이번에는 thick, adhsn, nucl, n.nuc 피처가 선택되었다. 이를 토대로 추정 및 정확도를 계산해봅니다. bic.fit = glm(class ~ thick + adhsn + nucl + n.nuc, family = binomial, data = train) test.bic.probs = predict(bic.fit, newdata = test, type = &#39;response&#39;) test.bic.probs = ifelse(test.bic.probs &gt; 0.5, 1, 0) %&gt;% as.factor() caret::confusionMatrix(test.bic.probs, test.class) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 138 1 ## 1 4 66 ## ## Accuracy : 0.976 ## 95% CI : (0.945, 0.992) ## No Information Rate : 0.679 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.946 ## ## Mcnemar&#39;s Test P-Value : 0.371 ## ## Sensitivity : 0.972 ## Specificity : 0.985 ## Pos Pred Value : 0.993 ## Neg Pred Value : 0.943 ## Prevalence : 0.679 ## Detection Rate : 0.660 ## Detection Prevalence : 0.665 ## Balanced Accuracy : 0.978 ## ## &#39;Positive&#39; Class : 0 ## 정확도가 0.9761으로 소폭 개선되었습니다. 3.7 ROC 분류 모형을 선택할 때는 ROC(Receiver Operating Characteristic) 차트를 주로 이용합니다. ROC 곡선은 거짓 긍정을 피하면서 참 긍정을 팀자하는 것 사이의 트레이드오프를 관찰하는데 사용되며, \\(y\\)축은 참 긍정율(TPR: True Positive Rate), \\(x\\)축은 거짓 긍정율(FPR: False Positive Rate)을 나타냅니다. \\[TPR = 긍정이라고\\ 제대로\\ 분류된 갯수 /\\ 전체\\ 긍정\\ 갯수\\] \\[FPR = 긍정이라고\\ 잘못\\ 분류된\\ 부정\\ 갯수 /\\ 전체\\ 부정\\ 갯수\\] ROC 곡선을 구성하는 점들은 거짓 긍정의 임계치가 변화할 때 참 긍정률을 나타냅니다. 곡선을 생성하기 위해 분류기의 예측을 긍정 클래스의 추정 확률로 내림차순 정렬합니다. 원점에서 시작해 참 긍정률과 거짓 긍정률에 미치는 영향은 수직 또는 수평으로 추적하는 곡선을 만듭니다. 다이어그램의 왼쪽 하단 모서리에서 오른쪽 상단의 모서리까지 대각선은 예측 값이 없는 분류기를 나타냅니다 이 분류기는 참 긍정과 거짓 긍정이 정확히 같은 비율로 탐지되는데, 분류기가 이 둘을 구별하지 못한다는 것을 의미하며, 다른 분류기를 판단하기 위한 기준선입니다. 이 선에 가까운 ROC 곡선은 그다지 유용하지 않은 모델을 나타냅니다. 분류기가 완벽하다면 True Positive는 100%, False Positive는 0%인 y축과 같을 것입니다. 실제 분류기는 위 그림처럼 ‘완벽한’ 분류기와 ‘쓸모없는’ 분류기 사이의 영역에 위치할 것입니다. ROC 곡선이 완벽한 분류기에 가까울수록 분류기는 Positive 값을 더욱 잘 식별하며, 이는 AUC (Area Under Curve)로 측정할 수 있습니다. AUC는 ROC 다이어그램을 2차원 정사각형으로 취급하며, ROC 곡선의 아래 전체 영역을 측정합니다. AUC는 0.5에서 1 사이 값을 나타냅니다. 다음은 모형의 ROC 및 AUC를 계산하는 방법입니다. full.fit = glm(class ~., family = binomial, data = train) test.full.props = predict(full.fit, newdata = test, type = &#39;response&#39;) head(test.full.props) ## 2 4 5 8 11 16 ## 0.960970 0.676287 0.022461 0.005702 0.001921 0.743727 먼저 모든 피처로 로지스틱 회귀 분석을 실시한 후, predict() 함수를 이용해 테스트 셋에 모델을 적용합니다. library(InformationValue) plotROC(test.class, test.full.props) InformationValue 패키지의 plotROC() 함수를 이용해 ROC 그림 및 AUC 값을 계산할 수 있습니다. "],
["ridge-lasso.html", "Chapter 4 RIDGE &amp; LASSO 4.1 규제화 4.2 전립선암 데이터 분석", " Chapter 4 RIDGE &amp; LASSO 4.1 규제화 선형 모형의 목적은 \\(Y = B_o + B_1x_1 + \\dots + B_nx_n + e\\) 수식에서 RSS를 최소화 하는 것입니다. 규제화란 RSS를 최소화하는 과정에 벌점(\\(\\lambda\\), Shrinkage penalty)을 적용합니다. 간단하게 말하면, 우리가 사용하는 모형에서는 앞으로 \\(RSS + \\lambda\\)를 최소화합니다. 이 중 \\(\\lambda\\)는 조정이 가능한 값이며, 해당 값이 0이면 OLS 모형과 같습니다. 4.1.1 규제화의 종류 규제화에는 일반적으로 두가지 방법이 사용됩니다. Ridge Regression: Ridge에서 사용하는 정규화 계수항은 가중값의 제곱 합으로, L2-norm 이라고도 부릅니다. 이 모델은 \\(RSS + \\lambda(\\sum b_k^2)\\)을 최소화하는 값입니다. 람다 값이 커질수록 계수는 0에 가까워지지만 0이 되지는 않습니다. 이는 예측의 정확성을 높이는 효과가 있지만, 어떠한 피처에 관한 가중값도 0으로 만들지 않기 때문에 모형을 해석하고 소통하는데 문제가 될 수도 있습니다. LASSO: LASSO는 정규화한 계수항에 L1-norm을 사용합니다. L1-norm은 피처 가중값의 절대값의 합으로, \\(RSS + \\lambda(\\sum |b_k|)\\)를 최소화합니다. 이러한 벌점은 어떤 피처의 가중값을 0으로 만들 수도 있으며, 모형의 해석 능력을 크게 향상시킬 수 있습니다. 4.2 전립선암 데이터 분석 암-전립선암 데이터를 통해 규제화 기법을 사용한 차이를 살펴보도록 하겠습니다. 4.2.1 데이터 불러오기 및 편집 library(ElemStatLearn) # 데이터 library(car) # VIF 계싼 library(corrplot) library(leaps) # 최량 부분 집합 회귀 library(glmnet) # Ridge, Lasso library(caret) data(&quot;prostate&quot;) str(prostate) ## &#39;data.frame&#39;: 97 obs. of 10 variables: ## $ lcavol : num -0.58 -0.994 -0.511 -1.204 0.751 ... ## $ lweight: num 2.77 3.32 2.69 3.28 3.43 ... ## $ age : int 50 58 74 58 62 50 64 58 47 63 ... ## $ lbph : num -1.39 -1.39 -1.39 -1.39 -1.39 ... ## $ svi : int 0 0 0 0 0 0 0 0 0 0 ... ## $ lcp : num -1.39 -1.39 -1.39 -1.39 -1.39 ... ## $ gleason: int 6 6 7 6 6 6 6 6 6 6 ... ## $ pgg45 : int 0 0 20 0 0 0 0 0 0 0 ... ## $ lpsa : num -0.431 -0.163 -0.163 -0.163 0.372 ... ## $ train : logi TRUE TRUE TRUE TRUE TRUE TRUE ... 각 피처는 다음과 같습니다. lcavol: 암 부피의 로그 값 lweight: 전립선 무게의 로그 값 age: 환자의 나이 lbph: 전립선 비대 크기의 로그 값 svi: 암 세포가 전립선 바깥에 있는 정낭에 침범했는지를 나타내는 변수, 1 = yes, 0 = no lcp: 암 세포가 전립선 표면에서 얼마나 확장했고, 내부로 얼마나 침투했는지를 나타내는 로그 값 gleason: 암 세포가 얼마나 비정상적으로 보이는지 생체 검사를 통해 병리학자가 2에서 10 사이의 점수를 매긴 값. 이 점수가 높을수록 더 공격적인 암 pgg45: 글래슨 패턴 4 또는 5 (높은 단계의 암) lpsa: PSA의 로그 값 train: 트레이닝, 테스트셋 데이터 여부 먼저 gleason의 분포를 확인해보도록 하겠습니다. table(prostate$gleason) ## ## 6 7 8 9 ## 35 56 1 5 글리슨 점수가 6, 7점에 대부분 모여 있으며, 8점인 것은 1개, 9점인 것은 5개 밖에 없습니다. 해당 데이터 처리를 위해 다음과 같은 선택이 있습니다. 해당 피처를 삭제 점수 8과 9만 삭제 해당 피처를 바꿔 새로운 변수를 만듬 이를 위해 글리슨 점수와 lpsa의 관계를 그림으로 살펴보도록 합니다. library(magrittr) library(ggplot2) prostate %&gt;% ggplot(aes(x = gleason, y = lpsa, group = gleason)) + geom_boxplot() 7~9점의 lpsa가 상당히 크므로, 피처를 남겨두는 것이 좋습니다. 따라서 글리슨 점수가 6점 일 때는 0, 7점 이상인 경우에는 1로 바꾸도록 합니다. prostate$gleason = ifelse(prostate$gleason == 6, 0, 1) table(prostate$gleason) ## ## 0 1 ## 35 62 이번에는 각 피처간의 상관관계를 살펴보도록 합니다. cor(prostate) %&gt;% corrplot.mixed lpsa와 lcavol, lcavol과 lcp, svi와 lcp 사이에는 상관관계가 높아 다중 공선성 문제가 발생할 수 있습니다. 4.2.2 데이터 나누기 다음으로 트레이닝 셋과 테스트 셋을 분리하도록 합니다. train = subset(prostate, train == TRUE)[, 1:9] test = subset(prostate, train == FALSE)[, 1:9] train 열이 TRUE이면 트레이닝 셋, FALSE면 테스트 셋으로 나누어주도록 하며, 마지막 열인 train은 모형에 필요치 않으므로 이를 제외하고 선택해줍니다. 4.2.3 모형화 먼저 최량 부분 집합 회귀를 실시한 후 규제화 기법을 활용하도록 합니다. 4.2.3.1 최량 부분 집합 regsubsets() 함수를 이용해 최량 부분 집합 객체를 만듭니다. subfit = regsubsets(lpsa ~ ., data = train) b.sum = summary(subfit) which.min(b.sum$bic) ## [1] 3 plot(b.sum$bic, type = &#39;l&#39;, xlab = &#39;# of features&#39;, ylab = &#39;BIC&#39;) 세 가지 피처를 사용한 모형이 가장 낮은 BIC를 보입니다. 도표를 통해 좀 더 자세하게 비교하도록 합니다. plot(subfit, scale = &#39;bic&#39;) lcavol, lweight, gleason 3개의 결합에서 가장 낮은 BIC를 보입니다. 이제 해당 모형을 통해 OLS 회귀분석을 실시합니다. ols = lm(lpsa ~ lcavol + lweight + gleason, data = train) plot(ols$fitted.values, train$lpsa, xlab = &#39;Predicted&#39;, ylab = &#39;Actual&#39;) 둘 간에는 선형 관계가 보입니다. 이번에는 predict() 함수를 이용해 해당 모형을 테스트 셋에 적용해보도록 합니다. pred.subfit = predict(ols, newdata = test) plot(pred.subfit, test$lpsa, xlab = &#39;Predicted&#39;, ylab = &#39;Actual&#39;) 마지막으로 MSE를 계산하도록 합니다. resid.subfit = test$lpsa - pred.subfit mse.subfit = mean(resid.subfit ^ 2) print(mse.subfit) ## [1] 0.5084 위의 0.51 값을 기준으로 삼은 후, 규제화 기법과 비교하도록 하겠습니다. 4.2.3.2 Ridge Regression glmnet() 함수를 이용해 Ridge 회귀분석을 수행할 수 있으며, 해당 함수는 입력 피처가 데이터 프레임이 아닌 행렬의 형태여야 합니다. 다음과 같은 형태로 함수를 입력합니다. \\[glmnet(x = 입력 데이터 행렬, y = 반응값, family = 분포 방법, alpha = 0)\\] 이 중 alpha가 0이면 Ridge Regression, 1이면 LASSO 방법으로 분석을 합니다. 먼저 Ridge Regression을 수행합니다. x = as.matrix(train[, 1:8]) y = train[, 9] ridge = glmnet(x, y, family = &#39;gaussian&#39;, alpha = 0) print(ridge) 마지막 100번째 결과를 살펴보면 사용하는 피처의 수가 여전히 8개입니다. 편차의 백분율은 0.6971이고, 람다 값은 0.08789 입니다. plot(ridge, label = TRUE) \\(y\\)축은 계수의 값이고, \\(x\\)축은 L1-norm 입니다. 이번에는 람다 값이 바뀜에 따라 계수의 값이 어떻게 바뀌는지 살펴보도록 합니다. plot(ridge, xvar = &#39;lambda&#39;, label = TRUE) 람다 값이 줄어들수록 벌점이 줄어들고 계수의 절대값이 올라갑니다. 해당 모형을 테스트 셋에 적용해 보도록 합니다. newx = as.matrix(test[, 1:8]) ridge.y = predict(ridge, newx = newx, type = &#39;response&#39;, s = 0.1) plot(ridge.y, test$lpsa, xlab = &#39;Predicted&#39;, ylab = &#39;Actual&#39;, main = &#39;Ridge Regression&#39;) 마지막으로 MSE를 계산하도록 합니다. ridge.resid = ridge.y - test$lpsa ridge.mse = mean(ridge.resid^2) print(ridge.mse) ## [1] 0.4784 최량 부분 집합의 MSE 보다 약간 줄어들었습니다. 4.2.3.3 LASSO glmnet()의 alpha 인자를 1로 변경하면 간단하게 LASSO 분석을 실시할 수 있습니다. lasso = glmnet(x, y, family = &#39;gaussian&#39;, alpha = 1) print(lasso) 모형의 람다 값이 줄어드는 데도 편차가 더 이상 나아지지 않아 69번째에서 멈추게 됩니다. plot(lasso, xvar = &#39;lambda&#39;, label = TRUE) 해당 모델을 테스트 셋에 적용하고 MSE를 구하도록 합니다. lasso.y = predict(lasso, newx = newx, type = &#39;response&#39;, s = 0.045) plot(lasso.y, test$lpsa, xlab = &#39;Predicted&#39;, ylab = &#39;Actual&#39;, main = &#39;LASSO&#39;) lasso.resid = lasso.y - test$lpsa lasso.mse = mean(lasso.resid ^2) print(lasso.mse) ## [1] 0.4437 가장 낮은 MSE를 보입니다. 3가지 모형의 MSE를 비교하면 다음과 같습니다. 표 4.1: 각 모형의 MSE 비교 모형 MSE 최량 부분 집합 0.5084 Ridge 0.4784 LASSO 0.4437 "]
]
