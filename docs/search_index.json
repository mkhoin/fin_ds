[
["index.html", "금융 데이터 사이언스 Welcome 사용 패키지", " 금융 데이터 사이언스 이현열 2019-11-19 Welcome R을 이용한 금융 데이터 사이언스 과정 페이지 입니다. 현재 머신러닝 파트만 업로드 중입니다. 사용 패키지 본 과정에서 사용되는 패키지는 다음과 같이 설치할 수 있습니다. (업데이트 중) pkg = c(&#39;alr3&#39;, &#39;caret&#39;, &#39;ISLR&#39;, &#39;MASS&#39;, &#39;InformationValue&#39;, &#39;leaps&#39;, &#39;car&#39;, &#39;corrplot&#39;, &#39;lmtest&#39;, &#39;bestglm&#39;, &#39;ElemStatLearn&#39;, &#39;psych&#39;, &#39;rpart.plot&#39;, &#39;xgboost&#39;, &#39;compareGroups&#39;, &#39;HDclassif&#39;, &#39;NbClust&#39;, &#39;sparcl&#39;) new.pkg = pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg)) { install.packages(new.pkg, dependencies = TRUE)} "],
["머신러닝이란.html", "Chapter 1 머신러닝이란? 1.1 지도학습(Supervised Learning) 1.2 비지도학습(Unsupervised Learning) 1.3 딥러닝 / 강화학습(Reinforcement Learning)", " Chapter 1 머신러닝이란? 머신러닝이란 데이터에서 패턴을 찾아 새로운 데이터의 결과값을 예측하는 방법이며 크게 지도학습, 비지도학습, 강화학습으로 나눌수 있습니다. 1.1 지도학습(Supervised Learning) 지도학습은 정답을 알려주며 학습시키는 것입니다. 예를들어 고양이 사진을 준 후(input data), 이 사진은 고양이(label data)라는 것를 알려준 후, 이러한 패턴을 바탕으로 새로운 데이터의 결과값을 예측하는 방식입니다. 따라서 기계가 정답을 잘 맞췄는지 아닌지 쉽게 알 수 있다. 지도학습에는 크게 분류(classification)과 회귀(regression)가 있습니다. 분류(classification): 두 가지 혹은 여러 값 중 하나로 분류하는 것입니다. 예를 들어 특정 이메일이 스팸인가 아닌가 혹은 개인의 신용등급을 분류하는 방식입니다. 회귀(regression): 어떤 데이터들의 특징(feature)을 토대로 값을 예측하는 것입니다. 1.2 비지도학습(Unsupervised Learning) 정답(label)을 따로 알려주지 않은 상태에서, 비슷한 데이터들을 군집화 하는 것입니다. 예를들어 고양이, 병아리, 기린, 호랑이 사진을 비지도학습 시킬 경우, 각 사진이 무슨 동물인지 정답(label)을 알려주지 않았기 때문에 이 동물이 ’무엇’이라고 기계가 정의는 할 수 없지만 비슷한 단위로 군집화 해준다.다리가 4개인 고양이와 호랑이를 한 분류로 묶고, 다리가 4개지만 목이 긴 기린은 다른 분류로, 다리가 얇고 몸통이 둥그런 병아리는 또 다른 분류로 나누어 놓을 것입니다. 실무에서는 지도학습에서의 적절한 feature를 찾아내기 위한 전처리 방법으로 비지도 학습을 쓰기도 합니다. 1.3 딥러닝 / 강화학습(Reinforcement Learning) 상과 벌이라는 보상(reward)을 주며 상을 최대화하고 벌을 최소화 하도록 강화 학습하는 방식입니다. 알파고가 이 방법으로 학습 되었고, 주로 게임에서 최적의 동작을 찾는데 쓰는 학습 방식입니다. "],
["회귀분석.html", "Chapter 2 회귀분석 2.1 상관관계 이해하기 2.2 회귀의 이해 2.3 단변량 회귀분석 2.4 다변량 회귀분석 2.5 다른 고려사항", " Chapter 2 회귀분석 2.1 상관관계 이해하기 먼저 R에서 제공하는 기본 데이터를 불러옵니다. data(anscombe) attach(anscombe) head(anscombe) ## x1 x2 x3 x4 y1 y2 y3 y4 ## 1 10 10 10 8 8.04 9.14 7.46 6.58 ## 2 8 8 8 8 6.95 8.14 6.77 5.76 ## 3 13 13 13 8 7.58 8.74 12.74 7.71 ## 4 9 9 9 8 8.81 8.77 7.11 8.84 ## 5 11 11 11 8 8.33 9.26 7.81 8.47 ## 6 14 14 14 8 9.96 8.10 8.84 7.04 각 변수의 상관관계를 살펴보도록 합니다. cor(x1, y1) ## [1] 0.8164 cor(x2, y2) ## [1] 0.8162 둘 간의 상관관계는 0.8164로 동일합니다. 이를 그림으로 확인해보도록 합니다. par(mfrow = c(2, 2)) plot(x1, y1, main = &#39;Plot 1&#39;) plot(x2, y2, main = &#39;Plot 2&#39;) plot(x3, y3, main = &#39;Plot 3&#39;) plot(x4, y4, main = &#39;Plot 4&#39;) Plot 1은 선형관계를, Plot 2는 곡선 모양을, Plot 3은 특이점이, Plot 4는 특이점 하나만이 상관관계가 있는것 처럼 보입니다. 이처럼 상관관계에만 전적으로 의존하면 제대로 된 결과를 확인할 수 없습니다. 2.2 회귀의 이해 회귀분석의 식은 다음과 같이 나타납니다. \\(y = a + bx\\) \\(y\\): 종속변수 \\(x\\): 독립변수 \\(b\\): 기울기. \\(x\\)가 증가할 때마다 직선이 얼마나 올라가는지를 명시 \\(a\\): 절편. 직선이 세로 \\(y\\)축과 교차하는 지점을 명시 2.2.1 보통 최소 제곱(OLS) 추정 OLS 회귀의 목표는 다음 방정식을 최소화하는 작업입니다. \\(\\sum(y_i - \\hat{y_i})^2 = \\sum{e_i}^2\\) 즉 실제 값과 예측 값의 차로 \\(e\\)(오차)를 정의됩니다. \\(\\bar{y} = a + b\\bar{x}\\)에서 다음식이 유도됩니다. \\(a = \\bar{y} - b\\bar{x}\\) \\(b = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2}\\) \\(Var(x) = \\frac{\\sum(x_i - \\bar{x})^2}{n}\\) \\(Cov(x,y) = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{n}\\) 따라서 b는 다음과 같이 나타낼 수 있습니다. \\(b = \\frac{Cov(x,y)}{Var(x)}\\) R에서 해당 계수는 lm() 함수를 이용해 손쉽게 추정할 수 있습니다. 2.3 단변량 회귀분석 2.3.1 챌린저 호 데이터 미국 우주왕복선 챌린저가 로켓 부스터 고장으로 분해되면서 일곱 명의 승무원이 사망했으며, 잠재 요인으로 발사 온도가 의심되었습니다. 로켓 연결 부분의 밀봉을 담당하는 패킹용 고무 오링이 40°F 미만에서는 테스트되지 않았었고, 발사일의 날씨가 평소와 달리 매우 춥고 영하(31°F)인 상태였기 때문입니다. 다음 데이터는 온도에 따른 오링의 손상여부 테스트 데이터입니다. challenger = read.csv(&#39;http://www.math.usu.edu/~symanzik/teaching/2009_stat6560/RDataAndScripts/sharif_abbass_project1_challenger.csv&#39;) plot(challenger$temperature, challenger$r, xlab = &#39;Temp&#39;, ylab = &#39;Damage&#39;, pch = 1) abline(v = 65) 고온에서 발사될 때 오링의 손상 이벤트가 적어지는 경향이 있습니다. 회귀분석을 통해 둘간의 관계를 살펴보도록 합니다. reg.challenger= lm(r ~ temperature, data = challenger) summary(reg.challenger) ## ## Call: ## lm(formula = r ~ temperature, data = challenger) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.5608 -0.3944 -0.0854 0.1056 1.8671 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.6984 1.2195 3.03 0.0063 ** ## temperature -0.0475 0.0174 -2.73 0.0127 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.577 on 21 degrees of freedom ## Multiple R-squared: 0.261, Adjusted R-squared: 0.226 ## F-statistic: 7.43 on 1 and 21 DF, p-value: 0.0127 temperature의 회귀계수가 -0.05로써 온도와 손상 이벤트 간에는 역의 관계가 있음이 보입니다. 당시 온도인 31°F를 대입하면 오링의 예상 손상 이벤트는 \\(3.69841 + 31 \\times (-0.04754) = 2.22467\\) 이 됩니다. 회귀분석 결과를 그림으로 확인해보도록 하겠습니다. plot(challenger$temperature, challenger$r, xlab = &#39;Temp&#39;, ylab = &#39;Damage&#39;, pch = 1) abline(reg.challenger, lwd = 3, col = &#39;red&#39;) 2.3.2 미국 와이오밍 주 용출량 예측 미국 와이오밍 주 스네이크 강 유역의 용출량을 예측변수, 해당 연도 눈의 강우량을 이용하여 예측합니다. 먼저 해당 데이터를 그림으로 나타내봅니다. library(alr3) data(snake) colnames(snake) = c(&#39;content&#39;, &#39;yield&#39;) head(snake) ## content yield ## 1 23.1 10.5 ## 2 32.8 16.7 ## 3 31.8 18.2 ## 4 32.0 17.0 ## 5 30.4 16.3 ## 6 24.0 10.5 plot(snake, xlab = &#39;water content of snow&#39;, ylab = &#39;water yield&#39;) 양 끝에 특이점 두개가 있습니다. 다음으로 lm() 함수를 이용해 단변량 회귀분석을 실행합니다. reg = lm(yield ~ content, data = snake) summary(reg) ## ## Call: ## lm(formula = yield ~ content, data = snake) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.179 -1.515 -0.362 1.628 3.197 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.7254 1.5488 0.47 0.65 ## content 0.4981 0.0495 10.06 0.000000046 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.74 on 15 degrees of freedom ## Multiple R-squared: 0.871, Adjusted R-squared: 0.862 ## F-statistic: 101 on 1 and 15 DF, p-value: 0.0000000463 content 변수가 유의미한 변수임이 확인됩니다. 다음으로 산포도에 회귀식을 그려보도록 하겠습니다. plot(snake, xlab = &#39;water content of snow&#39;, ylab = &#39;water yield&#39;) abline(reg, lwd = 3, col = &#39;red&#39;) 회귀분석의 가정은 다음과 같습니다. 선형성(linearity): 독립 변수(x)와 종속 변수(y) 사이에 선형적 관계 오류항의 비상관(non-correlation): 오류항 사이에 상관관계가 없음 등분산성(homoscedasticity): 오류항은 정규분포를 따르며 일정한 분산을 가짐. 이 가정을 위배되면 이분산성(heteroscedasticity) 비공선성(non-collinearity): 두 예측 변수 사이에도 선형적인 관계가 있으면 안됨 특이점의 부재(absence of outliers): 특이점이 있으면 추정값이 심하게 왜곡될 수 있음 회귀분석 결과에 plot() 함수를 입력하여 해당 가정을 확인할 수 있습니다. par(mfrow = c(2, 2)) plot(reg) car 패키지의 qqPlot() 함수를 통해 Q-Q 플롯의 신뢰구간을 확인할 수 있습니다. qqPlot(reg) ## [1] 7 10 2.4 다변량 회귀분석 2.4.1 다이아몬드 데이터 다이아몬드 가격에 영향을 미치는 요소에 대해 회귀분석을 실시하도록 합니다. library(caret) data(diamonds) head(diamonds) ## # A tibble: 6 x 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 종속변수로 price, 독립변수로 caret, depth, table 피처를 사용하도록 하겠습니다. caret: 다이아몬드 무게 depth: 깊이 비율, z / mean(x, y) table: 가장 넓은 부분의 너비 대비 다이아몬드 꼭대기의 너비 reg.diamonds = lm(price ~ carat + depth + table, data = diamonds) summary(reg.diamonds) ## ## Call: ## lm(formula = price ~ carat + depth + table, data = diamonds) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18288 -786 -33 527 12487 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13003.44 390.92 33.3 &lt;2e-16 *** ## carat 7858.77 14.15 555.4 &lt;2e-16 *** ## depth -151.24 4.82 -31.4 &lt;2e-16 *** ## table -104.47 3.14 -33.3 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1530 on 53936 degrees of freedom ## Multiple R-squared: 0.854, Adjusted R-squared: 0.854 ## F-statistic: 1.05e+05 on 3 and 53936 DF, p-value: &lt;2e-16 price와 carat은 양의 관계, depth와 table은 음의 관계가 있습니다. 2.4.2 캘리포니아 물 가용량 캘리포니아 오웬스 벨리의 여섯 지점에서 측정한 강설량을 토대로 물 가용량을 예측해보도록 하겠습니다. data(water) str(water) ## &#39;data.frame&#39;: 43 obs. of 8 variables: ## $ Year : int 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 ... ## $ APMAM : num 9.13 5.28 4.2 4.6 7.15 9.7 5.02 6.7 10.5 9.1 ... ## $ APSAB : num 3.58 4.82 3.77 4.46 4.99 5.65 1.45 7.44 5.85 6.13 ... ## $ APSLAKE: num 3.91 5.2 3.67 3.93 4.88 4.91 1.77 6.51 3.38 4.08 ... ## $ OPBPC : num 4.1 7.55 9.52 11.14 16.34 ... ## $ OPRC : num 7.43 11.11 12.2 15.15 20.05 ... ## $ OPSLAKE: num 6.47 10.26 11.35 11.13 22.81 ... ## $ BSAAM : int 54235 67567 66161 68094 107080 67594 65356 67909 92715 70024 ... Year는 불필요한 변수이므로 삭제해주도록 합니다. socal.water = water[, -1] head(socal.water) ## APMAM APSAB APSLAKE OPBPC OPRC OPSLAKE BSAAM ## 1 9.13 3.58 3.91 4.10 7.43 6.47 54235 ## 2 5.28 4.82 5.20 7.55 11.11 10.26 67567 ## 3 4.20 3.77 3.67 9.52 12.20 11.35 66161 ## 4 4.60 4.46 3.93 11.14 15.15 11.13 68094 ## 5 7.15 4.99 4.88 16.34 20.05 22.81 107080 ## 6 9.70 5.65 4.91 8.88 8.15 7.41 67594 각 변수들 간 상관관계를 살펴보도록 하겠습니다. library(corrplot) water.cor = cor(socal.water) print(water.cor) ## APMAM APSAB APSLAKE OPBPC OPRC OPSLAKE BSAAM ## APMAM 1.0000 0.82769 0.81608 0.12239 0.1544 0.10754 0.2386 ## APSAB 0.8277 1.00000 0.90030 0.03954 0.1056 0.02961 0.1833 ## APSLAKE 0.8161 0.90030 1.00000 0.09345 0.1064 0.10059 0.2493 ## OPBPC 0.1224 0.03954 0.09345 1.00000 0.8647 0.94335 0.8857 ## OPRC 0.1544 0.10564 0.10638 0.86471 1.0000 0.91914 0.9196 ## OPSLAKE 0.1075 0.02961 0.10059 0.94335 0.9191 1.00000 0.9384 ## BSAAM 0.2386 0.18329 0.24934 0.88575 0.9196 0.93844 1.0000 corrplot(water.cor) AP와 OP 변수들 간의 강한 상관관계가 존재하며, 다중 공선성 문제에 맞닥뜨릴 것이라는 사실을 알 수 있습니다. lm() 함수를 통해 회귀분석을 실시하며, 독립변수로 모든 변수를 입력하고자 할 때는 변수를 모두 입력하는 대신 y ~ . 형태로 입력이 가능합니다. reg = lm(BSAAM ~ ., data = socal.water) summary(reg) ## ## Call: ## lm(formula = BSAAM ~ ., data = socal.water) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12690 -4936 -1424 4173 18542 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 15944.7 4099.8 3.89 0.00042 *** ## APMAM -12.8 708.9 -0.02 0.98572 ## APSAB -664.4 1522.9 -0.44 0.66524 ## APSLAKE 2270.7 1341.3 1.69 0.09911 . ## OPBPC 69.7 461.7 0.15 0.88084 ## OPRC 1916.5 641.4 2.99 0.00503 ** ## OPSLAKE 2211.6 752.7 2.94 0.00573 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7560 on 36 degrees of freedom ## Multiple R-squared: 0.925, Adjusted R-squared: 0.912 ## F-statistic: 73.8 on 6 and 36 DF, p-value: &lt;2e-16 2.4.3 최적화를 통한 변수 선택 변수 선택에는 크게 두가지 방법이 있습니다. 단계적 전방 선택법(forward stepwise selection): 피처가 하나도 없는 모형에서 시작해, 피처를 한 번에 하나씩 더해 모든 피처가 포함될 때까지 계속한다. 잔차 제곱합(RSS)이 제일 작은 피처를 선택 단계적 후방 회귀분석(backward stepwise regression): 모형에 모든 피처를 더해 놓고 시작해 가장 덜 유용한 피처를 한 번에 하나씩 제거 두 방법 모두 편향된 회귀 계수를 생성할 수 있으므로, 최량 부분 집합 회귀 분석법(best subsets regression)을 실시힙합니다. 이는 가능한 모든 피처의 조합을 이용해 모형을 적합화합니다. leaps 패키지의 regsubsets() 함수를 통해 최량 부분 집합 회귀를 수행할 수 있습니다. library(leaps) reg.sub = regsubsets(BSAAM ~ ., data = socal.water) best.summary = summary(reg.sub) best.summary$rss ## [1] 3264010454 2600641788 2068947585 2057133378 2055849271 2055830733 which.min(best.summary$rss) ## [1] 6 피처가 6개 일때 RSS가 가장 낮음이 보입니다. 그러나 피처를 더하면 더할 수록 RSS는 감소하고 \\(R^2\\)는 증가하기 마련입니다. 따라서 피처 선택을 위해 여러 기준을 살펴봐야 합니다. \\(AIC = n \\times log(\\frac{RSS_p}{n}) + 2 \\times p\\) \\(p\\): 테스트하고 있는 모형의 피처 수 \\(C_p = \\frac{RSS_p}{MSE_f} - n + 2 \\times p\\) \\(MSE_t\\): 모든 피처를 포함한 모형의 평균 제곱 오차 \\(n\\): 표본 크기 \\(BIC = n \\times log \\frac{RSS_p}{n} + p \\times log(n)\\) \\(Adjusted\\ R^2 = 1 - \\frac{RSS}{n-p-1} / \\frac{R^2}{n-1}\\) 선형 모형에서 AIC와 Cp는 서로 비례하므로 Cp만 살펴보도록 하며, Cp는 leaps 패키지로 출력할 수 있습니다. plot(best.summary$cp, xlab = &#39;number of features&#39;, ylab = &#39;cp&#39;) 피처가 3개로 구성된 모형이 가장 작은 Cp 값을 가집니다. plot(reg.sub, scale = &#39;Cp&#39;) 가장 작은 Cp 값을 제공하는 피처를 나타내고 있으며 APSLAKE, OPRC, OPSLAKE가 이 모형에 포함된 피처들입니다. 위에서 선택된 피처만으로 다중 회귀분석을 실시하도록 하겠습니다. reg.best = lm(BSAAM ~ APSLAKE + OPRC + OPSLAKE, data = socal.water) summary(reg.best) ## ## Call: ## lm(formula = BSAAM ~ APSLAKE + OPRC + OPSLAKE, data = socal.water) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12964 -5140 -1252 4446 18649 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 15425 3638 4.24 0.00013 *** ## APSLAKE 1712 500 3.42 0.00148 ** ## OPRC 1798 568 3.17 0.00300 ** ## OPSLAKE 2390 447 5.35 0.0000042 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7280 on 39 degrees of freedom ## Multiple R-squared: 0.924, Adjusted R-squared: 0.919 ## F-statistic: 159 on 3 and 39 DF, p-value: &lt;2e-16 3개의 피처만으로 회귀분석한 \\(R^2\\)가 0.9185로써, 전체 피처로 회귀분석한 \\(R^2\\)인 0.9123 대비 증가합니다. par(mfrow = c(2, 2)) plot(reg.best) 2.4.4 Robustness Check 회귀분석의 가정이 맞는지 강건성 체크를 해보도록 하겠습니다. 2.4.4.1 다중공선성 다중공선성(multicollinearity) 여부를 조사하기 위해서는 분산 팽창 인자(VIF: Variance inflation factor) 통계량을 사용해야 합니다. VIF는 모든 피처가 들어 있는 전체 모형을 적합화할 때 계산된 특정한 피처 계수의 분산과 그 피처만 들어 있는 부분 모형으로 적합화했을 때의 계수 분산의 비율입니다. \\[VIF = 1 / (1 - R^2_i)\\] car 패키지의 vif() 함수를 통해 해당 값을 계산할 수 있습니다. vif(reg.best) ## APSLAKE OPRC OPSLAKE ## 1.011 6.453 6.445 OPRC과 OPSLAKE의 vif가 매우 높게 나오며, 이는 OPRC와 OPSLAKE 간 상관관계가 지나치게 높기 때문입니다. plot(socal.water$OPRC, socal.water$OPSLAKE, xlab = &#39;OPRC&#39;, ylab = &#39;OPSLAKE&#39;) 따라서 둘 중 하나의 변수를 탈락시키는 것이 좋습니다. best.summary$adjr2 ## [1] 0.8778 0.9002 0.9185 0.9169 0.9147 0.9123 변수가 2개인 경우 \\(R^2\\)는 0.900이며, 3개인 경우 \\(R^2\\)는 0.918여서 증가가 경미합니다. 변수 2개로만 이뤄진 모형의 가정을 점검합니다. fit.2 = lm(BSAAM ~ APSLAKE + OPSLAKE, data = socal.water) summary(fit.2) ## ## Call: ## lm(formula = BSAAM ~ APSLAKE + OPSLAKE, data = socal.water) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13336 -5893 -172 4220 19500 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 19145 3812 5.02 0.000011 *** ## APSLAKE 1769 554 3.19 0.0027 ** ## OPSLAKE 3690 196 18.83 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8060 on 40 degrees of freedom ## Multiple R-squared: 0.905, Adjusted R-squared: 0.9 ## F-statistic: 190 on 2 and 40 DF, p-value: &lt;2e-16 par(mfrow = c(2, 2)) plot(fit.2) vif(fit.2) ## APSLAKE OPSLAKE ## 1.01 1.01 2.4.4.2 등분산성 등분산성에 여부는 브루시-페이건(Breusch-Pagan, BP) 테스트를 통해 확인이 가능하며, lmtest 패키지의 bptest() 함수를 이용합니다. library(lmtest) bptest(fit.2) ## ## studentized Breusch-Pagan test ## ## data: fit.2 ## BP = 0.0046, df = 2, p-value = 1 BP 테스트의 귀무가설과 대립가설은 다음과 같습니다 귀무가설: “오차항은 등분산성을 띤다” 대립가설: “오차항은 이분산성을 띤다” p 값이 0.9977로 매우크므로 귀무가설을 기각할 근거가 부족해, 오차항은 등분산을 띤다는 것을 알 수 있습니다. 2.4.5 실제와 예측간의 차이 model$fitted.values에는 모델을 통해 나온 예측값이 있으므로, 실제 값과 차이를 살펴볼 수 있습니다. plot(fit.2$fitted.values, socal.water$BSAAM, xlab = &#39;predicted&#39;, ylab = &#39;actual&#39;, main = &#39;Predicted vs. Actual&#39;) ggplot을 이용 이용하면 더욱 깔끔하게 이를 나타낼 수 있다. library(ggplot2) library(magrittr) socal.water[&#39;Actual&#39;] = water$BSAAM socal.water$Forecast = predict(fit.2) socal.water %&gt;% ggplot(aes(x = Forecast, y = Actual)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se = FALSE) + labs(title = &#39;Forecast vs. Actuals&#39;) 2.5 다른 고려사항 2.5.1 질적 피처 질적 피처(qualitative feature)에서는 남성/여성 또는 나쁨/중간/좋음 등 2개나 그 이상의 단계를 정할 수 있습니다. 예를 들어 성별처럼 두 가지 단계를 갖는 피처가 있다면, 지표 혹은 더미 피처라는 변수를 만들어 임의로 단계 하나는 0, 다른 하나는 1로 줄 수 있습니다. 지표만을 이용해 모형을 만들어도 여전히 선형 모형은 기존 식과 같습니다. \\[Y = B_0 + B_1x + e\\] 피처가 남성일 때 0, 여성일 때 1로 할당할 경우, 남성의 기대값은 \\(y\\) 절편인 \\(B_0\\)이고, 여성의 기대값은 \\(B_0 + B_1x\\) 입니다. R 내에서 factor 형태로 된 피처를 사용할 경우 자동으로 질적 피처로 계산이 됩니다. 예제로 ISLR 패키지의 Carseats 데이터 세트를 사용하도록 합니다. library(ISLR) data(Carseats) str(Carseats) ## &#39;data.frame&#39;: 400 obs. of 11 variables: ## $ Sales : num 9.5 11.22 10.06 7.4 4.15 ... ## $ CompPrice : num 138 111 113 117 141 124 115 136 132 132 ... ## $ Income : num 73 48 35 100 64 113 105 81 110 113 ... ## $ Advertising: num 11 16 10 4 3 13 0 15 0 0 ... ## $ Population : num 276 260 269 466 340 501 45 425 108 131 ... ## $ Price : num 120 83 80 97 128 72 108 120 124 124 ... ## $ ShelveLoc : Factor w/ 3 levels &quot;Bad&quot;,&quot;Good&quot;,&quot;Medium&quot;: 1 2 3 3 1 1 3 2 3 3 ... ## $ Age : num 42 65 59 55 38 78 71 67 76 76 ... ## $ Education : num 17 10 12 14 13 16 15 10 10 17 ... ## $ Urban : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 1 2 2 1 1 ... ## $ US : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 1 2 1 2 1 2 ... 해당 데이터 중 정량적 피처인 광고(Advertising)과 질적 피처인 진열대 위치(ShelveLoc)만을 이용해 카시트(Carseats)의 판매량을 예측합니다. 이 중 진열대 위치는 Bad, Good, Medium 총 3개 level로 구성되어 있습니다. sales.fit = lm(Sales ~ Advertising + ShelveLoc, data = Carseats) summary(sales.fit) ## ## Call: ## lm(formula = Sales ~ Advertising + ShelveLoc, data = Carseats) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.648 -1.620 -0.048 1.531 6.410 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.8966 0.2521 19.43 &lt; 2e-16 *** ## Advertising 0.1007 0.0169 5.95 5.9e-09 *** ## ShelveLocGood 4.5769 0.3348 13.67 &lt; 2e-16 *** ## ShelveLocMedium 1.7514 0.2748 6.37 5.1e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.24 on 396 degrees of freedom ## Multiple R-squared: 0.373, Adjusted R-squared: 0.369 ## F-statistic: 78.6 on 3 and 396 DF, p-value: &lt;2e-16 진열대 위치가 좋은 경우(ShelveLocGood)는 위치가 나쁜 경우의 판매량인 Intercept 값인 4.89662 대비 4.57686이 더 높습니다. 2.5.2 상호작용 항 어떤 피처가 예측에 미치는 영향이 또 다른 피처에 종속적일 경우, 이 두 피처는 서로 상호작용한다고 말합니다. \\[Y = B_0 + B_1x + B_2 + B_1B_2x + e\\] MASS 패키지의 Boston 데이터 세트를 이용해 상호작용 회귀분석을 살펴보도록 하겠습니다. library(MASS) data(Boston) str(Boston) ## &#39;data.frame&#39;: 506 obs. of 14 variables: ## $ crim : num 0.00632 0.02731 0.02729 0.03237 0.06905 ... ## $ zn : num 18 0 0 0 0 0 12.5 12.5 12.5 12.5 ... ## $ indus : num 2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ... ## $ chas : int 0 0 0 0 0 0 0 0 0 0 ... ## $ nox : num 0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ... ## $ rm : num 6.58 6.42 7.18 7 7.15 ... ## $ age : num 65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ... ## $ dis : num 4.09 4.97 4.97 6.06 6.06 ... ## $ rad : int 1 2 2 3 3 3 5 5 5 5 ... ## $ tax : num 296 242 242 222 222 222 311 311 311 311 ... ## $ ptratio: num 15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ... ## $ black : num 397 397 393 395 397 ... ## $ lstat : num 4.98 9.14 4.03 2.94 5.33 ... ## $ medv : num 24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ... 이 중 사용할 피처의 설명은 다음과 같습니다. medv: 주택 가치의 중위값 lstat: 낮은 사회 경제적 지위를 갖는 가구의 백분율 age: 주택의 연령 lm() 함수에 \\(feature1 * feature2\\)를 쓰면, 각 피처뿐만 아니라 두 피처의 상호작용 항도 모형에 포함됩니다. value.fit = lm(medv ~ lstat * age, data = Boston) summary(value.fit) ## ## Call: ## lm(formula = medv ~ lstat * age, data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.81 -4.04 -1.33 2.08 27.55 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 36.088536 1.469835 24.55 &lt; 2e-16 *** ## lstat -1.392117 0.167456 -8.31 8.8e-16 *** ## age -0.000721 0.019879 -0.04 0.971 ## lstat:age 0.004156 0.001852 2.24 0.025 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.15 on 502 degrees of freedom ## Multiple R-squared: 0.556, Adjusted R-squared: 0.553 ## F-statistic: 209 on 3 and 502 DF, p-value: &lt;2e-16 lstat은 매우 예측력이 높은 피처이며, age는 예측력이 높지 않습니다. 그러나 이 두 피처는 유의한 상호작용을 보이며, medv를 설명하는 변수입니다. "],
["로지스틱-회귀.html", "Chapter 3 로지스틱 회귀 3.1 오즈비 3.2 로지스틱 회귀 3.3 입학 데이터 분석 3.4 위스콘신 유방암 데이터 3.5 교차검증을 포함한 로지스틱 회귀 3.6 BIC 기준 최적의 피처 선택 3.7 ROC", " Chapter 3 로지스틱 회귀 3.1 오즈비 오즈는 성공할 확률이 실패할 확률의 몇 배인지를 나타내는 것으로써, \\(Probability(Y) / 1 - (Probability(Y))\\) 공식을 통해 계산됩니다. 예를 들어, 브라질이 월드컵 경기에서 이길 확률이 20%라면, 오즈는 \\(0.2 / (1-0.2) = 0.25\\)가 되고, 1대 4의 승산입니다. 오즈를 확률로 역변환하려면 오즈를 \\(1 + (오즈)\\)로 나누며, 앞의 예에서는 \\(0.25 / (1+0.25) = 0.2\\), 즉 20%가 됩니다. 만일 독일이 우승할 오즈가 0.18, 브라질이 우승할 오즈가 0.25인 경우 둘 간의 오즈를 오즈비를 이용해 비교할 수 있습니다. 브라질이 독일 대비 월드컵에서 우승할 확률은 0.25 / 0.18 = 1.39 입니다. 3.2 로지스틱 회귀 결과가 이항 혹은 다항 범주일 경우, 관찰값이 출력 변수의 특정 범주에 속할 확률을 예측해야 합니다. 이를 위해 기존 OLS 선형 회귀를 사용할 경우 매우 큰 측정 오차가 생길 수 있으며 편향된 결과를 낳습니다. 분류 문제는 0과 1 사이의 값을 갖는 확률로 가장 잘 모형화할 수 있습니다. 로지스틱 회귀와 선형 회귀의 관계는 로지스틱 회귀의 종속 변수를 로그 오즈, 즉 \\(log(P(Y) / 1-P(Y))\\)로 표현하고 이 값이 \\(a + bX\\)와 같음을 밝힘으로써 보일 수 있습니다. 이를 정리하면 다음과 같습니다. \\[log(\\frac{P(Y)}{1-P(Y)}) = a + bX\\] \\[\\frac{P(Y)}{1-P(Y)} = e^{a + bX}\\] \\[ P(Y) = \\frac{e^{a + bX}}{1 + e^{a + bX}} \\] 위 \\(P(Y)\\) 를 그래프로 나타내면 다음과 같습니다. 즉 \\(x\\)에 따른 \\(y\\)의 확률이 0과 1 사이에 놓이게 됩니다. 3.3 입학 데이터 분석 GRE, GPA, RANK가 입학(admission)에 어떤 영향을 주는지 로지스틱 회귀분석을 통해 분석하도록 하겠습니다. admission = read.csv(&quot;https://stats.idre.ucla.edu/stat/data/binary.csv&quot;) head(admission) ## admit gre gpa rank ## 1 0 380 3.61 3 ## 2 1 660 3.67 3 ## 3 1 800 4.00 1 ## 4 1 640 3.19 4 ## 5 0 520 2.93 4 ## 6 1 760 3.00 2 glm() 함수를 이용하여 로지스틱 회귀분석을 실시합니다. ad.logit = glm(admit ~ ., family = binomial, data = admission) summary(ad.logit) ## ## Call: ## glm(formula = admit ~ ., family = binomial, data = admission) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.580 -0.885 -0.638 1.157 2.173 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.44955 1.13285 -3.05 0.0023 ** ## gre 0.00229 0.00109 2.10 0.0356 * ## gpa 0.77701 0.32748 2.37 0.0177 * ## rank -0.56003 0.12714 -4.40 0.000011 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 499.98 on 399 degrees of freedom ## Residual deviance: 459.44 on 396 degrees of freedom ## AIC: 467.4 ## ## Number of Fisher Scoring iterations: 4 모든 변수가 유의미한 결과를 보입니다. 로지스틱 회귀에서는 OLS와는 다르게 피처의 계수를 \\(X\\)가 한 단위 변화할 때 \\(Y\\)가 변화하는 양을 나타낸다고 해석할 수 없습니다. 로그 함수에서 \\(\\beta\\)라는 계수는 오즈비 \\(e^\\beta\\)로 변환해 해석해야 합니다. exp(coef(ad.logit)) ## (Intercept) gre gpa rank ## 0.03176 1.00230 2.17497 0.57119 오즈비는 피처가 한 단위 변했을 때 나타나는 결과의 오지로 해석할 수 있습니다. 만일 이 값이 1보다 크면 피처가 증가할 때 결과의 오즈도 증가하며, 1보다 작으면 피처가 증가할 때 결과의 오즈는 감소합니다. 위의 예에서 gre와 gpa는 로그 오즈를 증가시키지만, rank는 로그 오즈를 감소시킵니다. ad.probs = ad.logit$fitted.values ad.probs = ifelse(ad.probs &gt; 0.5, 1, 0) 회귀분석 결과의 fitted.values에는 확률이 저장되어 있으며, 해당 값이 0.5보다 크면 1, 그렇지 않으면 0로 변환해줍니다. 이를 실제 데이터와 비교해보도록 합니다. table(ad.probs, admission$admit) ## ## ad.probs 0 1 ## 0 253 98 ## 1 20 29 prop.table(table(ad.probs, admission$admit)) ## ## ad.probs 0 1 ## 0 0.6325 0.2450 ## 1 0.0500 0.0725 맞게 판단할 확률이 대략 70% 입니다. 3.4 위스콘신 유방암 데이터 위스콘신 유방암 데이터를 통해 종양이 양성 혹은 악성인지에 대해 예측해보도록 하겠습니다. 해당 데이터는 MASS 패키지의 biopsy 이름으로 저장되어 있습니다. 3.4.1 데이터 불러오기 및 편집 library(MASS) data(biopsy) str(biopsy) ## &#39;data.frame&#39;: 699 obs. of 11 variables: ## $ ID : chr &quot;1000025&quot; &quot;1002945&quot; &quot;1015425&quot; &quot;1016277&quot; ... ## $ V1 : int 5 5 3 6 4 8 1 2 2 4 ... ## $ V2 : int 1 4 1 8 1 10 1 1 1 2 ... ## $ V3 : int 1 4 1 8 1 10 1 2 1 1 ... ## $ V4 : int 1 5 1 1 3 8 1 1 1 1 ... ## $ V5 : int 2 7 2 3 2 7 2 2 2 2 ... ## $ V6 : int 1 10 2 4 1 10 10 1 1 1 ... ## $ V7 : int 3 3 3 3 3 9 3 3 1 2 ... ## $ V8 : int 1 2 1 7 1 7 1 1 1 1 ... ## $ V9 : int 1 1 1 1 1 1 1 1 5 1 ... ## $ class: Factor w/ 2 levels &quot;benign&quot;,&quot;malignant&quot;: 1 1 1 1 1 2 1 1 1 1 ... 각 피처는 다음과 같습니다. ID: 표본의 코드 번호 V1: 두께 V2: 세포 크기의 균일성 V3: 세포 모양의 균일성 V4: 한계 부착력 V5: 단일 상피세포 크기 V6: 나핵(16개의 관찰값 결측) V7: 특징 없는 염색질 V8: 정상 핵소체 V9: 분열 class: 종양의 진단의 결과, 양성 또는 악성. 우리가 예측하려는 결과 피처명이 입력되어 있지 않으므로, 이를 입력해주도록 합니다. biopsy$ID = NULL names(biopsy) = c(&#39;thick&#39;, &#39;u.size&#39;, &#39;u.shape&#39;, &#39;adhsn&#39;, &#39;s.size&#39;, &#39;nucl&#39;, &#39;chrom&#39;, &#39;n.nuc&#39;, &#39;mit&#39;, &#39;class&#39;) head(biopsy) ## thick u.size u.shape adhsn s.size nucl chrom n.nuc mit class ## 1 5 1 1 1 2 1 3 1 1 benign ## 2 5 4 4 5 7 10 3 2 1 benign ## 3 3 1 1 1 2 2 3 1 1 benign ## 4 6 8 8 1 3 4 3 7 1 benign ## 5 4 1 1 3 2 1 3 1 1 benign ## 6 8 10 10 8 7 10 9 7 1 malignant 다음으로 결측 관측치를 삭제 및 데이터를 변형해줍니다. sum(is.na(biopsy)) ## [1] 16 biopsy.v2 = na.omit(biopsy) y = ifelse(biopsy.v2$class == &#39;malignant&#39;, 1, 0) table(y) ## y ## 0 1 ## 444 239 총 16개의 na 데이터가 존재하며, na.omit() 함수를 통해 해당 데이터를 모두 지워주도록 합니다. 또한 예측변수 y에는 class가 malignant(악성)일 경우 1, 그렇지 않을 경우 0을 입력합니다. gather() 함수를 통해 테이블을 변경한 후, ggplot() 함수를 통해 각 class 별 피처들의 분포를 살펴보도록 합니다. library(ggplot2) library(dplyr) library(tidyr) library(magrittr) biop.m = biopsy.v2 %&gt;% gather(key, value, -class) biop.m %&gt;% ggplot(aes(x = class, y = value)) + geom_boxplot() + facet_wrap( ~ key) 다중공선성 확인을 위해 상관관계를 검사하도록 합니다. library(corrplot) bc = biopsy.v2 %&gt;% dplyr::select(-class) %&gt;% cor() corrplot.mixed(bc) u.size와 u.shape 간 상관관계가 0.91로 다중공선성 문제가 두드러져 보입니다. 3.4.2 데이터 나누기 기존에는 모든 데이터를 이용하여 모델을 훈련시켰습니다. 그러나 모델의 예측력을 평가하기 위해서는 모델링에 사용되지 않은 데이터와 평가하야 합니다. 이를 위해 트레이닝 및 테스트 세트로 나누도록 합니다. 일반적으로 트레이닝과 테스트 셋의 비율은 7:3 혹은 8:2로 합니다. set.seed(123) ind = sample(2, nrow(biopsy.v2), replace = TRUE, prob = c(0.7, 0.3)) train = biopsy.v2[ind==1, ] test = biopsy.v2[ind==2, ] prop.table(table(train$class)) ## ## benign malignant ## 0.6371 0.3629 prop.table(table(test$class)) ## ## benign malignant ## 0.6794 0.3206 sample() 을 통해 무작위 숫자를 7:3 비율로 생성한 후, train과 test 셋으로 나눠주도록 합니다. 그 후 각 데이터 셋의 종속변수의 비율을 확인해 7:3 비율과 비슷한지 확인합니다. 3.4.3 모형화 먼저 모든 입력 변수로 로지스틱 모형을 만든 후 점차 줄여 나가며 최량 부분 집합을 생성하도록 합니다. full.fit = glm(class ~ ., family = binomial, data = train) summary(full.fit) ## ## Call: ## glm(formula = class ~ ., family = binomial, data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.340 -0.139 -0.072 0.032 2.356 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -9.429 1.227 -7.68 1.6e-14 *** ## thick 0.525 0.160 3.28 0.00104 ** ## u.size -0.105 0.245 -0.43 0.66917 ## u.shape 0.280 0.253 1.11 0.26804 ## adhsn 0.309 0.174 1.78 0.07572 . ## s.size 0.287 0.207 1.38 0.16702 ## nucl 0.406 0.121 3.34 0.00083 *** ## chrom 0.274 0.217 1.26 0.20801 ## n.nuc 0.224 0.137 1.63 0.10213 ## mit 0.430 0.339 1.27 0.20540 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 620.989 on 473 degrees of freedom ## Residual deviance: 78.373 on 464 degrees of freedom ## AIC: 98.37 ## ## Number of Fisher Scoring iterations: 8 exp(coef(full.fit)) %&gt;% round(., 4) ## (Intercept) thick u.size u.shape adhsn s.size ## 0.0001 1.6909 0.9007 1.3228 1.3615 1.3319 ## nucl chrom n.nuc mit ## 1.5003 1.3148 1.2516 1.5367 위 예제에서 u.size를 제외한 모든 피처가 로그 오즈를 증가시킵니다. 다음으로 다중공선성을 확인합니다. library(car) vif(full.fit) ## thick u.size u.shape adhsn s.size nucl chrom n.nuc mit ## 1.235 3.249 2.830 1.302 1.636 1.373 1.523 1.343 1.060 위의 값들 중 어느 것도 통계값이 5보다 크지 않으므로 공선성을 크게 문제가 되지 안습니다. train.probs = full.fit$fitted.values head(train.probs) ## 1 3 6 7 9 10 ## 0.02053 0.01088 0.99993 0.08987 0.01379 0.00842 확률을 선택한 후, 해당 값이 0.5보다 클 경우 1, 아닐 경우 0으로 구분합니다. 그 후 train 데이터의 class와 비교하여 예측 정확도를 비교해보도록 한다. train.bi = ifelse(train.probs &gt; 0.5, 1, 0) %&gt;% as.factor() train.class = ifelse(train$class == &#39;malignant&#39;, 1, 0) %&gt;% as.factor() true.ratio = prop.table(table(train.bi, train.class)) print(true.ratio[1,1] + true.ratio[2,2]) ## [1] 0.9684 예측 정확도가 0.6203, 0.0169, 0.0148, 0.3481로 매우 높게 나타납니다. 3.4.3.1 혼돈 행렬(Confusion Matrix) 이해하기 혼돈 행렬은 예측 값이 실제 값과 일치하는지에 따라 여측을 범주화한 표입니다. 한 차원은 예측 값을 나타내고, 다른 차원은 실제값을 나타냅니다. 일반적으로 관심 있는 클래스를 positive 클래스, 다른 클래스들을 false 클래스라고 하며, 두 클래스의 관계는 네 종류의 범주 중 예측이 속하는 범주를 도표화한 2 X 2 혼동 행렬로 표현할 수 있습니다. 각 항목의 설명은 다음과 같습니다. True Positive: 관심 클래스로 정확하게 분류 True Negative: 관심 클래스가 아닌 클래스로 정확하게 분류 False Positive: 관심 클래스로 부정확학 분류 False Negative: 관심 클래스가 아닌 클래스로 부정확하게 분류 혼돈 행렬을 이용한 성능 측정에는 다음과 같은 값들이 있습니다. 정확도(Accuracy): \\(\\frac{TP + TN}{TP + TN + FP + FN}\\), True Positive과 True Negative의 횟수를 전체 예측 횟수로 나눈 값 오류율(Error rate): \\(\\frac{FP + FN}{TP + TN + FP + FN} = 1 - 정확도\\), 부정확한 분류된 예시 재현율(Recall): \\(\\frac{TP}{TP + FN}\\), True Postiive 개수를 전체 긍정 개수로 나누어 계산. 민감도(Sensitivity)로도 불림 특이도(Specificity, 참 부정률): \\(\\frac{TN}{TN + FP}\\), True Negative의 개수를 전체 부정으로 나누어 계산 정밀도(Precision, 긍정 예측 값): \\(\\frac{TP}{TP + FP}\\), 모델이 Positive로 예측할 때 예측이 얼마나 정확한지 여부 F 점수(F-score): \\(\\frac{2 \\times 정밀도 \\times 재현율}{재현율 + 정밀도} = \\frac{2 \\times TP}{2 \\times TP + FP+ FN}\\), 조화 평균을 이용하여 정밀도와 재현율을 결합 혼돈 행렬은 caret 패키지의 confusionMatrix(predict, truth) 함수를 이용해 계산할 수 있습니다. caret::confusionMatrix(train.bi, train.class) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 294 7 ## 1 8 165 ## ## Accuracy : 0.968 ## 95% CI : (0.948, 0.982) ## No Information Rate : 0.637 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.932 ## ## Mcnemar&#39;s Test P-Value : 1 ## ## Sensitivity : 0.974 ## Specificity : 0.959 ## Pos Pred Value : 0.977 ## Neg Pred Value : 0.954 ## Prevalence : 0.637 ## Detection Rate : 0.620 ## Detection Prevalence : 0.635 ## Balanced Accuracy : 0.966 ## ## &#39;Positive&#39; Class : 0 ## 정확도를 의미하는 Accuracy가 0.9684로 직접 계산한 값과 동일합니다. 3.4.4 테스트 셋에 적용 위 모형은 트레이닝 셋을 대상으로 만들어졌습니다. 따라서 모형에 포함되지 않은 데이터인 테스트 셋의 데이터를 대상으로 모델의 정확도를 구해보도록 합니다. test.probs = predict(full.fit, newdata = test, type = &#39;response&#39;) test.bi = ifelse(test.probs &gt; 0.5, 1, 0) %&gt;% as.factor() test.class = ifelse(test$class == &#39;malignant&#39;, 1, 0) %&gt;% as.factor() caret::confusionMatrix(test.bi, test.class) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 139 2 ## 1 3 65 ## ## Accuracy : 0.976 ## 95% CI : (0.945, 0.992) ## No Information Rate : 0.679 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.945 ## ## Mcnemar&#39;s Test P-Value : 1 ## ## Sensitivity : 0.979 ## Specificity : 0.970 ## Pos Pred Value : 0.986 ## Neg Pred Value : 0.956 ## Prevalence : 0.679 ## Detection Rate : 0.665 ## Detection Prevalence : 0.675 ## Balanced Accuracy : 0.975 ## ## &#39;Positive&#39; Class : 0 ## predict() 함수의 newdata 인자에 test를 입력하여 확률을 계산한 후, 혼돈 행렬을 구하도록 합니다. 정확도가 0.9761로써 역시나 뛰어난 성과를 보입니다. 3.5 교차검증을 포함한 로지스틱 회귀 K-폴드 교차검증은 데이터 세트를 같은 크기를 갖는 조각으로 K등분한 후, K-세트 중에 1개의 세트를 번갈아 제외하며 학습합니다. bestglm 패키지를 이용하여 교차 검증을 이용한 로지스틱 회귀분석을 실행할 수 있습니다. 해당 패키지를 이용하기 위해서 결과값을 0과 1로 코드화할 필요가 있으며, 만일 변수형이 팩터 형태로 남아 있으면 작동이 되지 않습니다. 또한 결과값인 \\(y\\)가 맨 마지막 컬럼에 위치해야 하며, 불필요한 컬럼은 삭제되어야 합니다. 이를 고려하여 새로운 데이터 테이블을 만들고, 교차 검증을 실시합니다. library(bestglm) df = train %&gt;% mutate(class = ifelse(class == &#39;malignant&#39;, 1, 0)) bestglm(df, IC = &#39;CV&#39;, CVArgs = list(Method = &#39;HTF&#39;, K = 10, REP = 1), family = binomial) ## CV(K = 10, REP = 1) ## BICq equivalent for q in (0.0000716797006619085, 0.273173435514231) ## Best Model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -7.8147 0.90996 -8.588 8.855e-18 ## thick 0.6188 0.14713 4.206 2.598e-05 ## u.size 0.6582 0.15295 4.303 1.683e-05 ## nucl 0.5726 0.09923 5.771 7.899e-09 K = 10 개를 대상으로 교차 검증을 수행한 결과 최적의 변수가 선택되었습니다. 해당 변수만을 이용하여 다시 로지스틱 회귀분석을 실시합니다. reduce.fit = glm(class ~ thick + u.size + nucl, family = binomial, data = train) summary(reduce.fit) ## ## Call: ## glm(formula = class ~ thick + u.size + nucl, family = binomial, ## data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.579 -0.181 -0.072 0.042 2.373 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -7.8147 0.9100 -8.59 &lt; 2e-16 *** ## thick 0.6188 0.1471 4.21 0.0000259816 *** ## u.size 0.6582 0.1530 4.30 0.0000168303 *** ## nucl 0.5726 0.0992 5.77 0.0000000079 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 620.989 on 473 degrees of freedom ## Residual deviance: 97.665 on 470 degrees of freedom ## AIC: 105.7 ## ## Number of Fisher Scoring iterations: 7 위 모델을 테스트 셋에 적용한 후, 혼돈 행렬을 이용해 측값과 실제 값을 비교해보도록 합니다. library(caret) test.cv.probs = predict(reduce.fit, newdata = test, type = &#39;response&#39;) test.cv.probs = ifelse(test.cv.probs &gt; 0.5, 1, 0) %&gt;% as.factor() test.class = ifelse(test$class == &#39;malignant&#39;, 1, 0) %&gt;% as.factor() caret::confusionMatrix(test.cv.probs, test.class) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 139 5 ## 1 3 62 ## ## Accuracy : 0.962 ## 95% CI : (0.926, 0.983) ## No Information Rate : 0.679 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.911 ## ## Mcnemar&#39;s Test P-Value : 0.724 ## ## Sensitivity : 0.979 ## Specificity : 0.925 ## Pos Pred Value : 0.965 ## Neg Pred Value : 0.954 ## Prevalence : 0.679 ## Detection Rate : 0.665 ## Detection Prevalence : 0.689 ## Balanced Accuracy : 0.952 ## ## &#39;Positive&#39; Class : 0 ## 모든 피처를 포함하는 모형에 비하면 정확도가 다소 떨어졌습니다. 3.6 BIC 기준 최적의 피처 선택 bestglm() 함수의 IC 인자를 변경하여 타 기준 최적 피처를 선택할 수 있으며, BIC 기준 최적의 피처를 선택하도로 하겠습니다. bestglm(df, IC = &#39;BIC&#39;, family = binomial) ## BIC ## BICq equivalent for q in (0.273173435514231, 0.577036596263764) ## Best Model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -8.6170 1.03155 -8.353 6.633e-17 ## thick 0.7114 0.14752 4.822 1.419e-06 ## adhsn 0.4538 0.15034 3.018 2.541e-03 ## nucl 0.5580 0.09848 5.666 1.462e-08 ## n.nuc 0.4291 0.11846 3.622 2.920e-04 이번에는 thick, adhsn, nucl, n.nuc 피처가 선택되었다. 이를 토대로 추정 및 정확도를 계산해봅니다. bic.fit = glm(class ~ thick + adhsn + nucl + n.nuc, family = binomial, data = train) test.bic.probs = predict(bic.fit, newdata = test, type = &#39;response&#39;) test.bic.probs = ifelse(test.bic.probs &gt; 0.5, 1, 0) %&gt;% as.factor() caret::confusionMatrix(test.bic.probs, test.class) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 138 1 ## 1 4 66 ## ## Accuracy : 0.976 ## 95% CI : (0.945, 0.992) ## No Information Rate : 0.679 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.946 ## ## Mcnemar&#39;s Test P-Value : 0.371 ## ## Sensitivity : 0.972 ## Specificity : 0.985 ## Pos Pred Value : 0.993 ## Neg Pred Value : 0.943 ## Prevalence : 0.679 ## Detection Rate : 0.660 ## Detection Prevalence : 0.665 ## Balanced Accuracy : 0.978 ## ## &#39;Positive&#39; Class : 0 ## 정확도가 0.9761으로 소폭 개선되었습니다. 3.7 ROC 분류 모형을 선택할 때는 ROC(Receiver Operating Characteristic) 차트를 주로 이용합니다. ROC 곡선은 거짓 긍정을 피하면서 참 긍정을 팀자하는 것 사이의 트레이드오프를 관찰하는데 사용되며, \\(y\\)축은 참 긍정율(TPR: True Positive Rate), \\(x\\)축은 거짓 긍정율(FPR: False Positive Rate)을 나타냅니다. \\[TPR = 긍정이라고\\ 제대로\\ 분류된 갯수 /\\ 전체\\ 긍정\\ 갯수\\] \\[FPR = 긍정이라고\\ 잘못\\ 분류된\\ 부정\\ 갯수 /\\ 전체\\ 부정\\ 갯수\\] ROC 곡선을 구성하는 점들은 거짓 긍정의 임계치가 변화할 때 참 긍정률을 나타냅니다. 곡선을 생성하기 위해 분류기의 예측을 긍정 클래스의 추정 확률로 내림차순 정렬합니다. 원점에서 시작해 참 긍정률과 거짓 긍정률에 미치는 영향은 수직 또는 수평으로 추적하는 곡선을 만듭니다. 다이어그램의 왼쪽 하단 모서리에서 오른쪽 상단의 모서리까지 대각선은 예측 값이 없는 분류기를 나타냅니다 이 분류기는 참 긍정과 거짓 긍정이 정확히 같은 비율로 탐지되는데, 분류기가 이 둘을 구별하지 못한다는 것을 의미하며, 다른 분류기를 판단하기 위한 기준선입니다. 이 선에 가까운 ROC 곡선은 그다지 유용하지 않은 모델을 나타냅니다. 분류기가 완벽하다면 True Positive는 100%, False Positive는 0%인 y축과 같을 것입니다. 실제 분류기는 위 그림처럼 ‘완벽한’ 분류기와 ‘쓸모없는’ 분류기 사이의 영역에 위치할 것입니다. ROC 곡선이 완벽한 분류기에 가까울수록 분류기는 Positive 값을 더욱 잘 식별하며, 이는 AUC (Area Under Curve)로 측정할 수 있습니다. AUC는 ROC 다이어그램을 2차원 정사각형으로 취급하며, ROC 곡선의 아래 전체 영역을 측정합니다. AUC는 0.5에서 1 사이 값을 나타냅니다. 다음은 모형의 ROC 및 AUC를 계산하는 방법입니다. full.fit = glm(class ~., family = binomial, data = train) test.full.props = predict(full.fit, newdata = test, type = &#39;response&#39;) head(test.full.props) ## 2 4 5 8 11 16 ## 0.960970 0.676287 0.022461 0.005702 0.001921 0.743727 먼저 모든 피처로 로지스틱 회귀 분석을 실시한 후, predict() 함수를 이용해 테스트 셋에 모델을 적용합니다. library(InformationValue) plotROC(test.class, test.full.props) InformationValue 패키지의 plotROC() 함수를 이용해 ROC 그림 및 AUC 값을 계산할 수 있습니다. "],
["ridge-lasso.html", "Chapter 4 RIDGE &amp; LASSO 4.1 규제화 4.2 전립선암 데이터 분석", " Chapter 4 RIDGE &amp; LASSO 4.1 규제화 선형 모형의 목적은 \\(Y = B_o + B_1x_1 + \\dots + B_nx_n + e\\) 수식에서 RSS를 최소화 하는 것입니다. 규제화란 RSS를 최소화하는 과정에 벌점(\\(\\lambda\\), Shrinkage penalty)을 적용합니다. 간단하게 말하면, 우리가 사용하는 모형에서는 앞으로 \\(RSS + \\lambda\\)를 최소화합니다. 이 중 \\(\\lambda\\)는 조정이 가능한 값이며, 해당 값이 0이면 OLS 모형과 같습니다. 4.1.1 규제화의 종류 규제화에는 일반적으로 두가지 방법이 사용됩니다. Ridge Regression: Ridge에서 사용하는 정규화 계수항은 가중값의 제곱 합으로, L2-norm 이라고도 부릅니다. 이 모델은 \\(RSS + \\lambda(\\sum b_k^2)\\)을 최소화하는 값입니다. 람다 값이 커질수록 계수는 0에 가까워지지만 0이 되지는 않습니다. 이는 예측의 정확성을 높이는 효과가 있지만, 어떠한 피처에 관한 가중값도 0으로 만들지 않기 때문에 모형을 해석하고 소통하는데 문제가 될 수도 있습니다. LASSO: LASSO는 정규화한 계수항에 L1-norm을 사용합니다. L1-norm은 피처 가중값의 절대값의 합으로, \\(RSS + \\lambda(\\sum |b_k|)\\)를 최소화합니다. 이러한 벌점은 어떤 피처의 가중값을 0으로 만들 수도 있으며, 모형의 해석 능력을 크게 향상시킬 수 있습니다. 4.2 전립선암 데이터 분석 암-전립선암 데이터를 통해 규제화 기법을 사용한 차이를 살펴보도록 하겠습니다. 4.2.1 데이터 불러오기 및 편집 library(ElemStatLearn) # 데이터 library(car) # VIF 계싼 library(corrplot) library(leaps) # 최량 부분 집합 회귀 library(glmnet) # Ridge, Lasso library(caret) data(&quot;prostate&quot;) str(prostate) ## &#39;data.frame&#39;: 97 obs. of 10 variables: ## $ lcavol : num -0.58 -0.994 -0.511 -1.204 0.751 ... ## $ lweight: num 2.77 3.32 2.69 3.28 3.43 ... ## $ age : int 50 58 74 58 62 50 64 58 47 63 ... ## $ lbph : num -1.39 -1.39 -1.39 -1.39 -1.39 ... ## $ svi : int 0 0 0 0 0 0 0 0 0 0 ... ## $ lcp : num -1.39 -1.39 -1.39 -1.39 -1.39 ... ## $ gleason: int 6 6 7 6 6 6 6 6 6 6 ... ## $ pgg45 : int 0 0 20 0 0 0 0 0 0 0 ... ## $ lpsa : num -0.431 -0.163 -0.163 -0.163 0.372 ... ## $ train : logi TRUE TRUE TRUE TRUE TRUE TRUE ... 각 피처는 다음과 같습니다. lcavol: 암 부피의 로그 값 lweight: 전립선 무게의 로그 값 age: 환자의 나이 lbph: 전립선 비대 크기의 로그 값 svi: 암 세포가 전립선 바깥에 있는 정낭에 침범했는지를 나타내는 변수, 1 = yes, 0 = no lcp: 암 세포가 전립선 표면에서 얼마나 확장했고, 내부로 얼마나 침투했는지를 나타내는 로그 값 gleason: 암 세포가 얼마나 비정상적으로 보이는지 생체 검사를 통해 병리학자가 2에서 10 사이의 점수를 매긴 값. 이 점수가 높을수록 더 공격적인 암 pgg45: 글래슨 패턴 4 또는 5 (높은 단계의 암) lpsa: PSA의 로그 값 train: 트레이닝, 테스트셋 데이터 여부 먼저 gleason의 분포를 확인해보도록 하겠습니다. table(prostate$gleason) ## ## 6 7 8 9 ## 35 56 1 5 글리슨 점수가 6, 7점에 대부분 모여 있으며, 8점인 것은 1개, 9점인 것은 5개 밖에 없습니다. 해당 데이터 처리를 위해 다음과 같은 선택이 있습니다. 해당 피처를 삭제 점수 8과 9만 삭제 해당 피처를 바꿔 새로운 변수를 만듬 이를 위해 글리슨 점수와 lpsa의 관계를 그림으로 살펴보도록 합니다. library(magrittr) library(ggplot2) prostate %&gt;% ggplot(aes(x = gleason, y = lpsa, group = gleason)) + geom_boxplot() 7~9점의 lpsa가 상당히 크므로, 피처를 남겨두는 것이 좋습니다. 따라서 글리슨 점수가 6점 일 때는 0, 7점 이상인 경우에는 1로 바꾸도록 합니다. prostate$gleason = ifelse(prostate$gleason == 6, 0, 1) table(prostate$gleason) ## ## 0 1 ## 35 62 이번에는 각 피처간의 상관관계를 살펴보도록 합니다. cor(prostate) %&gt;% corrplot.mixed lpsa와 lcavol, lcavol과 lcp, svi와 lcp 사이에는 상관관계가 높아 다중 공선성 문제가 발생할 수 있습니다. 4.2.2 데이터 나누기 다음으로 트레이닝 셋과 테스트 셋을 분리하도록 합니다. train = subset(prostate, train == TRUE)[, 1:9] test = subset(prostate, train == FALSE)[, 1:9] train 열이 TRUE이면 트레이닝 셋, FALSE면 테스트 셋으로 나누어주도록 하며, 마지막 열인 train은 모형에 필요치 않으므로 이를 제외하고 선택해줍니다. 4.2.3 모형화 먼저 최량 부분 집합 회귀를 실시한 후 규제화 기법을 활용하도록 합니다. 4.2.3.1 최량 부분 집합 regsubsets() 함수를 이용해 최량 부분 집합 객체를 만듭니다. subfit = regsubsets(lpsa ~ ., data = train) b.sum = summary(subfit) which.min(b.sum$bic) ## [1] 3 plot(b.sum$bic, type = &#39;l&#39;, xlab = &#39;# of features&#39;, ylab = &#39;BIC&#39;) 세 가지 피처를 사용한 모형이 가장 낮은 BIC를 보입니다. 도표를 통해 좀 더 자세하게 비교하도록 합니다. plot(subfit, scale = &#39;bic&#39;) lcavol, lweight, gleason 3개의 결합에서 가장 낮은 BIC를 보입니다. 이제 해당 모형을 통해 OLS 회귀분석을 실시합니다. ols = lm(lpsa ~ lcavol + lweight + gleason, data = train) plot(ols$fitted.values, train$lpsa, xlab = &#39;Predicted&#39;, ylab = &#39;Actual&#39;) 둘 간에는 선형 관계가 보입니다. 이번에는 predict() 함수를 이용해 해당 모형을 테스트 셋에 적용해보도록 합니다. pred.subfit = predict(ols, newdata = test) plot(pred.subfit, test$lpsa, xlab = &#39;Predicted&#39;, ylab = &#39;Actual&#39;) 마지막으로 MSE를 계산하도록 합니다. resid.subfit = test$lpsa - pred.subfit mse.subfit = mean(resid.subfit ^ 2) print(mse.subfit) ## [1] 0.5084 위의 0.51 값을 기준으로 삼은 후, 규제화 기법과 비교하도록 하겠습니다. 4.2.3.2 Ridge Regression glmnet() 함수를 이용해 Ridge 회귀분석을 수행할 수 있으며, 해당 함수는 입력 피처가 데이터 프레임이 아닌 행렬의 형태여야 합니다. 다음과 같은 형태로 함수를 입력합니다. \\[glmnet(x = 입력 데이터 행렬, y = 반응값, family = 분포 방법, alpha = 0)\\] 이 중 alpha가 0이면 Ridge Regression, 1이면 LASSO 방법으로 분석을 합니다. 먼저 Ridge Regression을 수행합니다. x = as.matrix(train[, 1:8]) y = train[, 9] ridge = glmnet(x, y, family = &#39;gaussian&#39;, alpha = 0) print(ridge) 마지막 100번째 결과를 살펴보면 사용하는 피처의 수가 여전히 8개입니다. 편차의 백분율은 0.6971이고, 람다 값은 0.08789 입니다. plot(ridge, label = TRUE) \\(y\\)축은 계수의 값이고, \\(x\\)축은 L1-norm 입니다. 이번에는 람다 값이 바뀜에 따라 계수의 값이 어떻게 바뀌는지 살펴보도록 합니다. plot(ridge, xvar = &#39;lambda&#39;, label = TRUE) 람다 값이 줄어들수록 벌점이 줄어들고 계수의 절대값이 올라갑니다. 해당 모형을 테스트 셋에 적용해 보도록 합니다. newx = as.matrix(test[, 1:8]) ridge.y = predict(ridge, newx = newx, type = &#39;response&#39;, s = 0.1) plot(ridge.y, test$lpsa, xlab = &#39;Predicted&#39;, ylab = &#39;Actual&#39;, main = &#39;Ridge Regression&#39;) 마지막으로 MSE를 계산하도록 합니다. ridge.resid = ridge.y - test$lpsa ridge.mse = mean(ridge.resid^2) print(ridge.mse) ## [1] 0.4784 최량 부분 집합의 MSE 보다 약간 줄어들었습니다. 4.2.3.3 LASSO glmnet()의 alpha 인자를 1로 변경하면 간단하게 LASSO 분석을 실시할 수 있습니다. lasso = glmnet(x, y, family = &#39;gaussian&#39;, alpha = 1) print(lasso) 모형의 람다 값이 줄어드는 데도 편차가 더 이상 나아지지 않아 69번째에서 멈추게 됩니다. plot(lasso, xvar = &#39;lambda&#39;, label = TRUE) 해당 모델을 테스트 셋에 적용하고 MSE를 구하도록 합니다. lasso.y = predict(lasso, newx = newx, type = &#39;response&#39;, s = 0.045) plot(lasso.y, test$lpsa, xlab = &#39;Predicted&#39;, ylab = &#39;Actual&#39;, main = &#39;LASSO&#39;) lasso.resid = lasso.y - test$lpsa lasso.mse = mean(lasso.resid ^2) print(lasso.mse) ## [1] 0.4437 가장 낮은 MSE를 보입니다. 3가지 모형의 MSE를 비교하면 다음과 같습니다. 표 4.1: 각 모형의 MSE 비교 모형 MSE 최량 부분 집합 0.5084 Ridge 0.4784 LASSO 0.4437 "],
["knn과-svm.html", "Chapter 5 KNN과 SVM 5.1 KNN 5.2 SVM 5.3 데이터 불러오기 및 편집", " Chapter 5 KNN과 SVM 대표적인 분류모형인 KNN과 SVM에 대해 알아보도록 하겠습니다. 5.1 KNN KNN 방법은 가장 가까운 점들, 즉 최근접 이웃들을 들여다보고 적합한 클래스를 결정하는 것입니다. \\(K\\)는 알고리즘이 얼마나 많은 이웃을 검사해야 하는지를 정하는 값이며, 만일 \\(K=5\\)였다면 5개의 가장 가까운 점들을 검사할 것입니다. 새로운 데이터가 들어온 경우, \\(K=3\\) 에서는 주위 3개 데이터를 바탕으로 Class B라 판단합니다. 그러나 \\(K=7\\) 에서는 주위 7개 데이터를 바탕으로 Class A라 판단합니다. 5.2 SVM SVM은 두 개의 데이터 그룹을 가장 잘 나누는 분류기를 찾는 방법입니다. 이 중 관찰값과 Margin이 만나는 부분을 Support Vector라 하며, 두 그룹 가운데의 음영부분(Margin)을 최대화 하는 선을 찾습니다. 일부 데이터의 경우 그룹을 완벽하게 분할할 수 없으므로 약간의 오류를 허용하며, wider margin과 lower total error penalty 간의 트레이드 오프를 최적화한 것을 Soft Margin Classification이라 합니다. 5.3 데이터 불러오기 및 편집 국립 당뇨, 소화기 및 신장병 연구소에서 수집한 데이터를 사용하며, 532개 관찰값과 8개의 입력 피처 그리고 출력은 Y/N 을 갖습니다. 우리가 할 일은 인구 집단에서 당뇨를 앓거나 당뇨 위험 인자를 갖고 있는 개인들의 자료를 검사하고 당뇨병을 예측하는 것입니다. MASS 패키지의 Pima.tr과 Pima.te 데이터를 사용하도록 합니다. library(e1071) # SVM library(MASS) data(Pima.tr) str(Pima.tr) ## &#39;data.frame&#39;: 200 obs. of 8 variables: ## $ npreg: int 5 7 5 0 0 5 3 1 3 2 ... ## $ glu : int 86 195 77 165 107 97 83 193 142 128 ... ## $ bp : int 68 70 82 76 60 76 58 50 80 78 ... ## $ skin : int 28 33 41 43 25 27 31 16 15 37 ... ## $ bmi : num 30.2 25.1 35.8 47.9 26.4 35.6 34.3 25.9 32.4 43.3 ... ## $ ped : num 0.364 0.163 0.156 0.259 0.133 ... ## $ age : int 24 55 35 26 23 52 25 24 63 31 ... ## $ type : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 2 1 1 1 2 ... data(Pima.te) str(Pima.te) ## &#39;data.frame&#39;: 332 obs. of 8 variables: ## $ npreg: int 6 1 1 3 2 5 0 1 3 9 ... ## $ glu : int 148 85 89 78 197 166 118 103 126 119 ... ## $ bp : int 72 66 66 50 70 72 84 30 88 80 ... ## $ skin : int 35 29 23 32 45 19 47 38 41 35 ... ## $ bmi : num 33.6 26.6 28.1 31 30.5 25.8 45.8 43.3 39.3 29 ... ## $ ped : num 0.627 0.351 0.167 0.248 0.158 0.587 0.551 0.183 0.704 0.263 ... ## $ age : int 50 31 21 26 53 51 31 33 27 29 ... ## $ type : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 1 1 2 2 2 2 1 1 2 ... pima = rbind(Pima.tr, Pima.te) 각 피처는 다음과 같습니다. npreg: 임신 횟수 glu: 구강 포도당 내성 검사에서 혈장 포도당 농도 (혈당값) bp: 확장기 혈압 Skin: 삼두근 피하 지방 두께 bmi: 체질량 지수 ped: 당뇨 가족력 함수 age: 연령 type: 당뇨병 여부, Yes 또는 No 당뇨병 여부에 따라 피처들의 특성을 살펴보도록 합니다. library(ggplot2) library(magrittr) library(tidyr) pima %&gt;% gather(key, value, -type) %&gt;% ggplot(aes(x = type, y = value)) + geom_boxplot() + facet_wrap( ~ key, scale = &#39;free&#39;) 모든 데이터의 스케일이 다르므로 표준화를 해주어야 합니다. 표준화 방법에는 크게 두가지 방법이 있습니다. 최소-최대 정규화(min-max normalization): \\(X_{normal} = \\frac{X - min(x)}{max(x) - min(x)}\\) z-점수 표준화(z-score standardization): \\(Z = \\frac{X - \\mu}{\\sigma} = \\frac{X - Mean(x)}{StdDev(x)}\\) 이 중 scale() 함수를 이용하여 z-점수 표준화를 해주도록 합니다. library(dplyr) pima.scale = pima %&gt;% select(-8) %&gt;% scale() %&gt;% data.frame() %&gt;% mutate(type = pima$type) head(pima.scale) ## npreg glu bp skin bmi ped age type ## 1 0.4478 -1.1300 -0.2848 -0.1123 -0.3910 -0.4033 -0.7076 No ## 2 1.0516 2.3862 -0.1223 0.3628 -1.1321 -0.9867 2.1730 Yes ## 3 0.4478 -1.4204 0.8525 1.1229 0.4229 -1.0070 0.3146 No ## 4 -1.0619 1.4184 0.3651 1.3130 2.1813 -0.7081 -0.5217 No ## 5 -1.0619 -0.4526 -0.9346 -0.3974 -0.9432 -1.0738 -0.8005 No ## 6 0.4478 -0.7752 0.3651 -0.2074 0.3938 -0.3627 1.8943 Yes 표준화된 데이터로 다시 그림을 나타내도록 합니다. pima.scale %&gt;% gather(key, value, -type) %&gt;% ggplot(aes(x = type, y = value)) + geom_boxplot() + facet_wrap( ~ key, scale = &#39;free&#39;) 다음으로 각 피처 간 상관관계를 살펴보도록 합니다. library(corrplot) pima.scale %&gt;% dplyr::select(-type ) %&gt;% cor() %&gt;% corrplot.mixed() npreg와 age, skin과 bmi는 상관관계가 높은 편이지만, 제대로 훈련되고 하이퍼 파라미터가 제대로 조정되었을 경우 이런 다중 공선성은 대체로 분류 방법에서는 문제가 되지 않습니다. 트레이딩 셋과 테스트 셋으로 데이터를 나누기 전에 라벨 피처의 비율을 점검하도록 합니다. prop.table(table(pima.scale$type)) ## ## No Yes ## 0.6673 0.3327 No와 Yes의 비중이 대략 7:3 이므로 트레이딩 세트와 테스트 세트를 70/30 비율로 가르도록 합니다. set.seed(502) ind = sample(2, nrow(pima.scale), replace = TRUE, prob = c(0.7, 0.3)) train = pima.scale[ind == 1, ] test = pima.scale[ind == 2, ] str(train) ## &#39;data.frame&#39;: 385 obs. of 8 variables: ## $ npreg: num 0.448 0.448 -0.156 -0.76 -0.156 ... ## $ glu : num -1.42 -0.775 -1.227 2.322 0.676 ... ## $ bp : num 0.852 0.365 -1.097 -1.747 0.69 ... ## $ skin : num 1.123 -0.207 0.173 -1.253 -1.348 ... ## $ bmi : num 0.4229 0.3938 0.2049 -1.0159 -0.0712 ... ## $ ped : num -1.007 -0.363 -0.485 0.441 -0.879 ... ## $ age : num 0.315 1.894 -0.615 -0.708 2.916 ... ## $ type : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 2 2 1 1 1 ... str(test) ## &#39;data.frame&#39;: 147 obs. of 8 variables: ## $ npreg: num 0.448 1.052 -1.062 -1.062 -0.458 ... ## $ glu : num -1.13 2.386 1.418 -0.453 0.225 ... ## $ bp : num -0.285 -0.122 0.365 -0.935 0.528 ... ## $ skin : num -0.112 0.363 1.313 -0.397 0.743 ... ## $ bmi : num -0.391 -1.132 2.181 -0.943 1.513 ... ## $ ped : num -0.403 -0.987 -0.708 -1.074 2.093 ... ## $ age : num -0.7076 2.173 -0.5217 -0.8005 -0.0571 ... ## $ type : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 2 1 2 1 1 1 ... 데이터 점들의 거리 또는 근접 정도를 계산할때는 디폴트로 유클리디안 거리를 사용합니다. 이는 단순히 두 점 A와 B 사이의 직선 거리를 나타냅니다. \\[Euclidean\\ Distance(A, B) = \\sqrt{\\sum_{i=1}^{n}(p_i - q_i)^2}\\] 이는 피처를 측정할 때 사용한 스케일에 매우 종속적이므로 스케일을 표준화하는 일은 매우 중요합니다. 5.3.1 KNN KNN 기법을 사용할 때는 가장 적절한 파라미터(K)를 선택하는 일이 매우 중요합니다. K를 구하기 위해 caret 패키지를 이용하며, 실험을 위해 K의 입력값을 위한 격자망을 2부터 20까지 1씩 증가하도록 만듭니다. expand.grid() 함수와 seq() 함수를 이용하면 쉽게 만들 수 있습니다. grid1 = expand.grid(.k = seq(2, 20, by = 1)) K를 선택하기 위해 caret 패키지의 trainControl() 함수에 교차 검증법을 이용해 control 이라는 오브젝트를 만든다. library(caret) control = trainControl(method = &#39;cv&#39;) caret 패키지의 train() 함수를 이용해 최적의 K 값을 구하는 오브젝트를 생성한다. knn.train = train(type ~ ., data = train, method = &#39;knn&#39;, trControl = control, tuneGrid = grid1) print(knn.train) ## k-Nearest Neighbors ## ## 385 samples ## 7 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 345, 347, 347, 347, 347, 346, ... ## Resampling results across tuning parameters: ## ## k Accuracy Kappa ## 2 0.7357 0.3684 ## 3 0.7692 0.4352 ## 4 0.7510 0.3987 ## 5 0.7615 0.4066 ## 6 0.7614 0.4140 ## 7 0.7589 0.4042 ## 8 0.7693 0.4277 ## 9 0.7745 0.4372 ## 10 0.7720 0.4360 ## 11 0.7667 0.4194 ## 12 0.7799 0.4478 ## 13 0.7691 0.4133 ## 14 0.7847 0.4587 ## 15 0.7848 0.4555 ## 16 0.7797 0.4391 ## 17 0.7795 0.4416 ## 18 0.7692 0.4105 ## 19 0.7746 0.4255 ## 20 0.7615 0.3998 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was k = 15. 위의 오브젝트를 호출하면 최적의 K가 출력됩니다. 최적의 K는 15이며, 이 때 정확도는 0.7795 입니다. 이제 위 모형을 class 패키지의 knn() 함수를 활용하여 테스트 데이터에 적용합니다. 해당 함수는 knn(train, test, cl(training set), k) 형태로 입력합니다. library(class) knn.test = knn(train[, -8], test[, -8], train[, 8], k = 15) print(knn.test) ## [1] No Yes No No No Yes Yes No No No Yes No No Yes No No Yes No ## [19] No Yes No No No No Yes No No No No Yes No No No Yes No Yes ## [37] No No No No No No Yes Yes Yes No No No No No No No No No ## [55] Yes Yes No No Yes No Yes Yes Yes No No No No No No No No No ## [73] No No Yes No Yes No No Yes Yes No No No Yes No No No No No ## [91] No No No Yes Yes No No No No No No No No No No No No No ## [109] No Yes Yes Yes No Yes No No Yes No Yes No Yes No Yes No Yes Yes ## [127] No No Yes No Yes No No No No No No Yes No No No Yes No Yes ## [145] Yes No Yes ## Levels: No Yes caret::confusionMatrix(knn.test, test$type) ## Confusion Matrix and Statistics ## ## Reference ## Prediction No Yes ## No 76 27 ## Yes 17 27 ## ## Accuracy : 0.701 ## 95% CI : (0.62, 0.773) ## No Information Rate : 0.633 ## P-Value [Acc &gt; NIR] : 0.0505 ## ## Kappa : 0.33 ## ## Mcnemar&#39;s Test P-Value : 0.1748 ## ## Sensitivity : 0.817 ## Specificity : 0.500 ## Pos Pred Value : 0.738 ## Neg Pred Value : 0.614 ## Prevalence : 0.633 ## Detection Rate : 0.517 ## Detection Prevalence : 0.701 ## Balanced Accuracy : 0.659 ## ## &#39;Positive&#39; Class : No ## 정확도가 0.7007로써, 기존 트레이닝 셋의 정확도인 0.7795에 비해 다소 감소하였습니다. 5.3.2 SVM SVM 모형화를 위해서는 e1071 패키지의 tune.svm() 함수를 이용하도록 합니다. library(e1071) set.seed(123) linear.tune = tune.svm(type ~ ., data = train, kernel = &#39;linear&#39;, cost = c(0.001, 0.01, 01, 1, 5, 10)) summary(linear.tune) ## ## Parameter tuning of &#39;svm&#39;: ## ## - sampling method: 10-fold cross validation ## ## - best parameters: ## cost ## 0.01 ## ## - best performance: 0.2 ## ## - Detailed performance results: ## cost error dispersion ## 1 0.001 0.3192 0.04699 ## 2 0.010 0.2000 0.04579 ## 3 1.000 0.2076 0.06253 ## 4 1.000 0.2076 0.06253 ## 5 5.000 0.2103 0.06322 ## 6 10.000 0.2103 0.06322 cost는 데이터를 잘못 분류하는 선을 긋게 될 경우 얼마만큼의 비용을 지불할 것인지를 지정합니다. SVM은 1) 데이터를 한 가운데로 얼마나 잘 나누는지와 2) 잘못 구분한 점으로 인한 비용의 합을 최소화하는 선을 찾습니다. 결과적으로 SVM은 cost를 사용해 과적합 정도를 조절하게 됩니다. 위 예제에서 최적의 cost 함수는 0.01로 나타났고, 분류 오류 비율은 대략 20% 정도입니다. best.linear = linear.tune$best.model tune.test = predict(best.linear, newdata = test) caret::confusionMatrix(tune.test, test$type) ## Confusion Matrix and Statistics ## ## Reference ## Prediction No Yes ## No 82 24 ## Yes 11 30 ## ## Accuracy : 0.762 ## 95% CI : (0.685, 0.828) ## No Information Rate : 0.633 ## P-Value [Acc &gt; NIR] : 0.000562 ## ## Kappa : 0.461 ## ## Mcnemar&#39;s Test P-Value : 0.042522 ## ## Sensitivity : 0.882 ## Specificity : 0.556 ## Pos Pred Value : 0.774 ## Neg Pred Value : 0.732 ## Prevalence : 0.633 ## Detection Rate : 0.558 ## Detection Prevalence : 0.721 ## Balanced Accuracy : 0.719 ## ## &#39;Positive&#39; Class : No ## 테스트 셋을 대상으로 정확도가 0.7619로써 knn 대비 약간 높은 정확도를 보입니다. "],
["cart.html", "Chapter 6 CART 6.1 의사결정나무 6.2 회귀 트리 6.3 분류 트리 6.4 익스트림 그레디언트 부스트 기법 (XGboost)", " Chapter 6 CART 6.1 의사결정나무 의사결정나무는 특정 항목에 대한 의사 결정 규칙을 나무 형태로 분류해 나가는 분석 기법을 말합니다. 예를 들어, 타이타닉 호 탑승자의 성별, 나이, 자녀의 수를 이용해서 생존 확률을 아래와 같이 구분해 나갑니다. 의사결정나무의 가장 큰 장점은 분석 과정을 실제로 눈으로 관측할 수 있으므로 직관적이고 이해하기 쉽다는 점입니다. 또한 수치형/범주형 변수를 모두 사용할 수 있다는 점, 계산 비용이 낮아 대규모의 데이터 셋에서도 비교적 빠르게 연산이 가능하다는 장점이 있습니다. 의사결정나무 분석 방법에는 통계학에 기반한 (카이스퀘어, T검정, F검정 등을 사용한) CART 및 CHAID 알고리즘이나, 기계학습 계열인(엔트로피, 정보 이득 등을 사용한) ID3, C4.5, C5.0 등의 알고리즘이 존재하며, 일반적으로 CART를 많이 사용합니다. 6.1.1 랜덤 포레스트 모형의 예측력을 높이기 위해서는 많은 트리를 만들고 결과를 결합하면 되며, 이를 랜덤 포레스트라 합니다. 랜덤 포레스트 기법에서는 크게 두 가지 방법을 사용합니다. 부트스트랩(bootstrap aggregation) 혹은 배깅(bagging)은 전체 관찰값 3분의 2 정도의 데이터 집합에서 무작위로 샘플을 선정해 트리는 만듭니다. 개별 분할에서 입력 피처를 무작위로 선정하는 방법입니다. R 패키지에서는 기본적으로 회귀 분석 문제를 풀 때는 전체 예측 변수 수를 3으로 나눈 값(\\(n / 3\\))을, 분류 문제를 풀 때는 전체 예측 변수 수의 제곱근 한 값을 (\\(\\sqrt{n}\\)) 사용합니다. 6.1.2 익스트림 그레디언트 부스트 기법 (XGboost) 부스트 기법은 기본 모형을 만든 후 잔차를 검사하고 손실함수에 맞춰 해당 잔차를 바탕으로 모형을 적합하게 만들며, 이는 특정 기준에 이를 때까지 계속 반복하는 방법입니다. 예를 들어 학생이 연습 시험을 봐서 100문제 중에 30문제를 틀린 경우 해당 30문제만 다시 공부하며, 다음 시험에서 30문제 중 10개를 틀리면 다시 10문제만 집중하는 과정입니다. 6.2 회귀 트리 먼저 CART 모형을 이용하여 회귀 트리를 만드는 법을 살펴보도록 하겠습니다. 6.2.1 데이터 불러오기 및 편집 이전에 사용한 암-전립선암 데이터를 불러옵니다. library(ElemStatLearn) data(prostate) prostate$gleason = ifelse(prostate$gleason == 6, 0, 1) pros.train = subset(prostate, train == TRUE)[, 1:9] pros.test = subset(prostate, train == FALSE)[, 1:9] gleason이 6이면 0, 그렇지 않으면 1로 변형해주며, 트레인과 테스트 셋을 각각 나누어줍니다. 6.2.2 모형화 R에서는 rpart 패키지의 rpart() 함수를 이용해 회귀 트리를 만들 수 있습니다. set.seed(123) library(rpart) tree.pros = rpart(lpsa ~ ., data = pros.train) print(tree.pros) ## n= 67 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 67 96.280 2.452 ## 2) lcavol&lt; 1.051 25 24.820 1.522 ## 4) lcavol&lt; -0.4786 8 5.374 0.546 * ## 5) lcavol&gt;=-0.4786 17 8.234 1.981 * ## 3) lcavol&gt;=1.051 42 36.950 3.006 ## 6) lcavol&lt; 2.792 34 21.290 2.749 ## 12) lweight&lt; 3.463 12 7.788 2.222 * ## 13) lweight&gt;=3.463 22 8.349 3.036 ## 26) age&gt;=65 15 5.694 2.891 * ## 27) age&lt; 65 7 1.660 3.347 * ## 7) lcavol&gt;=2.792 8 3.820 4.101 * lpsa를 나누는 기준의 트리가 생성되었습니다. 이를 보기 쉽게 그림으로 나타내보도록 하겠습니다. library(rpart.plot) rpart.plot(tree.pros) 해당 모형을 통한 회귀분석을 통해 MSE을 구하도록 합니다. library(magrittr) party.test = predict(tree.pros, newdata = pros.test) party.resid = (party.test - pros.test$lpsa) %&gt;% .^2 %&gt;% mean() print(party.resid) ## [1] 0.6136 MSE 값이 0.6136 입니다. 6.2.3 프루닝(가지치기) 가지수가 많아질수록 트레이닝 셋에서의 설명력은 높아지지만, 오버피팅이 발생할 수 있으므로 적절하게 가지수를 제한할 필요가 있습니다. 먼저 cptable을 통해 모델의 각종 결과값을 확인하고 이를 통해 적절한 가지수를 찾도록 합니다. print(tree.pros$cptable) ## CP nsplit rel error xerror xstd ## 1 0.35852 0 1.0000 1.0196 0.17964 ## 2 0.12296 1 0.6415 0.8742 0.12878 ## 3 0.11640 2 0.5185 0.7949 0.10420 ## 4 0.05351 3 0.4021 0.7905 0.09822 ## 5 0.01033 4 0.3486 0.7044 0.09116 ## 6 0.01000 5 0.3383 0.7322 0.09382 cptable의 내용은 다음과 같다. CP: 비용 복잡도(complexity parameter) nsplit: 트리의 분할 횟수 rel error: 분할 횟수에 다른 RSS의 값을 분할하지 않았을 때의 RSS 값으로 나눔. \\(RSS(k) / RSS(0)\\) xerror: 10겹 교차검증의 평균오차 xstd: 10겹 교차검증의 표준편차 plotcp() 함수를 이용해 이를 그림으로 나타낼 수도 있습니다. plotcp(tree.pros) 일반적으로 선 하단에 위치한 포인트 중 가장 왼쪽에 위치한 값을 선택하는 것이 좋습니다. 오른쪽으로 갈수록 Error율은 낮아지지만 가지수가 많아지기 때문입니다. 위 그림을 통해 CP가 4(3번 분할)을 선택하도록 합니다. cp = min(tree.pros$cptable[4, ]) # select cp prune.tree.pros = prune(tree.pros, cp) prune() 함수를 이용해 가지를 친 트리를 만들 수 있습니다. rpart.plot(prune.tree.pros) 가치지기를 통해 lcavol 변수만 남게 되었습니다. 해당 모형을 테스트 셋에 적용해보도록 합니다. party.pros.test = predict(prune.tree.pros, newdata = pros.test) rpart.resid = (party.pros.test - pros.test$lpsa) %&gt;% .^2 %&gt;% mean() print(rpart.resid) ## [1] 0.5145 MSE가 0.5145로써, 가지치기 전 대비 다소 줄어들었습니다. 6.2.4 랜덤 포레스트: 회귀 트리 먼저 회귀분석 트리를 대상으로 랜덤 포레스트 방법을 적용합니다. R에서는 randomForest() 함수를 이용해 랜덤 포레스트 모델을 만들 수 있습니다. library(randomForest) set.seed(123) rf.pros = randomForest(lpsa ~., data = pros.train) rf.pros ## ## Call: ## randomForest(formula = lpsa ~ ., data = pros.train) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 2 ## ## Mean of squared residuals: 0.6937 ## % Var explained: 51.73 랜덤 포레스트 기법에서는 매 분할마다 2개의 변수를 샘플링해 500개의 각기 다른 트리를 생성하며, MSE는 0.6937 입니다. 트리가 너무 많아지면 과적합이 발생할 수도 있으므로, rf.pros를 그림으로 그려보도록 합니다. plot(rf.pros) 트리수가 100개가 넘어가면 MSE에 큰 변화가 없는 모습입니다. which.min() 함수를 통해 최적의 트리를 찾아보도록 합니다. which.min(rf.pros$mse) ## [1] 80 set.seed(123) rf.pros.2 = randomForest(lpsa ~ ., data = pros.train, ntree = which.min(rf.pros$mse)) rf.pros.2 ## ## Call: ## randomForest(formula = lpsa ~ ., data = pros.train, ntree = which.min(rf.pros$mse)) ## Type of random forest: regression ## Number of trees: 80 ## No. of variables tried at each split: 2 ## ## Mean of squared residuals: 0.6567 ## % Var explained: 54.31 MSE가 가장 낮은 80개의 트리로 랜덤포레스트를 수행한 결과, MSE가 0.6567로 개선되었습니다. 다음으로 변수의 중요도를 살펴보도록 한다. varImpPlot(rf.pros.2, scale = T, main = &#39;Variance Importance Plot - PSA Score&#39;) importance(rf.pros.2) ## IncNodePurity ## lcavol 25.012 ## lweight 15.822 ## age 7.167 ## lbph 5.471 ## svi 8.498 ## lcp 8.114 ## gleason 4.990 ## pgg45 6.664 위 모델(트리 갯수를 줄인 모댈)을 테스트 셋에 적용해보도록 합니다. rf.pros.test = predict(rf.pros.2, newdata = pros.test) rf.resid = (rf.pros.test - pros.test$lpsa) %&gt;% .^2 %&gt;% mean() print(rf.resid) ## [1] 0.5513 MSE 값이 기존에 비해 훨씬 줄어들었습니다. 표 6.1: 각 모형의 MSE 비교 모형 MSE 기본 트리 0.6136 가지치기 0.5145 랜덤 포레스트 0.6937 랜덤 포레스트 2 0.6567 6.3 분류 트리 이번에는 CART 모형을 이용하여 분류 트리를 만드는 법을 살펴보도록 하겠습니다. 6.3.1 데이터 불러오기 및 편집 위스콘신 유방암 데이터를 불러온 후, 데이터를 나누도록 합니다. library(MASS) data(biopsy) biopsy$ID = NULL names(biopsy) = c(&#39;thick&#39;, &#39;u.size&#39;, &#39;u.shape&#39;, &#39;adhsn&#39;, &#39;s.size&#39;, &#39;nucl&#39;, &#39;chrom&#39;, &#39;n.nuc&#39;, &#39;mit&#39;, &#39;class&#39;) biopsy.v2 = na.omit(biopsy) set.seed(123) ind = sample(2, nrow(biopsy.v2), replace = TRUE, prob = c(0.7, 0.3)) biop.train = biopsy.v2[ind==1, ] biop.test = biopsy.v2[ind==2, ] rpart() 함수를 이용해 트리 모형을 수행합니다. class가 benign과 malignant로 구성된 factor 형태이므로, 분류 트리가 만들어집니다. set.seed(123) tree.biop = rpart(class ~ ., data = biop.train) print(tree.biop) ## n= 474 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 474 172 benign (0.63713 0.36287) ## 2) u.size&lt; 3.5 321 27 benign (0.91589 0.08411) ## 4) nucl&lt; 4.5 294 7 benign (0.97619 0.02381) * ## 5) nucl&gt;=4.5 27 7 malignant (0.25926 0.74074) ## 10) thick&lt; 3.5 8 3 benign (0.62500 0.37500) * ## 11) thick&gt;=3.5 19 2 malignant (0.10526 0.89474) * ## 3) u.size&gt;=3.5 153 8 malignant (0.05229 0.94771) * cptable을 살펴보도록 합니다. tree.biop$cptable ## CP nsplit rel error xerror xstd ## 1 0.79651 0 1.0000 1.0000 0.06086 ## 2 0.07558 1 0.2035 0.2616 0.03710 ## 3 0.01163 2 0.1279 0.1512 0.02882 ## 4 0.01000 3 0.1163 0.1512 0.02882 plotcp(tree.biop) cptable과 그림을 살펴본 결과, cp가 3(2번분할) 한 모델을 적용하도록 합니다. cp = min(tree.biop$cptable[3, ]) prune.tree.biop = prune(tree.biop, cp = cp) rpart.plot(prune.tree.biop) 트리 결과를 살펴보면 u.size가 첫 번째 분할이며, nuci가 두 번째 분할 기준입니다. 위 모델을 test 데이터에 적용해보도록 합니다. rpart.test = predict(prune.tree.biop, newdata = biop.test, type = &#39;class&#39;) caret::confusionMatrix(rpart.test, biop.test$class) ## Confusion Matrix and Statistics ## ## Reference ## Prediction benign malignant ## benign 136 3 ## malignant 6 64 ## ## Accuracy : 0.957 ## 95% CI : (0.92, 0.98) ## No Information Rate : 0.679 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.902 ## ## Mcnemar&#39;s Test P-Value : 0.505 ## ## Sensitivity : 0.958 ## Specificity : 0.955 ## Pos Pred Value : 0.978 ## Neg Pred Value : 0.914 ## Prevalence : 0.679 ## Detection Rate : 0.651 ## Detection Prevalence : 0.665 ## Balanced Accuracy : 0.956 ## ## &#39;Positive&#39; Class : benign ## 두 번의 분할 트리 모형만으로도 95.69%의 정확도를 얻을 수 있습니다. 변수의 중요도를 살펴보도록 합니다. 6.3.2 랜덤 포레스트: 분류 트리 randomForest() 함수를 이용해 랜덤 포레스트 모델을 적용합니다. set.seed(123) rf.biop = randomForest(class ~ ., data = biop.train) rf.biop ## ## Call: ## randomForest(formula = class ~ ., data = biop.train) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 3 ## ## OOB estimate of error rate: 3.38% ## Confusion matrix: ## benign malignant class.error ## benign 294 8 0.02649 ## malignant 8 164 0.04651 OOB(out of bag) 오차율은 0.0338 로 나타납니다. 다음으로 트리 수에 따른 오차를 그린 후, err.rate가 최소가 되는 트리 갯수를 찾도록 합니다. plot(rf.biop) which.min(rf.biop$err.rate[, 1]) ## [1] 125 해당 트리 갯수 만큼 랜덤 포레스트를 다시 만듭니다. set.seed(123) rf.biop.2 = randomForest(class ~., data = biop.train, ntree = which.min(rf.biop$err.rate[, 1])) print(rf.biop.2) ## ## Call: ## randomForest(formula = class ~ ., data = biop.train, ntree = which.min(rf.biop$err.rate[, 1])) ## Type of random forest: classification ## Number of trees: 125 ## No. of variables tried at each split: 3 ## ## OOB estimate of error rate: 2.95% ## Confusion matrix: ## benign malignant class.error ## benign 294 8 0.02649 ## malignant 6 166 0.03488 에러가 0로 기존에 비해 감소하였습니다. 이제 해당 모델을 테스트 셋에 적용합니다. rf.biop.test = predict(rf.biop.2, newdata = biop.test, type = &#39;class&#39;) caret::confusionMatrix(rf.biop.test, biop.test$class) ## Confusion Matrix and Statistics ## ## Reference ## Prediction benign malignant ## benign 138 0 ## malignant 4 67 ## ## Accuracy : 0.981 ## 95% CI : (0.952, 0.995) ## No Information Rate : 0.679 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.957 ## ## Mcnemar&#39;s Test P-Value : 0.134 ## ## Sensitivity : 0.972 ## Specificity : 1.000 ## Pos Pred Value : 1.000 ## Neg Pred Value : 0.944 ## Prevalence : 0.679 ## Detection Rate : 0.660 ## Detection Prevalence : 0.660 ## Balanced Accuracy : 0.986 ## ## &#39;Positive&#39; Class : benign ## test 데이터를 상대로 98% 이상의 정확도를 나타내며, 단일 트리에 훨씬 개선된 성과를 보입니다. 이처럼 랜덤 포레스트는 회귀 트리 보다는 분류 트리에서 더욱 뛰어난 성능 개선을 보입니다. 변수의 중요도를 살펴보도록 합니다. varImpPlot(rf.biop.2, scale = T, main = &#39;Variance Importance Plot - PSA Score&#39;) importance(rf.biop.2) ## MeanDecreaseGini ## thick 10.301 ## u.size 57.177 ## u.shape 46.932 ## adhsn 4.425 ## s.size 25.199 ## nucl 34.220 ## chrom 18.064 ## n.nuc 21.229 ## mit 1.103 6.4 익스트림 그레디언트 부스트 기법 (XGboost) 6.4.1 데이터 불러오기 및 편집 먼저 피마 인디어 당뇨병 모형 데이터를 불러온 후, 트레이닝과 테스트 셋으로 나눠주도록 합니다. data(Pima.tr) data(Pima.te) pima = rbind(Pima.tr, Pima.te) set.seed(502) ind = sample(2, nrow(pima), replace = TRUE, prob = c(0.7, 0.3)) pima.train = pima[ind == 1, ] pima.test = pima[ind == 2, ] 6.4.2 랜덤 포레스트 먼저 비교를 위해 랜덤포레스트를 이용한 분류 모형을 만들도록 합니다. set.seed(321) rf.pima = randomForest(type ~ ., data = pima.train) rf.pima ## ## Call: ## randomForest(formula = type ~ ., data = pima.train) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 2 ## ## OOB estimate of error rate: 20.26% ## Confusion matrix: ## No Yes class.error ## No 235 27 0.1031 ## Yes 51 72 0.4146 20% 가량의 오분류가 발생하였습니다. 트리 크기 최적화를 통한 성능 개선여부를 살펴보도록 합니다. which.min(rf.pima$err.rate[, 1]) ## [1] 88 set.seed(321) rf.pima.2 = randomForest(type ~ ., data = pima.train, ntree = which.min(rf.pima$err.rate[, 1])) print(rf.pima.2) ## ## Call: ## randomForest(formula = type ~ ., data = pima.train, ntree = which.min(rf.pima$err.rate[, 1])) ## Type of random forest: classification ## Number of trees: 88 ## No. of variables tried at each split: 2 ## ## OOB estimate of error rate: 19.74% ## Confusion matrix: ## No Yes class.error ## No 236 26 0.09924 ## Yes 50 73 0.40650 오류율이 역시 20% 정도로써 모델이 크게 개선되지는 않습니다. 해당 모형을 테스트 셋에 적용해보도록 합니다. rf.pima.test = predict(rf.pima.2, newdata = pima.test, type = &#39;class&#39;) caret::confusionMatrix(rf.pima.test, pima.test$type) ## Confusion Matrix and Statistics ## ## Reference ## Prediction No Yes ## No 74 17 ## Yes 19 37 ## ## Accuracy : 0.755 ## 95% CI : (0.677, 0.822) ## No Information Rate : 0.633 ## P-Value [Acc &gt; NIR] : 0.00105 ## ## Kappa : 0.477 ## ## Mcnemar&#39;s Test P-Value : 0.86763 ## ## Sensitivity : 0.796 ## Specificity : 0.685 ## Pos Pred Value : 0.813 ## Neg Pred Value : 0.661 ## Prevalence : 0.633 ## Detection Rate : 0.503 ## Detection Prevalence : 0.619 ## Balanced Accuracy : 0.740 ## ## &#39;Positive&#39; Class : No ## 75% 가량의 정확도를 보입니다. 이처럼 해당 데이터에는 랜덤포레스트 기법을 적용하여도 모델이 크게 개선되지 않습니다. 6.4.3 XGboost 모형 만들기 XGboost 모형 적용을 위해, 먼저 다음과 같이 그리드를 만들도록 합니다. grid = expand.grid( nrounds = c(75, 100), colsample_bytree = 1, min_child_weight = 1, eta = c(0.01, 0.1, 0.3), gamma = c(0.5, 0.25), subsample = 0.5, max_depth = c(2,3) ) 위에서 입력한 인자값의 내용은 다음과 같습니다. nrounds: 최대 반복 횟수(최종 모형에서의 트리 수) colsample_bytree: 트리를 생성할 때 표본 추출한 피처 수(비율로 표시), 기본값은 1(피처 수의 100%) min_child_weight: 부스트되는 트리에서 최소 가중값. 기본값은 1 eta: 학습 속도. 해법에 관한 각 트리의 기여도를 의미. 기본값은 0.3 gamma: 트리에서 다른 리프 분할을 하기 위해 필요한 최소 손실 감소(minimum loss reduction) subsample: 데이터 관찰값의 비율. 기본값은 1(100%) max_depth: 개별 트리의 최대 깊이 다음은 trainControl() 함수를 이용하여 인자를 지정합니다. library(caret) cntrl = trainControl( method = &#39;cv&#39;, number = 5, verboseIter = FALSE, # TRUE 설정시 과정이 보임 returnData = FALSE, returnResamp = &#39;final&#39; ) 먼저 최적화된 인자를 구하도록 합니다. trControl와 tuneGrid는 위에서 입력한 인자를 입력한다. set.seed(1) train.xgb = train( x = pima.train[, 1:7], y = pima.train[, 8], trControl = cntrl, tuneGrid = grid, method = &#39;xgbTree&#39; ) print(train.xgb) 모형 생성을 위한 최적 인자들의 조합이 출력됩니다. 다음으로 xgb.train() 함수에서 사용할 인자 목록(param)에 위에서 출력된 값을 입력합니다. 그 후 데이터 프레임을 입력 피처의 행렬을 x로, 레이블을 0과 1로 변환한 값을 y로 입력한 후, x와 y를 xgb.Dmatrix() 함수에서 입력값으로 사용합니다. library(xgboost) param = list( objective = &#39;binary:logistic&#39;, eval_metric = &#39;error&#39;, eta = 0.01, max_depth = 2, subsample = 0.5, comsample_byree = 1, gamma = 0.5 ) x = as.matrix(pima.train[, 1:7]) y = ifelse(pima.train$type == &#39;Yes&#39;, 1, 0) train.mat = xgb.DMatrix(data = x, label = y) 다음으로 모형을 만들도록 합니다. param 인자에는 위에서 입력한 param을, data 인자에는 위에서 만든 train.mat을 입력합니다. set.seed(1) xgb.fit = xgb.train(params = param, data = train.mat, nrounds = 75) xgb.importance() 함수를 이용해 변수의 중요도를 살펴보록 합니다. impMatrix = xgb.importance(feature_names = dimnames(x)[[2]], model = xgb.fit) print(impMatrix) ## Feature Gain Cover Frequency ## 1: glu 0.67995 0.52640 0.41553 ## 2: age 0.14210 0.17592 0.19178 ## 3: ped 0.08877 0.14562 0.18721 ## 4: bmi 0.03496 0.04465 0.06393 ## 5: npreg 0.03104 0.05888 0.06849 ## 6: bp 0.01182 0.02899 0.03196 ## 7: skin 0.01136 0.01954 0.04110 xgb.plot.importance(impMatrix, main = &#39;Gain by Feature&#39;) 다음으로 수행 결과를 살펴보도록 합니다. pred = predict(xgb.fit, x) pred.bi = ifelse(pred &gt; 0.5, 1, 0) caret::confusionMatrix(as.factor(y), as.factor(pred.bi)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 243 19 ## 1 49 74 ## ## Accuracy : 0.823 ## 95% CI : (0.782, 0.86) ## No Information Rate : 0.758 ## P-Value [Acc &gt; NIR] : 0.001319 ## ## Kappa : 0.566 ## ## Mcnemar&#39;s Test P-Value : 0.000437 ## ## Sensitivity : 0.832 ## Specificity : 0.796 ## Pos Pred Value : 0.927 ## Neg Pred Value : 0.602 ## Prevalence : 0.758 ## Detection Rate : 0.631 ## Detection Prevalence : 0.681 ## Balanced Accuracy : 0.814 ## ## &#39;Positive&#39; Class : 0 ## 82%의 정확도를 보입니다. library(InformationValue) optim = optimalCutoff(y, pred) print(optim) ## [1] 0.4342 optimalCutoff() 함수를 사용하여 로지스틱 함수에서 최적의 cut off 지점을 찾을 수도 있습니다. 즉 0.5를 기준으로 분류하는 것이 아닌, 0.4342을 기준으로 분류할 때 더욱 뛰어난 성과를 보입니다. 해당 모형을 테스트 셋에 적용해봅니다. pima.testMat = as.matrix(pima.test[, 1:7]) xgb.pima.test = predict(xgb.fit, pima.testMat) y.test = ifelse(pima.test$type == &#39;Yes&#39;, 1, 0) 1 - misClassError(y.test, xgb.pima.test, threshold = optim) ## [1] 0.7279 테스트 데이터에 해당 모델을 적용한 결과는 위와 같다. cut off 지점을 0.4342으로 하였을 경우 27% 가량의 오차가 발생하여, 73% 가량의 정확도를 보입니다. plotROC(y.test, xgb.pima.test) plotROC() 함수를 이용해 ROC 및 AUC를 확인해보면, AUC가 0.8 정도로 계산됩니다. "],
["pca.html", "Chapter 7 PCA 7.1 주성분분석(PCA) 7.2 iris 데이터 분석 7.3 북미 프로 아이스하키 리그 데이터 분석", " Chapter 7 PCA 7.1 주성분분석(PCA) PCA는 데이터의 분산(variance)을 최대한 보존하면서 서로 직교하는 새 기저(축)를 찾아, 고차원 공간의 표본들을 선형 연관성이 없는 저차원 공간으로 변환하는 기법입니다. 그림에서 알 수 있듯이, 2차원 공간에 있는 데이터들이 하나의 주성분(PC1)을 새로운 기저로 선형변환된 걸 확인할 수 있습니다. 여기에서 1시 방향의 사선축이 원 데이터의 분산을 최대한 보존하는 새로운 기저입니다. 두 번째 성분은 첫 번째 성분의 방향에 직각인 방향으로 분산을 최대화하는 선형 결합을 선택하는 식으로 만들며, 그 후 성분들도 변수 수만큼 같은 방법으로 만듭니다. PCA의 목적은 바로 이런 축을 찾는 데 있습니다. 7.2 iris 데이터 분석 먼저 R의 기본 데이터인 iris 데이터를 통해 PCA 분석을 해보도록 합니다. 7.2.1 데이터 불러오기 library(magrittr) library(corrplot) data(&quot;iris&quot;) iris.scale = iris[, 1:4] %&gt;% scale() iris.scale %&gt;% cor() %&gt;% corrplot.mixed() 먼저 데이터를 평균 0, 표준편차 1로 표준화한 후, 상관관계를 구합니다. Petal.Width와 Petal.Length 간, Sepal.Length와 Petal.Length 간에 높은 상관관계가 있습니다. 7.2.2 모형화 PCA는 다음 단계를 거친다. 성분 추출 및 남길 성분의 수 결정 남은 성분을 회전 회전된 결과를 해석 요인 점수를 생성 요인 점수를 입력 변수로 사용해 회귀 분석을 하고, 테스트 데이터에 관한 평가 R에서는 기본함수인 prcomp(), princomp() 함수를 통해 PCA 분석을 수행할 수 있으며, psych 패키지를 통해 더욱 다양한 분석을 할 수 있습니다. 먼저 기본함수인 prcomp() 함수를 이용해 성분을 추출합니다. pca.iris = prcomp(iris.scale) summary(pca.iris) ## Importance of components: ## PC1 PC2 PC3 PC4 ## Standard deviation 1.71 0.956 0.3831 0.14393 ## Proportion of Variance 0.73 0.229 0.0367 0.00518 ## Cumulative Proportion 0.73 0.958 0.9948 1.00000 Cumulative Proportion을 살펴보면, PC1 만으로도 73% 가량의 분산을 설명하며, PC 2까지는 95%의 분산을 설명합니다. 몇번째 성분까지 사용하는 것이 좋은지를 판단하기 위해 screeplot을 그려주도록 합니다. screeplot(pca.iris, type = &quot;l&quot;) 흔히 기울기가 달라지며 꺽이는 지점을 Elbow Point라고 부르는데 보통 이 부분의 PC까지를 사용해서 변수를 축소합니다. 2개 성분만으로도 충분하며, 3개 성분으로 대부분을 설명할 수 있습니다. 이번에는 biplot() 함수를 이용해 행렬도를 그려보도록 합니다. biplot(pca.iris) 위 그림은 각 개체에 대한 첫번째, 두번째 성분에 대한 점수 및 행렬도를 나타낸 것으로써, 가까운 거리와 방향일수록 변수들의 상관성이 높습니다. 마지막으로 각 성분에 대한 기여도를 출력합니다. pca.iris$rotation ## PC1 PC2 PC3 PC4 ## Sepal.Length 0.5211 -0.37742 0.7196 0.2613 ## Sepal.Width -0.2693 -0.92330 -0.2444 -0.1235 ## Petal.Length 0.5804 -0.02449 -0.1421 -0.8014 ## Petal.Width 0.5649 -0.06694 -0.6343 0.5236 7.3 북미 프로 아이스하키 리그 데이터 분석 북미 프로 아이스하키 리그에 관한 데이터를 분석합니다. 7.3.1 데이터 불러오기 해당 데이터는 https://github.com/datameister66/data 에서 nhlTrain.csv와 nhlTest.csv 데이터를 다운받도록 합니다. train = read.csv(&#39;https://raw.githubusercontent.com/datameister66/data/master/NHLtrain.csv&#39;) test = read.csv(&#39;https://raw.githubusercontent.com/datameister66/data/master/NHLtest.csv&#39;) 각 피처는 다음과 같습니다. Team: 팀의 연고지 ppg: 게임당 점수의 평균 Goals_For: 팀의 경기당 평균 득점 Goals_Against: 팀의 경기당 평균 실점 Shots_For: 경기당 팀의 골 근처에서 슛을 한 횟수 Shots_Against: 경기당 팀이 골 근처에서 상대팀의 슛을 허용한 횟수 PP_perc: 파워플레이 상황에서 팀이 득점한 퍼센트 PK_perc: 상대팀이 파워플레이 상황일 때 실점을 하지 않은 시간의 퍼센트 CF60_pp: 팀의 파워플레이 60분당 Corsi 점수. Corsi 점수는 Shots_For와 상대에게 막힌 것이나 네트를 벗어난 것의 개수를 합한 것이다. CA60_sh: 상대 팀의 파워플레이 60분당 Corsi 점수 OZFOperc_pp: 팀이 파워플레이 상황일 때 공격자 지역에서 시합이 재개된 퍼센트 Give: 팀이 경기당 퍽을 준 평균 횟수 Take: 팀이 경기당 퍽을 가져온 평균 횟수 hits: 팀의 경기당 보디체크 평균 횟수 blks: 팀의 경기당 상대방 슛을 블로키한 횟수의 평균 데이터를 평균 0, 표준편차 1로 표준화한 후, 상관관계를 구하도록 합니다. library(magrittr) train.scale = scale(train[, -1:-2]) nhl.cor = cor(train.scale) nhl.cor %&gt;% corrplot() 7.3.2 성분 추출 library(psych) pca = principal(train.scale, rotate = &#39;none&#39;) plot(pca$values, type = &#39;b&#39;, ylab = &#39;Eigenvalues&#39;, xlab = &#39;Component&#39;) psych 패키지의 principal() 함수를 통해 성분을 추출하도록 합니다. 5개 성분만으로도 충분한 설명력이 있는 것으로 보입니다. 7.3.3 직각 회전과 해석 회전의 목적은 특정한 성분에 관해 변수의 기여도를 최대화함으로써 각 성분 사이의 상관 관계를 줄여 해석을 간단학 하는 것입니다. pca.rotate = principal(train.scale, nfactors = 5, rotate = &#39;varimax&#39;) print(pca.rotate) ## Principal Components Analysis ## Call: principal(r = train.scale, nfactors = 5, rotate = &quot;varimax&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## RC1 RC2 RC5 RC3 RC4 h2 u2 com ## Goals_For -0.21 0.82 0.21 0.05 -0.11 0.78 0.22 1.3 ## Goals_Against 0.88 -0.02 -0.05 0.21 0.00 0.82 0.18 1.1 ## Shots_For -0.22 0.43 0.76 -0.02 -0.10 0.81 0.19 1.8 ## Shots_Against 0.73 -0.02 -0.20 -0.29 0.20 0.70 0.30 1.7 ## PP_perc -0.73 0.46 -0.04 -0.15 0.04 0.77 0.23 1.8 ## PK_perc -0.73 -0.21 0.22 -0.03 0.10 0.64 0.36 1.4 ## CF60_pp -0.20 0.12 0.71 0.24 0.29 0.69 0.31 1.9 ## CA60_sh 0.35 0.66 -0.25 -0.48 -0.03 0.85 0.15 2.8 ## OZFOperc_pp -0.02 -0.18 0.70 -0.01 0.11 0.53 0.47 1.2 ## Give -0.02 0.58 0.17 0.52 0.10 0.65 0.35 2.2 ## Take 0.16 0.02 0.01 0.90 -0.05 0.83 0.17 1.1 ## hits -0.02 -0.01 0.27 -0.06 0.87 0.83 0.17 1.2 ## blks 0.19 0.63 -0.18 0.14 0.47 0.70 0.30 2.4 ## ## RC1 RC2 RC5 RC3 RC4 ## SS loadings 2.69 2.33 1.89 1.55 1.16 ## Proportion Var 0.21 0.18 0.15 0.12 0.09 ## Cumulative Var 0.21 0.39 0.53 0.65 0.74 ## Proportion Explained 0.28 0.24 0.20 0.16 0.12 ## Cumulative Proportion 0.28 0.52 0.72 0.88 1.00 ## ## Mean item complexity = 1.7 ## Test of the hypothesis that 5 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.08 ## with the empirical chi square 28.59 with prob &lt; 0.19 ## ## Fit based upon off diagonal values = 0.91 먼저 각 5개 성분에 관한 변수들의 기여도는 RC1부터 RC5까지 열로 구분되어 있습니다. RC1(성분1)은 Goals_Against와 Shots_Against 변수의 성분에 관한 기여도가 높은 양의 값이고, PP_perc와 PK_perc 변수의 기여도가 높은 음의 값입니다. RC2(성분2)은 Goals_For 변수가 높은 기여도를 가지고 있으며, RC5는 Shots_For, CF60_pp, OZFOperc_pp가 높은 기여도를 갖고 있습니다. RC3는 take 변수만 연관이 있으며, RC4는 hits 변수와 연관이 있습니다. SS loadings 제곱합으로 시작하는 표의 숫자는 각 성분의 고윳값(Eigenvalue) 입니다. 이 고윳값이 정규화되면 Proportion Explained 행의 값이며, 성분 1이 5개의 회전된 성분 모두가 설명하는 분산의 28%를 설명하는 것을 볼 수 있습니다. 경험적으로 선택된 성분들이 설명하는 분산의 총합이 최소한 전체 분산의 70%를 넘어야 하며, Cumulative Var 행의 5개 성분 합이 총 74%의 분산을 나타내고 있습니다. 7.3.4 요인 점수 생성 회전된 성분들의 기여도를 각 팀의 요인 점수로 변환합니다. pca.scores = data.frame(pca.rotate$scores) head(pca.scores) ## RC1 RC2 RC5 RC3 RC4 ## 1 -2.2153 0.002821 0.3162 -0.1572 1.52780 ## 2 0.8815 -0.569239 -1.2361 -0.2703 -0.01132 ## 3 0.1032 0.481754 1.8135 -0.1607 0.73465 ## 4 -0.0663 -0.630676 -0.2121 -1.3086 0.15413 ## 5 1.4966 1.156906 -0.3222 0.9647 -0.65648 ## 6 -0.4890 -2.119952 1.0456 2.7375 -1.37358 독립변수(png)를 데이터의 열로 불러오도록 합니다. pca.scores$ppg = train$ppg 7.3.5 회귀 분석 nhl.lm = lm(ppg ~ ., data = pca.scores) summary(nhl.lm) ## ## Call: ## lm(formula = ppg ~ ., data = pca.scores) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.16327 -0.04819 0.00372 0.03872 0.16591 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.11133 0.01575 70.55 &lt; 2e-16 *** ## RC1 -0.11220 0.01602 -7.00 0.00000031 *** ## RC2 0.07099 0.01602 4.43 0.00018 *** ## RC5 0.02295 0.01602 1.43 0.16500 ## RC3 -0.01778 0.01602 -1.11 0.27804 ## RC4 -0.00531 0.01602 -0.33 0.74300 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0863 on 24 degrees of freedom ## Multiple R-squared: 0.75, Adjusted R-squared: 0.698 ## F-statistic: 14.4 on 5 and 24 DF, p-value: 0.00000145 \\(R^2\\)가 70%에 달하며, p값이 1.446e-06로 나와 통계적으로 높은 유의성을 갖고 있습니다. 그러나 RC1과 RC2를 제외한 3개 성분은 유의하지 않은 것으로 보입니다. 두 가지 피처만을 대상으로 회귀분석을 다시 실시합니다. nhl.lm2 = lm(ppg ~ RC1 + RC2, data = pca.scores) summary(nhl.lm2) ## ## Call: ## lm(formula = ppg ~ RC1 + RC2, data = pca.scores) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.1891 -0.0443 0.0144 0.0565 0.1647 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.1113 0.0159 70.04 &lt; 2e-16 *** ## RC1 -0.1122 0.0161 -6.95 0.00000018 *** ## RC2 0.0710 0.0161 4.40 0.00015 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0869 on 27 degrees of freedom ## Multiple R-squared: 0.715, Adjusted R-squared: 0.694 ## F-statistic: 33.8 on 2 and 27 DF, p-value: 0.000000044 \\(R^2\\)가 역시나 70%에 가까우며, 통계적으로 유의한 모델입니다. plot(nhl.lm2$fitted.values, train$ppg, xlab = &#39;Predicted&#39;, ylab = &#39;Actual&#39;) sqrt(mean(nhl.lm2$residuals^2)) ## [1] 0.08244 평균 제곱 오차의 제곱근을 계산한 후 테스트 셋과 비교해보도록 합니다. test.scores = data.frame(predict(pca.rotate, test[, c(-1:-2)])) test.scores$pred = predict(nhl.lm2, test.scores) test.scores$ppg = test$ppg plot(test.scores$pred, test.scores$ppg, xlab = &#39;Predicted&#39;, ylab = &#39;Actual&#39;) resid = test.scores$ppg - test.scores$pred sqrt(mean(resid^2)) ## [1] 0.1012 "],
["군집화-분석.html", "Chapter 8 군집화 분석 8.1 K-Means (iris 데이터) 8.2 와인 데이터 분석", " Chapter 8 군집화 분석 군집화 분석의 목적은 관찰된 값을 일정 숫자의 집단으로 나누는 것입니다. 이 집단 사이에는 서로 최대한 다른 관찰값을 가지되, 한 집단에 소속된 관찰값은 최대한 비슷하도록 나누어야 합니다. 군집화 분석은 크게 두가지 방법이 쓰입니다. K-평균 군집화 기법(K-means): 원하는 군집의 개수인 k를 지정하면, 알고리즘은 각 관찰값이 k개의 군집 중 하나의 군집에만 속할 때까지 반복을 계속합니다. 해당 알고리즘의 작동 순서는 다음과 같습니다. 만들고자 하는 군집의 개수(k)를 지정합니다. 시작 평균으로 사용될 k개의 점들을 임의로 초기화합니다. 다음의 내용을 반복합니다. 각 관찰값을 가장 가까운 군집에 할당하는 방식으로 k개의 군집을 만듭니다. (군집 내 분산을 최소화) 각 군집의 중점이 새로운 평균이 됩니다. 각 군집의 중점이 더 이상 변하지 않을 때까지 이 과정을 반복합니다. 계층적 군집화 기법(hierarchical): 관찰값 사이의 비유사성 측정값을 기반으로 군집화를 합니다. 일반적으로 비유사성에는 유클리드 거리가 사용됩니다. \\[ dist(x,y) = \\sqrt{\\sum_{i=1}^n(x_i - yi)^2} \\] 8.1 K-Means (iris 데이터) 먼저 R의 기본 데이터인 iris 데이터를 통해 K-means 분석을 해보도록 합니다. 8.1.1 데이터 불러오기 및 편집 set.seed(1234) data(iris) table(iris$Species) ## ## setosa versicolor virginica ## 50 50 50 iris 데이터는 총 3가지 종류의 클래스로 구분되어 있습니다. iris_km = iris[, 1:4] iris_km = data.frame(scale(iris_km)) 비지도 학습을 위해 label 피쳐를 제거한 피처를 선택합니다. 그 후 scale() 함수를 이용해 표준화를 해주도록 합니다. library(ggplot2) library(magrittr) iris_km %&gt;% ggplot(aes(x = Petal.Length, y = Petal.Width)) + geom_point() 군집화 이전 Petal.Length와 Petal.Width를 점도표로 나타내 봅니다. iris_kmeans = kmeans(iris_km, centers = 3, iter.max = 10000) print(iris_kmeans) ## K-means clustering with 3 clusters of sizes 50, 53, 47 ## ## Cluster means: ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 -1.01119 0.85041 -1.3006 -1.2507 ## 2 -0.05005 -0.88043 0.3466 0.2806 ## 3 1.13218 0.08813 0.9928 1.0141 ## ## Clustering vector: ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 2 2 2 3 2 2 2 2 2 2 2 2 3 2 2 2 2 3 2 2 2 ## [75] 2 3 3 3 2 2 2 2 2 2 2 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3 3 3 2 3 3 3 3 ## [112] 3 3 2 2 3 3 3 3 2 3 2 3 2 3 3 2 3 3 3 3 3 3 2 2 3 3 3 2 3 3 3 2 3 3 3 2 3 ## [149] 3 2 ## ## Within cluster sum of squares by cluster: ## [1] 47.35 44.09 47.45 ## (between_SS / total_SS = 76.7 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; ## [6] &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; kmeans() 함수를 통해 군집화를 수행할 수 있으며, centers 인자를 통해 몇개의 군집으로 나눌지 선택할 수 있습니다. 1~3개 군집에 각각 50, 53개, 47개 데이터가 선택되었습니다. 이를 그림으로 나타내보도록 합니다. iris_km$cluster = as.factor(iris_kmeans$cluster) iris_km %&gt;% ggplot(aes(x = Petal.Length, y = Petal.Width)) + geom_point(aes(color = cluster)) 실제 데이터와 비교해보도록 하겠습니다. 1번 군집은 setosa, 2번 군집은 versicolor, 3번 군집은 virginica와 매칭됩니다. iris_km$culster = ifelse(iris_km$cluster == 1, &#39;setosa&#39;, ifelse(iris_km$cluster == 2, &#39;versicolor&#39;, &#39;virginica&#39;)) caret::confusionMatrix(as.factor(iris_km$culster), as.factor(iris$Species)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 39 14 ## virginica 0 11 36 ## ## Overall Statistics ## ## Accuracy : 0.833 ## 95% CI : (0.764, 0.889) ## No Information Rate : 0.333 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.75 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 1.000 0.780 0.720 ## Specificity 1.000 0.860 0.890 ## Pos Pred Value 1.000 0.736 0.766 ## Neg Pred Value 1.000 0.887 0.864 ## Prevalence 0.333 0.333 0.333 ## Detection Rate 0.333 0.260 0.240 ## Detection Prevalence 0.333 0.353 0.313 ## Balanced Accuracy 1.000 0.820 0.805 setosa는 완벽하게 구분했지만 versicolor와 virginica를 구분하는데는 오류가 있어, 약 83% 정도의 정확도를 보입니다. 8.2 와인 데이터 분석 178개 와인의 화학 조성을 나타내는 13개 변수를 통해 군집화를 하도록 하겠습니다. 8.2.1 데이터 불러오기 및 편집 library(HDclassif) data(wine) str(wine) ## &#39;data.frame&#39;: 178 obs. of 14 variables: ## $ class: int 1 1 1 1 1 1 1 1 1 1 ... ## $ V1 : num 14.2 13.2 13.2 14.4 13.2 ... ## $ V2 : num 1.71 1.78 2.36 1.95 2.59 1.76 1.87 2.15 1.64 1.35 ... ## $ V3 : num 2.43 2.14 2.67 2.5 2.87 2.45 2.45 2.61 2.17 2.27 ... ## $ V4 : num 15.6 11.2 18.6 16.8 21 15.2 14.6 17.6 14 16 ... ## $ V5 : int 127 100 101 113 118 112 96 121 97 98 ... ## $ V6 : num 2.8 2.65 2.8 3.85 2.8 3.27 2.5 2.6 2.8 2.98 ... ## $ V7 : num 3.06 2.76 3.24 3.49 2.69 3.39 2.52 2.51 2.98 3.15 ... ## $ V8 : num 0.28 0.26 0.3 0.24 0.39 0.34 0.3 0.31 0.29 0.22 ... ## $ V9 : num 2.29 1.28 2.81 2.18 1.82 1.97 1.98 1.25 1.98 1.85 ... ## $ V10 : num 5.64 4.38 5.68 7.8 4.32 6.75 5.25 5.05 5.2 7.22 ... ## $ V11 : num 1.04 1.05 1.03 0.86 1.04 1.05 1.02 1.06 1.08 1.01 ... ## $ V12 : num 3.92 3.4 3.17 3.45 2.93 2.85 3.58 3.58 2.85 3.55 ... ## $ V13 : int 1065 1050 1185 1480 735 1450 1290 1295 1045 1045 ... 각 피처는 다음과 같습니다. V1: 알콜 V2: 말산 V3: 재 V4: 재의 알칼리성 V5: 마그네슘 V6: 페놀 총량 V7: 플라보노이드 V8: 비플라보노이드성 페놀 V9: 프로안토시아닌 V10: 색의 강도 V11: 빛깔 V12: OD280/OD315 V13: 프롤린 변수의 이름을 정해준 후, 표준화를 실시합니다. 또한 비지도 학습을 위해 label인 Class는 제거해주도록 합니다. names(wine) = c(&#39;Class&#39;, &#39;Alcohol&#39;, &#39;MalicAcid&#39;, &#39;Ash&#39;, &#39;Alk_ash&#39;, &#39;magnesium&#39;, &#39;T_phenols&#39;, &#39;flavonoids&#39;, &#39;Non_flav&#39;, &#39;Proantho&#39;, &#39;C_Intensity&#39;, &#39;Hue&#39;, &#39;00280_315&#39;, &#39;Proline&#39;) df = as.data.frame(scale(wine[, -1])) 품종(class)의 분포를 살펴보도록 하겠습니다. table(wine$Class) ## ## 1 2 3 ## 59 71 48 각 품종에 골고루 분포되어 있는 모습입니다. 8.2.2 K-평균 군집화 NbClust() 함수를 이용해 최적의 군집 수를 찾을 수 있습니다. library(NbClust) numKmeans = NbClust(df, min.nc = 2, max.nc = 15, method = &#39;kmeans&#39;) ## *** : The Hubert index is a graphical method of determining the number of clusters. ## In the plot of Hubert index, we seek a significant knee that corresponds to a ## significant increase of the value of the measure i.e the significant peak in Hubert ## index second differences plot. ## ## *** : The D index is a graphical method of determining the number of clusters. ## In the plot of D index, we seek a significant knee (the significant peak in Dindex ## second differences plot) that corresponds to a significant increase of the value of ## the measure. ## ## ******************************************************************* ## * Among all indices: ## * 2 proposed 2 as the best number of clusters ## * 19 proposed 3 as the best number of clusters ## * 1 proposed 14 as the best number of clusters ## * 1 proposed 15 as the best number of clusters ## ## ***** Conclusion ***** ## ## * According to the majority rule, the best number of clusters is 3 ## ## ## ******************************************************************* 결과를 보면 3개의 군집이 최적 숫자인 것으로 판명됩니다. 해당 k를 바탕으로 kmeans() 함수를 이용해 K-평균 군집화 분석을 수행합니다. nstart 인자는 초기 임의 군집을 몇개 생성할지를 정하는 값입니다. set.seed(1234) km = kmeans(df, 3, nstart = 25) table(km$cluster) ## ## 1 2 3 ## 62 65 51 원 데이터의 class와 비교를 통해 정확도를 평가해보도록 합니다. caret::confusionMatrix(as.factor(km$cluster), as.factor(wine$Class)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 1 2 3 ## 1 59 3 0 ## 2 0 65 0 ## 3 0 3 48 ## ## Overall Statistics ## ## Accuracy : 0.966 ## 95% CI : (0.928, 0.988) ## No Information Rate : 0.399 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.949 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: 1 Class: 2 Class: 3 ## Sensitivity 1.000 0.915 1.000 ## Specificity 0.975 1.000 0.977 ## Pos Pred Value 0.952 1.000 0.941 ## Neg Pred Value 1.000 0.947 1.000 ## Prevalence 0.331 0.399 0.270 ## Detection Rate 0.331 0.365 0.270 ## Detection Prevalence 0.348 0.365 0.287 ## Balanced Accuracy 0.987 0.958 0.988 0.9663의 높은 정확도를 보입니다. 8.2.3 계층적 군집화 위와 동일하게 NbClust() 함수 내 인자를 바꾸어, 계층적 군집화 기준 최적의 군집 수를 찾도록 합니다. numComplete = NbClust(df, distance = &#39;euclidean&#39;, min.nc = 2, max.nc = 6, method = &#39;complete&#39;, index = &#39;all&#39;) ## *** : The Hubert index is a graphical method of determining the number of clusters. ## In the plot of Hubert index, we seek a significant knee that corresponds to a ## significant increase of the value of the measure i.e the significant peak in Hubert ## index second differences plot. ## ## *** : The D index is a graphical method of determining the number of clusters. ## In the plot of D index, we seek a significant knee (the significant peak in Dindex ## second differences plot) that corresponds to a significant increase of the value of ## the measure. ## ## ******************************************************************* ## * Among all indices: ## * 1 proposed 2 as the best number of clusters ## * 11 proposed 3 as the best number of clusters ## * 6 proposed 5 as the best number of clusters ## * 5 proposed 6 as the best number of clusters ## ## ***** Conclusion ***** ## ## * According to the majority rule, the best number of clusters is 3 ## ## ## ******************************************************************* 역시나 3개의 군집이 최적으로 나타납니다. 이제 3개의 군집을 사용해 거리 행렬을 계산하도록 합니다. dis = dist(df, method = &#39;euclidean&#39;) 해당 행렬을 hclust() 함수의 입력값으로 사용해 군집화를 합니다. hc = hclust(dis, method = &#39;complete&#39;) plot(hc, hang = -1, labels = FALSE) cutree() 함수를 이용해 군집을 나눈후, sparcl 패키지의 cutree() 함수를 이용하면 군집을 시각화할 수 있습니다. library(sparcl) comp3 = cutree(hc, 3) ColorDendrogram(hc, y = comp3, branchlength = 50) 각 군집 별로 색이 다르게 나타납니다. 마지막으로 원 데이터의 class와 비교를 통해 정확도를 계산해보도록 합니다. caret::confusionMatrix(as.factor(comp3), as.factor(wine$Class)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 1 2 3 ## 1 51 18 0 ## 2 8 50 0 ## 3 0 3 48 ## ## Overall Statistics ## ## Accuracy : 0.837 ## 95% CI : (0.774, 0.888) ## No Information Rate : 0.399 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.755 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: 1 Class: 2 Class: 3 ## Sensitivity 0.864 0.704 1.000 ## Specificity 0.849 0.925 0.977 ## Pos Pred Value 0.739 0.862 0.941 ## Neg Pred Value 0.927 0.825 1.000 ## Prevalence 0.331 0.399 0.270 ## Detection Rate 0.287 0.281 0.270 ## Detection Prevalence 0.388 0.326 0.287 ## Balanced Accuracy 0.857 0.815 0.988 0.8371의 정확도를 보입니다. "]
]
