<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 CART | 금융 데이터 사이언스</title>
  <meta name="description" content="Chapter 6 CART | 금융 데이터 사이언스" />
  <meta name="generator" content="bookdown 0.15 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 CART | 금융 데이터 사이언스" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 CART | 금융 데이터 사이언스" />
  
  
  

<meta name="author" content="이현열" />


<meta name="date" content="2019-11-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="knn과-svm.html"/>
<link rel="next" href="pca.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">금융 데이터 사이언스</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#사용-패키지"><i class="fa fa-check"></i>사용 패키지</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="머신러닝이란.html"><a href="머신러닝이란.html"><i class="fa fa-check"></i><b>1</b> 머신러닝이란?</a><ul>
<li class="chapter" data-level="1.1" data-path="머신러닝이란.html"><a href="머신러닝이란.html#지도학습supervised-learning"><i class="fa fa-check"></i><b>1.1</b> 지도학습(Supervised Learning)</a></li>
<li class="chapter" data-level="1.2" data-path="머신러닝이란.html"><a href="머신러닝이란.html#비지도학습unsupervised-learning"><i class="fa fa-check"></i><b>1.2</b> 비지도학습(Unsupervised Learning)</a></li>
<li class="chapter" data-level="1.3" data-path="머신러닝이란.html"><a href="머신러닝이란.html#딥러닝-강화학습reinforcement-learning"><i class="fa fa-check"></i><b>1.3</b> 딥러닝 / 강화학습(Reinforcement Learning)</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="회귀분석.html"><a href="회귀분석.html"><i class="fa fa-check"></i><b>2</b> 회귀분석</a><ul>
<li class="chapter" data-level="2.1" data-path="회귀분석.html"><a href="회귀분석.html#상관관계-이해하기"><i class="fa fa-check"></i><b>2.1</b> 상관관계 이해하기</a></li>
<li class="chapter" data-level="2.2" data-path="회귀분석.html"><a href="회귀분석.html#회귀의-이해"><i class="fa fa-check"></i><b>2.2</b> 회귀의 이해</a><ul>
<li class="chapter" data-level="2.2.1" data-path="회귀분석.html"><a href="회귀분석.html#보통-최소-제곱ols-추정"><i class="fa fa-check"></i><b>2.2.1</b> 보통 최소 제곱(OLS) 추정</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="회귀분석.html"><a href="회귀분석.html#단변량-회귀분석"><i class="fa fa-check"></i><b>2.3</b> 단변량 회귀분석</a><ul>
<li class="chapter" data-level="2.3.1" data-path="회귀분석.html"><a href="회귀분석.html#챌린저-호-데이터"><i class="fa fa-check"></i><b>2.3.1</b> 챌린저 호 데이터</a></li>
<li class="chapter" data-level="2.3.2" data-path="회귀분석.html"><a href="회귀분석.html#미국-와이오밍-주-용출량-예측"><i class="fa fa-check"></i><b>2.3.2</b> 미국 와이오밍 주 용출량 예측</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="회귀분석.html"><a href="회귀분석.html#다변량-회귀분석"><i class="fa fa-check"></i><b>2.4</b> 다변량 회귀분석</a><ul>
<li class="chapter" data-level="2.4.1" data-path="회귀분석.html"><a href="회귀분석.html#다이아몬드-데이터"><i class="fa fa-check"></i><b>2.4.1</b> 다이아몬드 데이터</a></li>
<li class="chapter" data-level="2.4.2" data-path="회귀분석.html"><a href="회귀분석.html#캘리포니아-물-가용량"><i class="fa fa-check"></i><b>2.4.2</b> 캘리포니아 물 가용량</a></li>
<li class="chapter" data-level="2.4.3" data-path="회귀분석.html"><a href="회귀분석.html#최적화를-통한-변수-선택"><i class="fa fa-check"></i><b>2.4.3</b> 최적화를 통한 변수 선택</a></li>
<li class="chapter" data-level="2.4.4" data-path="회귀분석.html"><a href="회귀분석.html#robustness-check"><i class="fa fa-check"></i><b>2.4.4</b> Robustness Check</a></li>
<li class="chapter" data-level="2.4.5" data-path="회귀분석.html"><a href="회귀분석.html#실제와-예측간의-차이"><i class="fa fa-check"></i><b>2.4.5</b> 실제와 예측간의 차이</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="회귀분석.html"><a href="회귀분석.html#다른-고려사항"><i class="fa fa-check"></i><b>2.5</b> 다른 고려사항</a><ul>
<li class="chapter" data-level="2.5.1" data-path="회귀분석.html"><a href="회귀분석.html#질적-피처"><i class="fa fa-check"></i><b>2.5.1</b> 질적 피처</a></li>
<li class="chapter" data-level="2.5.2" data-path="회귀분석.html"><a href="회귀분석.html#상호작용-항"><i class="fa fa-check"></i><b>2.5.2</b> 상호작용 항</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="로지스틱-회귀.html"><a href="로지스틱-회귀.html"><i class="fa fa-check"></i><b>3</b> 로지스틱 회귀</a><ul>
<li class="chapter" data-level="3.1" data-path="로지스틱-회귀.html"><a href="로지스틱-회귀.html#오즈비"><i class="fa fa-check"></i><b>3.1</b> 오즈비</a></li>
<li class="chapter" data-level="3.2" data-path="로지스틱-회귀.html"><a href="로지스틱-회귀.html#로지스틱-회귀-1"><i class="fa fa-check"></i><b>3.2</b> 로지스틱 회귀</a></li>
<li class="chapter" data-level="3.3" data-path="로지스틱-회귀.html"><a href="로지스틱-회귀.html#입학-데이터-분석"><i class="fa fa-check"></i><b>3.3</b> 입학 데이터 분석</a></li>
<li class="chapter" data-level="3.4" data-path="로지스틱-회귀.html"><a href="로지스틱-회귀.html#위스콘신-유방암-데이터"><i class="fa fa-check"></i><b>3.4</b> 위스콘신 유방암 데이터</a><ul>
<li class="chapter" data-level="3.4.1" data-path="로지스틱-회귀.html"><a href="로지스틱-회귀.html#데이터-불러오기-및-편집"><i class="fa fa-check"></i><b>3.4.1</b> 데이터 불러오기 및 편집</a></li>
<li class="chapter" data-level="3.4.2" data-path="로지스틱-회귀.html"><a href="로지스틱-회귀.html#데이터-나누기"><i class="fa fa-check"></i><b>3.4.2</b> 데이터 나누기</a></li>
<li class="chapter" data-level="3.4.3" data-path="로지스틱-회귀.html"><a href="로지스틱-회귀.html#모형화"><i class="fa fa-check"></i><b>3.4.3</b> 모형화</a></li>
<li class="chapter" data-level="3.4.4" data-path="로지스틱-회귀.html"><a href="로지스틱-회귀.html#테스트-셋에-적용"><i class="fa fa-check"></i><b>3.4.4</b> 테스트 셋에 적용</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="로지스틱-회귀.html"><a href="로지스틱-회귀.html#교차검증을-포함한-로지스틱-회귀"><i class="fa fa-check"></i><b>3.5</b> 교차검증을 포함한 로지스틱 회귀</a></li>
<li class="chapter" data-level="3.6" data-path="로지스틱-회귀.html"><a href="로지스틱-회귀.html#bic-기준-최적의-피처-선택"><i class="fa fa-check"></i><b>3.6</b> BIC 기준 최적의 피처 선택</a></li>
<li class="chapter" data-level="3.7" data-path="로지스틱-회귀.html"><a href="로지스틱-회귀.html#roc"><i class="fa fa-check"></i><b>3.7</b> ROC</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ridge-lasso.html"><a href="ridge-lasso.html"><i class="fa fa-check"></i><b>4</b> RIDGE &amp; LASSO</a><ul>
<li class="chapter" data-level="4.1" data-path="ridge-lasso.html"><a href="ridge-lasso.html#규제화"><i class="fa fa-check"></i><b>4.1</b> 규제화</a><ul>
<li class="chapter" data-level="4.1.1" data-path="ridge-lasso.html"><a href="ridge-lasso.html#규제화의-종류"><i class="fa fa-check"></i><b>4.1.1</b> 규제화의 종류</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ridge-lasso.html"><a href="ridge-lasso.html#전립선암-데이터-분석"><i class="fa fa-check"></i><b>4.2</b> 전립선암 데이터 분석</a><ul>
<li class="chapter" data-level="4.2.1" data-path="ridge-lasso.html"><a href="ridge-lasso.html#데이터-불러오기-및-편집-1"><i class="fa fa-check"></i><b>4.2.1</b> 데이터 불러오기 및 편집</a></li>
<li class="chapter" data-level="4.2.2" data-path="ridge-lasso.html"><a href="ridge-lasso.html#데이터-나누기-1"><i class="fa fa-check"></i><b>4.2.2</b> 데이터 나누기</a></li>
<li class="chapter" data-level="4.2.3" data-path="ridge-lasso.html"><a href="ridge-lasso.html#모형화-1"><i class="fa fa-check"></i><b>4.2.3</b> 모형화</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="knn과-svm.html"><a href="knn과-svm.html"><i class="fa fa-check"></i><b>5</b> KNN과 SVM</a><ul>
<li class="chapter" data-level="5.1" data-path="knn과-svm.html"><a href="knn과-svm.html#knn"><i class="fa fa-check"></i><b>5.1</b> KNN</a></li>
<li class="chapter" data-level="5.2" data-path="knn과-svm.html"><a href="knn과-svm.html#svm"><i class="fa fa-check"></i><b>5.2</b> SVM</a></li>
<li class="chapter" data-level="5.3" data-path="knn과-svm.html"><a href="knn과-svm.html#데이터-불러오기-및-편집-2"><i class="fa fa-check"></i><b>5.3</b> 데이터 불러오기 및 편집</a><ul>
<li class="chapter" data-level="5.3.1" data-path="knn과-svm.html"><a href="knn과-svm.html#knn-1"><i class="fa fa-check"></i><b>5.3.1</b> KNN</a></li>
<li class="chapter" data-level="5.3.2" data-path="knn과-svm.html"><a href="knn과-svm.html#svm-1"><i class="fa fa-check"></i><b>5.3.2</b> SVM</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="cart.html"><a href="cart.html"><i class="fa fa-check"></i><b>6</b> CART</a><ul>
<li class="chapter" data-level="6.1" data-path="cart.html"><a href="cart.html#의사결정나무"><i class="fa fa-check"></i><b>6.1</b> 의사결정나무</a><ul>
<li class="chapter" data-level="6.1.1" data-path="cart.html"><a href="cart.html#랜덤-포레스트"><i class="fa fa-check"></i><b>6.1.1</b> 랜덤 포레스트</a></li>
<li class="chapter" data-level="6.1.2" data-path="cart.html"><a href="cart.html#익스트림-그레디언트-부스트-기법-xgboost"><i class="fa fa-check"></i><b>6.1.2</b> 익스트림 그레디언트 부스트 기법 (XGboost)</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="cart.html"><a href="cart.html#회귀-트리"><i class="fa fa-check"></i><b>6.2</b> 회귀 트리</a><ul>
<li class="chapter" data-level="6.2.1" data-path="cart.html"><a href="cart.html#데이터-불러오기-및-편집-3"><i class="fa fa-check"></i><b>6.2.1</b> 데이터 불러오기 및 편집</a></li>
<li class="chapter" data-level="6.2.2" data-path="cart.html"><a href="cart.html#모형화-2"><i class="fa fa-check"></i><b>6.2.2</b> 모형화</a></li>
<li class="chapter" data-level="6.2.3" data-path="cart.html"><a href="cart.html#프루닝가지치기"><i class="fa fa-check"></i><b>6.2.3</b> 프루닝(가지치기)</a></li>
<li class="chapter" data-level="6.2.4" data-path="cart.html"><a href="cart.html#랜덤-포레스트-회귀-트리"><i class="fa fa-check"></i><b>6.2.4</b> 랜덤 포레스트: 회귀 트리</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="cart.html"><a href="cart.html#분류-트리"><i class="fa fa-check"></i><b>6.3</b> 분류 트리</a><ul>
<li class="chapter" data-level="6.3.1" data-path="cart.html"><a href="cart.html#데이터-불러오기-및-편집-4"><i class="fa fa-check"></i><b>6.3.1</b> 데이터 불러오기 및 편집</a></li>
<li class="chapter" data-level="6.3.2" data-path="cart.html"><a href="cart.html#랜덤-포레스트-분류-트리"><i class="fa fa-check"></i><b>6.3.2</b> 랜덤 포레스트: 분류 트리</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="cart.html"><a href="cart.html#익스트림-그레디언트-부스트-기법-xgboost-1"><i class="fa fa-check"></i><b>6.4</b> 익스트림 그레디언트 부스트 기법 (XGboost)</a><ul>
<li class="chapter" data-level="6.4.1" data-path="cart.html"><a href="cart.html#데이터-불러오기-및-편집-5"><i class="fa fa-check"></i><b>6.4.1</b> 데이터 불러오기 및 편집</a></li>
<li class="chapter" data-level="6.4.2" data-path="cart.html"><a href="cart.html#랜덤-포레스트-1"><i class="fa fa-check"></i><b>6.4.2</b> 랜덤 포레스트</a></li>
<li class="chapter" data-level="6.4.3" data-path="cart.html"><a href="cart.html#xgboost-모형-만들기"><i class="fa fa-check"></i><b>6.4.3</b> XGboost 모형 만들기</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>7</b> PCA</a><ul>
<li class="chapter" data-level="7.1" data-path="pca.html"><a href="pca.html#주성분분석pca"><i class="fa fa-check"></i><b>7.1</b> 주성분분석(PCA)</a></li>
<li class="chapter" data-level="7.2" data-path="pca.html"><a href="pca.html#iris-데이터-분석"><i class="fa fa-check"></i><b>7.2</b> iris 데이터 분석</a><ul>
<li class="chapter" data-level="7.2.1" data-path="pca.html"><a href="pca.html#데이터-불러오기"><i class="fa fa-check"></i><b>7.2.1</b> 데이터 불러오기</a></li>
<li class="chapter" data-level="7.2.2" data-path="pca.html"><a href="pca.html#모형화-3"><i class="fa fa-check"></i><b>7.2.2</b> 모형화</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="pca.html"><a href="pca.html#북미-프로-아이스하키-리그-데이터-분석"><i class="fa fa-check"></i><b>7.3</b> 북미 프로 아이스하키 리그 데이터 분석</a><ul>
<li class="chapter" data-level="7.3.1" data-path="pca.html"><a href="pca.html#데이터-불러오기-1"><i class="fa fa-check"></i><b>7.3.1</b> 데이터 불러오기</a></li>
<li class="chapter" data-level="7.3.2" data-path="pca.html"><a href="pca.html#성분-추출"><i class="fa fa-check"></i><b>7.3.2</b> 성분 추출</a></li>
<li class="chapter" data-level="7.3.3" data-path="pca.html"><a href="pca.html#직각-회전과-해석"><i class="fa fa-check"></i><b>7.3.3</b> 직각 회전과 해석</a></li>
<li class="chapter" data-level="7.3.4" data-path="pca.html"><a href="pca.html#요인-점수-생성"><i class="fa fa-check"></i><b>7.3.4</b> 요인 점수 생성</a></li>
<li class="chapter" data-level="7.3.5" data-path="pca.html"><a href="pca.html#회귀-분석"><i class="fa fa-check"></i><b>7.3.5</b> 회귀 분석</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="군집화-분석.html"><a href="군집화-분석.html"><i class="fa fa-check"></i><b>8</b> 군집화 분석</a><ul>
<li class="chapter" data-level="8.1" data-path="군집화-분석.html"><a href="군집화-분석.html#k-means-iris-데이터"><i class="fa fa-check"></i><b>8.1</b> K-Means (iris 데이터)</a><ul>
<li class="chapter" data-level="8.1.1" data-path="군집화-분석.html"><a href="군집화-분석.html#데이터-불러오기-및-편집-6"><i class="fa fa-check"></i><b>8.1.1</b> 데이터 불러오기 및 편집</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="군집화-분석.html"><a href="군집화-분석.html#와인-데이터-분석"><i class="fa fa-check"></i><b>8.2</b> 와인 데이터 분석</a><ul>
<li class="chapter" data-level="8.2.1" data-path="군집화-분석.html"><a href="군집화-분석.html#데이터-불러오기-및-편집-7"><i class="fa fa-check"></i><b>8.2.1</b> 데이터 불러오기 및 편집</a></li>
<li class="chapter" data-level="8.2.2" data-path="군집화-분석.html"><a href="군집화-분석.html#k-평균-군집화"><i class="fa fa-check"></i><b>8.2.2</b> K-평균 군집화</a></li>
<li class="chapter" data-level="8.2.3" data-path="군집화-분석.html"><a href="군집화-분석.html#계층적-군집화"><i class="fa fa-check"></i><b>8.2.3</b> 계층적 군집화</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="http://henryquant.blogspot.com/" target="blank">Henry's Quantopia</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">금융 데이터 사이언스</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="cart" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> CART</h1>
<div id="의사결정나무" class="section level2">
<h2><span class="header-section-number">6.1</span> 의사결정나무</h2>
<p>의사결정나무는 특정 항목에 대한 의사 결정 규칙을 나무 형태로 분류해 나가는 분석 기법을 말합니다. 예를 들어, 타이타닉 호 탑승자의 성별, 나이, 자녀의 수를 이용해서 생존 확률을 아래와 같이 구분해 나갑니다.</p>
<p><img src="images/cart.png" width="50%" style="display: block; margin: auto;" /></p>
<p>의사결정나무의 가장 큰 장점은 분석 과정을 실제로 눈으로 관측할 수 있으므로 직관적이고 이해하기 쉽다는 점입니다.</p>
<p>또한 수치형/범주형 변수를 모두 사용할 수 있다는 점, 계산 비용이 낮아 대규모의 데이터 셋에서도 비교적 빠르게 연산이 가능하다는 장점이 있습니다.</p>
<p>의사결정나무 분석 방법에는 통계학에 기반한 (카이스퀘어, T검정, F검정 등을 사용한) CART 및 CHAID 알고리즘이나, 기계학습 계열인(엔트로피, 정보 이득 등을 사용한) ID3, C4.5, C5.0 등의 알고리즘이 존재하며, 일반적으로 CART를 많이 사용합니다.</p>
<div id="랜덤-포레스트" class="section level3">
<h3><span class="header-section-number">6.1.1</span> 랜덤 포레스트</h3>
<p>모형의 예측력을 높이기 위해서는 많은 트리를 만들고 결과를 결합하면 되며, 이를 랜덤 포레스트라 합니다.</p>
<p><img src="images/random_forest.png" width="50%" style="display: block; margin: auto;" /></p>
<p>랜덤 포레스트 기법에서는 크게 두 가지 방법을 사용합니다.</p>
<ol style="list-style-type: decimal">
<li><p>부트스트랩(bootstrap aggregation) 혹은 배깅(bagging)은 전체 관찰값 3분의 2 정도의 데이터 집합에서 무작위로 샘플을 선정해 트리는 만듭니다.</p></li>
<li><p>개별 분할에서 입력 피처를 무작위로 선정하는 방법입니다. R 패키지에서는 기본적으로 회귀 분석 문제를 풀 때는 전체 예측 변수 수를 3으로 나눈 값(<span class="math inline">\(n / 3\)</span>)을, 분류 문제를 풀 때는 전체 예측 변수 수의 제곱근 한 값을 (<span class="math inline">\(\sqrt{n}\)</span>) 사용합니다.</p></li>
</ol>
</div>
<div id="익스트림-그레디언트-부스트-기법-xgboost" class="section level3">
<h3><span class="header-section-number">6.1.2</span> 익스트림 그레디언트 부스트 기법 (XGboost)</h3>
<p>부스트 기법은 기본 모형을 만든 후 잔차를 검사하고 손실함수에 맞춰 해당 잔차를 바탕으로 모형을 적합하게 만들며, 이는 특정 기준에 이를 때까지 계속 반복하는 방법입니다.</p>
<p>예를 들어 학생이 연습 시험을 봐서 100문제 중에 30문제를 틀린 경우 해당 30문제만 다시 공부하며, 다음 시험에서 30문제 중 10개를 틀리면 다시 10문제만 집중하는 과정입니다.</p>
</div>
</div>
<div id="회귀-트리" class="section level2">
<h2><span class="header-section-number">6.2</span> 회귀 트리</h2>
<p>먼저 CART 모형을 이용하여 회귀 트리를 만드는 법을 살펴보도록 하겠습니다.</p>
<div id="데이터-불러오기-및-편집-3" class="section level3">
<h3><span class="header-section-number">6.2.1</span> 데이터 불러오기 및 편집</h3>
<p>이전에 사용한 암-전립선암 데이터를 불러옵니다.</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb175-1" data-line-number="1"><span class="kw">library</span>(ElemStatLearn)</a>
<a class="sourceLine" id="cb175-2" data-line-number="2"></a>
<a class="sourceLine" id="cb175-3" data-line-number="3"><span class="kw">data</span>(prostate)</a>
<a class="sourceLine" id="cb175-4" data-line-number="4">prostate<span class="op">$</span>gleason =<span class="st"> </span><span class="kw">ifelse</span>(prostate<span class="op">$</span>gleason <span class="op">==</span><span class="st"> </span><span class="dv">6</span>, <span class="dv">0</span>, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb175-5" data-line-number="5">pros.train =<span class="st"> </span><span class="kw">subset</span>(prostate, train <span class="op">==</span><span class="st"> </span><span class="ot">TRUE</span>)[, <span class="dv">1</span><span class="op">:</span><span class="dv">9</span>]</a>
<a class="sourceLine" id="cb175-6" data-line-number="6">pros.test =<span class="st"> </span><span class="kw">subset</span>(prostate, train <span class="op">==</span><span class="st"> </span><span class="ot">FALSE</span>)[, <span class="dv">1</span><span class="op">:</span><span class="dv">9</span>]</a></code></pre></div>
<p>gleason이 6이면 0, 그렇지 않으면 1로 변형해주며, 트레인과 테스트 셋을 각각 나누어줍니다.</p>
</div>
<div id="모형화-2" class="section level3">
<h3><span class="header-section-number">6.2.2</span> 모형화</h3>
<p>R에서는 rpart 패키지의 <code>rpart()</code> 함수를 이용해 회귀 트리를 만들 수 있습니다.</p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb176-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb176-2" data-line-number="2"></a>
<a class="sourceLine" id="cb176-3" data-line-number="3"><span class="kw">library</span>(rpart)</a>
<a class="sourceLine" id="cb176-4" data-line-number="4"></a>
<a class="sourceLine" id="cb176-5" data-line-number="5">tree.pros =<span class="st"> </span><span class="kw">rpart</span>(lpsa <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> pros.train)</a>
<a class="sourceLine" id="cb176-6" data-line-number="6"><span class="kw">print</span>(tree.pros)</a></code></pre></div>
<pre><code>## n= 67 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 67 96.280 2.452  
##    2) lcavol&lt; 1.051 25 24.820 1.522  
##      4) lcavol&lt; -0.4786 8  5.374 0.546 *
##      5) lcavol&gt;=-0.4786 17  8.234 1.981 *
##    3) lcavol&gt;=1.051 42 36.950 3.006  
##      6) lcavol&lt; 2.792 34 21.290 2.749  
##       12) lweight&lt; 3.463 12  7.788 2.222 *
##       13) lweight&gt;=3.463 22  8.349 3.036  
##         26) age&gt;=65 15  5.694 2.891 *
##         27) age&lt; 65 7  1.660 3.347 *
##      7) lcavol&gt;=2.792 8  3.820 4.101 *</code></pre>
<p>lpsa를 나누는 기준의 트리가 생성되었습니다. 이를 보기 쉽게 그림으로 나타내보도록 하겠습니다.</p>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb178-1" data-line-number="1"><span class="kw">library</span>(rpart.plot)</a>
<a class="sourceLine" id="cb178-2" data-line-number="2"></a>
<a class="sourceLine" id="cb178-3" data-line-number="3"><span class="kw">rpart.plot</span>(tree.pros)</a></code></pre></div>
<p><img src="06-CART_files/figure-html/unnamed-chunk-6-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>해당 모형을 통한 회귀분석을 통해 MSE을 구하도록 합니다.</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb179-1" data-line-number="1"><span class="kw">library</span>(magrittr)</a>
<a class="sourceLine" id="cb179-2" data-line-number="2"></a>
<a class="sourceLine" id="cb179-3" data-line-number="3">party.test =<span class="st"> </span><span class="kw">predict</span>(tree.pros, <span class="dt">newdata =</span> pros.test)</a>
<a class="sourceLine" id="cb179-4" data-line-number="4">party.resid =<span class="st"> </span>(party.test <span class="op">-</span><span class="st"> </span>pros.test<span class="op">$</span>lpsa) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb179-5" data-line-number="5"><span class="st">  </span>.<span class="op">^</span><span class="dv">2</span> <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mean</span>()</a>
<a class="sourceLine" id="cb179-6" data-line-number="6"></a>
<a class="sourceLine" id="cb179-7" data-line-number="7"><span class="kw">print</span>(party.resid)</a></code></pre></div>
<pre><code>## [1] 0.6136</code></pre>
<p>MSE 값이 0.6136 입니다.</p>
</div>
<div id="프루닝가지치기" class="section level3">
<h3><span class="header-section-number">6.2.3</span> 프루닝(가지치기)</h3>
<p>가지수가 많아질수록 트레이닝 셋에서의 설명력은 높아지지만, 오버피팅이 발생할 수 있으므로 적절하게 가지수를 제한할 필요가 있습니다.</p>
<p><img src="images/cart_prune.png" width="50%" style="display: block; margin: auto;" /></p>
<p>먼저 cptable을 통해 모델의 각종 결과값을 확인하고 이를 통해 적절한 가지수를 찾도록 합니다.</p>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb181-1" data-line-number="1"><span class="kw">print</span>(tree.pros<span class="op">$</span>cptable)</a></code></pre></div>
<pre><code>##        CP nsplit rel error xerror    xstd
## 1 0.35852      0    1.0000 1.0196 0.17964
## 2 0.12296      1    0.6415 0.8742 0.12878
## 3 0.11640      2    0.5185 0.7949 0.10420
## 4 0.05351      3    0.4021 0.7905 0.09822
## 5 0.01033      4    0.3486 0.7044 0.09116
## 6 0.01000      5    0.3383 0.7322 0.09382</code></pre>
<p>cptable의 내용은 다음과 같다.</p>
<ul>
<li>CP: 비용 복잡도(complexity parameter)</li>
<li>nsplit: 트리의 분할 횟수</li>
<li>rel error: 분할 횟수에 다른 RSS의 값을 분할하지 않았을 때의 RSS 값으로 나눔. <span class="math inline">\(RSS(k) / RSS(0)\)</span></li>
<li>xerror: 10겹 교차검증의 평균오차</li>
<li>xstd: 10겹 교차검증의 표준편차</li>
</ul>
<p><code>plotcp()</code> 함수를 이용해 이를 그림으로 나타낼 수도 있습니다.</p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb183-1" data-line-number="1"><span class="kw">plotcp</span>(tree.pros)</a></code></pre></div>
<p><img src="06-CART_files/figure-html/unnamed-chunk-10-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>일반적으로 선 하단에 위치한 포인트 중 가장 왼쪽에 위치한 값을 선택하는 것이 좋습니다. 오른쪽으로 갈수록 Error율은 낮아지지만 가지수가 많아지기 때문입니다. 위 그림을 통해 CP가 4(3번 분할)을 선택하도록 합니다.</p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb184-1" data-line-number="1">cp =<span class="st"> </span><span class="kw">min</span>(tree.pros<span class="op">$</span>cptable[<span class="dv">4</span>, ]) <span class="co"># select cp</span></a>
<a class="sourceLine" id="cb184-2" data-line-number="2">prune.tree.pros =<span class="st"> </span><span class="kw">prune</span>(tree.pros, cp)</a></code></pre></div>
<p><code>prune()</code> 함수를 이용해 가지를 친 트리를 만들 수 있습니다.</p>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb185-1" data-line-number="1"><span class="kw">rpart.plot</span>(prune.tree.pros)</a></code></pre></div>
<p><img src="06-CART_files/figure-html/unnamed-chunk-12-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>가치지기를 통해 lcavol 변수만 남게 되었습니다.</p>
<p>해당 모형을 테스트 셋에 적용해보도록 합니다.</p>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb186-1" data-line-number="1">party.pros.test =<span class="st"> </span><span class="kw">predict</span>(prune.tree.pros, <span class="dt">newdata =</span> pros.test)</a>
<a class="sourceLine" id="cb186-2" data-line-number="2">rpart.resid =<span class="st"> </span>(party.pros.test <span class="op">-</span><span class="st"> </span>pros.test<span class="op">$</span>lpsa) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb186-3" data-line-number="3"><span class="st">  </span>.<span class="op">^</span><span class="dv">2</span> <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mean</span>()</a>
<a class="sourceLine" id="cb186-4" data-line-number="4"></a>
<a class="sourceLine" id="cb186-5" data-line-number="5"><span class="kw">print</span>(rpart.resid)</a></code></pre></div>
<pre><code>## [1] 0.5145</code></pre>
<p>MSE가 0.5145로써, 가지치기 전 대비 다소 줄어들었습니다.</p>
</div>
<div id="랜덤-포레스트-회귀-트리" class="section level3">
<h3><span class="header-section-number">6.2.4</span> 랜덤 포레스트: 회귀 트리</h3>
<p>먼저 회귀분석 트리를 대상으로 랜덤 포레스트 방법을 적용합니다. R에서는 <code>randomForest()</code> 함수를 이용해 랜덤 포레스트 모델을 만들 수 있습니다.</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb188-1" data-line-number="1"><span class="kw">library</span>(randomForest)</a>
<a class="sourceLine" id="cb188-2" data-line-number="2"></a>
<a class="sourceLine" id="cb188-3" data-line-number="3"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb188-4" data-line-number="4">rf.pros =<span class="st"> </span><span class="kw">randomForest</span>(lpsa <span class="op">~</span>., <span class="dt">data =</span> pros.train)</a>
<a class="sourceLine" id="cb188-5" data-line-number="5">rf.pros</a></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = lpsa ~ ., data = pros.train) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 2
## 
##           Mean of squared residuals: 0.6937
##                     % Var explained: 51.73</code></pre>
<p>랜덤 포레스트 기법에서는 매 분할마다 2개의 변수를 샘플링해 500개의 각기 다른 트리를 생성하며, MSE는 0.6937 입니다.</p>
<p>트리가 너무 많아지면 과적합이 발생할 수도 있으므로, rf.pros를 그림으로 그려보도록 합니다.</p>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb190-1" data-line-number="1"><span class="kw">plot</span>(rf.pros)</a></code></pre></div>
<p><img src="06-CART_files/figure-html/unnamed-chunk-15-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>트리수가 100개가 넘어가면 MSE에 큰 변화가 없는 모습입니다. <code>which.min()</code> 함수를 통해 최적의 트리를 찾아보도록 합니다.</p>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb191-1" data-line-number="1"><span class="kw">which.min</span>(rf.pros<span class="op">$</span>mse)</a></code></pre></div>
<pre><code>## [1] 80</code></pre>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb193-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb193-2" data-line-number="2">rf.pros<span class="fl">.2</span> =<span class="st"> </span><span class="kw">randomForest</span>(lpsa <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> pros.train,</a>
<a class="sourceLine" id="cb193-3" data-line-number="3">                         <span class="dt">ntree =</span> <span class="kw">which.min</span>(rf.pros<span class="op">$</span>mse))</a>
<a class="sourceLine" id="cb193-4" data-line-number="4"></a>
<a class="sourceLine" id="cb193-5" data-line-number="5">rf.pros<span class="fl">.2</span></a></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = lpsa ~ ., data = pros.train, ntree = which.min(rf.pros$mse)) 
##                Type of random forest: regression
##                      Number of trees: 80
## No. of variables tried at each split: 2
## 
##           Mean of squared residuals: 0.6567
##                     % Var explained: 54.31</code></pre>
<p>MSE가 가장 낮은 80개의 트리로 랜덤포레스트를 수행한 결과, MSE가 0.6567로 개선되었습니다.</p>
<p>다음으로 변수의 중요도를 살펴보도록 한다.</p>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb195-1" data-line-number="1"><span class="kw">varImpPlot</span>(rf.pros<span class="fl">.2</span>, <span class="dt">scale =</span> T, </a>
<a class="sourceLine" id="cb195-2" data-line-number="2">           <span class="dt">main =</span> <span class="st">&#39;Variance Importance Plot - PSA Score&#39;</span>)</a>
<a class="sourceLine" id="cb195-3" data-line-number="3"></a>
<a class="sourceLine" id="cb195-4" data-line-number="4"><span class="kw">importance</span>(rf.pros<span class="fl">.2</span>)</a></code></pre></div>
<pre><code>##         IncNodePurity
## lcavol         25.012
## lweight        15.822
## age             7.167
## lbph            5.471
## svi             8.498
## lcp             8.114
## gleason         4.990
## pgg45           6.664</code></pre>
<p><img src="06-CART_files/figure-html/unnamed-chunk-17-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>위 모델(트리 갯수를 줄인 모댈)을 테스트 셋에 적용해보도록 합니다.</p>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb197-1" data-line-number="1">rf.pros.test =<span class="st"> </span><span class="kw">predict</span>(rf.pros<span class="fl">.2</span>, <span class="dt">newdata =</span> pros.test)</a>
<a class="sourceLine" id="cb197-2" data-line-number="2">rf.resid =<span class="st"> </span>(rf.pros.test <span class="op">-</span><span class="st"> </span>pros.test<span class="op">$</span>lpsa) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb197-3" data-line-number="3"><span class="st">  </span>.<span class="op">^</span><span class="dv">2</span> <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mean</span>()</a>
<a class="sourceLine" id="cb197-4" data-line-number="4"><span class="kw">print</span>(rf.resid)</a></code></pre></div>
<pre><code>## [1] 0.5513</code></pre>
<p>MSE 값이 기존에 비해 훨씬 줄어들었습니다.</p>
<table>
<caption><span id="tab:unnamed-chunk-19">표 6.1: </span>각 모형의 MSE 비교</caption>
<thead>
<tr class="header">
<th align="center">모형</th>
<th align="center">MSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">기본 트리</td>
<td align="center">0.6136</td>
</tr>
<tr class="even">
<td align="center">가지치기</td>
<td align="center">0.5145</td>
</tr>
<tr class="odd">
<td align="center">랜덤 포레스트</td>
<td align="center">0.6937</td>
</tr>
<tr class="even">
<td align="center">랜덤 포레스트 2</td>
<td align="center">0.6567</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="분류-트리" class="section level2">
<h2><span class="header-section-number">6.3</span> 분류 트리</h2>
<p>이번에는 CART 모형을 이용하여 분류 트리를 만드는 법을 살펴보도록 하겠습니다.</p>
<div id="데이터-불러오기-및-편집-4" class="section level3">
<h3><span class="header-section-number">6.3.1</span> 데이터 불러오기 및 편집</h3>
<p>위스콘신 유방암 데이터를 불러온 후, 데이터를 나누도록 합니다.</p>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb199-1" data-line-number="1"><span class="kw">library</span>(MASS)</a>
<a class="sourceLine" id="cb199-2" data-line-number="2"></a>
<a class="sourceLine" id="cb199-3" data-line-number="3"><span class="kw">data</span>(biopsy)</a>
<a class="sourceLine" id="cb199-4" data-line-number="4"></a>
<a class="sourceLine" id="cb199-5" data-line-number="5">biopsy<span class="op">$</span>ID =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb199-6" data-line-number="6"><span class="kw">names</span>(biopsy) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;thick&#39;</span>, <span class="st">&#39;u.size&#39;</span>, <span class="st">&#39;u.shape&#39;</span>, <span class="st">&#39;adhsn&#39;</span>, <span class="st">&#39;s.size&#39;</span>,</a>
<a class="sourceLine" id="cb199-7" data-line-number="7">                  <span class="st">&#39;nucl&#39;</span>, <span class="st">&#39;chrom&#39;</span>, <span class="st">&#39;n.nuc&#39;</span>, <span class="st">&#39;mit&#39;</span>, <span class="st">&#39;class&#39;</span>)</a>
<a class="sourceLine" id="cb199-8" data-line-number="8">biopsy.v2 =<span class="st"> </span><span class="kw">na.omit</span>(biopsy)</a>
<a class="sourceLine" id="cb199-9" data-line-number="9"></a>
<a class="sourceLine" id="cb199-10" data-line-number="10"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb199-11" data-line-number="11">ind =<span class="st"> </span><span class="kw">sample</span>(<span class="dv">2</span>, <span class="kw">nrow</span>(biopsy.v2), <span class="dt">replace =</span> <span class="ot">TRUE</span>,</a>
<a class="sourceLine" id="cb199-12" data-line-number="12">             <span class="dt">prob =</span> <span class="kw">c</span>(<span class="fl">0.7</span>, <span class="fl">0.3</span>))</a>
<a class="sourceLine" id="cb199-13" data-line-number="13"></a>
<a class="sourceLine" id="cb199-14" data-line-number="14">biop.train =<span class="st"> </span>biopsy.v2[ind<span class="op">==</span><span class="dv">1</span>, ]</a>
<a class="sourceLine" id="cb199-15" data-line-number="15">biop.test =<span class="st"> </span>biopsy.v2[ind<span class="op">==</span><span class="dv">2</span>, ]</a></code></pre></div>
<p><code>rpart()</code> 함수를 이용해 트리 모형을 수행합니다. class가 benign과 malignant로 구성된 factor 형태이므로, 분류 트리가 만들어집니다.</p>
<div class="sourceCode" id="cb200"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb200-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb200-2" data-line-number="2">tree.biop =<span class="st"> </span><span class="kw">rpart</span>(class <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> biop.train)</a>
<a class="sourceLine" id="cb200-3" data-line-number="3"><span class="kw">print</span>(tree.biop)</a></code></pre></div>
<pre><code>## n= 474 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 474 172 benign (0.63713 0.36287)  
##    2) u.size&lt; 3.5 321  27 benign (0.91589 0.08411)  
##      4) nucl&lt; 4.5 294   7 benign (0.97619 0.02381) *
##      5) nucl&gt;=4.5 27   7 malignant (0.25926 0.74074)  
##       10) thick&lt; 3.5 8   3 benign (0.62500 0.37500) *
##       11) thick&gt;=3.5 19   2 malignant (0.10526 0.89474) *
##    3) u.size&gt;=3.5 153   8 malignant (0.05229 0.94771) *</code></pre>
<p>cptable을 살펴보도록 합니다.</p>
<div class="sourceCode" id="cb202"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb202-1" data-line-number="1">tree.biop<span class="op">$</span>cptable</a></code></pre></div>
<pre><code>##        CP nsplit rel error xerror    xstd
## 1 0.79651      0    1.0000 1.0000 0.06086
## 2 0.07558      1    0.2035 0.2616 0.03710
## 3 0.01163      2    0.1279 0.1512 0.02882
## 4 0.01000      3    0.1163 0.1512 0.02882</code></pre>
<div class="sourceCode" id="cb204"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb204-1" data-line-number="1"><span class="kw">plotcp</span>(tree.biop)</a></code></pre></div>
<p><img src="06-CART_files/figure-html/unnamed-chunk-22-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>cptable과 그림을 살펴본 결과, cp가 3(2번분할) 한 모델을 적용하도록 합니다.</p>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb205-1" data-line-number="1">cp =<span class="st"> </span><span class="kw">min</span>(tree.biop<span class="op">$</span>cptable[<span class="dv">3</span>, ])</a>
<a class="sourceLine" id="cb205-2" data-line-number="2">prune.tree.biop =<span class="st"> </span><span class="kw">prune</span>(tree.biop, <span class="dt">cp =</span> cp)</a>
<a class="sourceLine" id="cb205-3" data-line-number="3"><span class="kw">rpart.plot</span>(prune.tree.biop)</a></code></pre></div>
<p><img src="06-CART_files/figure-html/unnamed-chunk-23-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>트리 결과를 살펴보면 u.size가 첫 번째 분할이며, nuci가 두 번째 분할 기준입니다. 위 모델을 test 데이터에 적용해보도록 합니다.</p>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb206-1" data-line-number="1">rpart.test =<span class="st"> </span><span class="kw">predict</span>(prune.tree.biop, <span class="dt">newdata =</span> biop.test, <span class="dt">type =</span> <span class="st">&#39;class&#39;</span>)</a>
<a class="sourceLine" id="cb206-2" data-line-number="2">caret<span class="op">::</span><span class="kw">confusionMatrix</span>(rpart.test, biop.test<span class="op">$</span>class)</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##            Reference
## Prediction  benign malignant
##   benign       136         3
##   malignant      6        64
##                                       
##                Accuracy : 0.957       
##                  95% CI : (0.92, 0.98)
##     No Information Rate : 0.679       
##     P-Value [Acc &gt; NIR] : &lt;2e-16      
##                                       
##                   Kappa : 0.902       
##                                       
##  Mcnemar&#39;s Test P-Value : 0.505       
##                                       
##             Sensitivity : 0.958       
##             Specificity : 0.955       
##          Pos Pred Value : 0.978       
##          Neg Pred Value : 0.914       
##              Prevalence : 0.679       
##          Detection Rate : 0.651       
##    Detection Prevalence : 0.665       
##       Balanced Accuracy : 0.956       
##                                       
##        &#39;Positive&#39; Class : benign      
## </code></pre>
<p>두 번의 분할 트리 모형만으로도 95.69%의 정확도를 얻을 수 있습니다.</p>
<p>변수의 중요도를 살펴보도록 합니다.</p>
</div>
<div id="랜덤-포레스트-분류-트리" class="section level3">
<h3><span class="header-section-number">6.3.2</span> 랜덤 포레스트: 분류 트리</h3>
<p><code>randomForest()</code> 함수를 이용해 랜덤 포레스트 모델을 적용합니다.</p>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb208-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb208-2" data-line-number="2">rf.biop =<span class="st"> </span><span class="kw">randomForest</span>(class <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> biop.train)</a>
<a class="sourceLine" id="cb208-3" data-line-number="3">rf.biop</a></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = class ~ ., data = biop.train) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 3
## 
##         OOB estimate of  error rate: 3.38%
## Confusion matrix:
##           benign malignant class.error
## benign       294         8     0.02649
## malignant      8       164     0.04651</code></pre>
<p>OOB(out of bag) 오차율은 0.0338 로 나타납니다. 다음으로 트리 수에 따른 오차를 그린 후, err.rate가 최소가 되는 트리 갯수를 찾도록 합니다.</p>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb210-1" data-line-number="1"><span class="kw">plot</span>(rf.biop)</a></code></pre></div>
<p><img src="06-CART_files/figure-html/unnamed-chunk-26-1.png" width="50%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb211"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb211-1" data-line-number="1"><span class="kw">which.min</span>(rf.biop<span class="op">$</span>err.rate[, <span class="dv">1</span>])</a></code></pre></div>
<pre><code>## [1] 125</code></pre>
<p>해당 트리 갯수 만큼 랜덤 포레스트를 다시 만듭니다.</p>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb213-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb213-2" data-line-number="2">rf.biop<span class="fl">.2</span> =<span class="st"> </span><span class="kw">randomForest</span>(class <span class="op">~</span>., <span class="dt">data =</span> biop.train,</a>
<a class="sourceLine" id="cb213-3" data-line-number="3">                         <span class="dt">ntree =</span> <span class="kw">which.min</span>(rf.biop<span class="op">$</span>err.rate[, <span class="dv">1</span>]))</a>
<a class="sourceLine" id="cb213-4" data-line-number="4"><span class="kw">print</span>(rf.biop<span class="fl">.2</span>)</a></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = class ~ ., data = biop.train, ntree = which.min(rf.biop$err.rate[,      1])) 
##                Type of random forest: classification
##                      Number of trees: 125
## No. of variables tried at each split: 3
## 
##         OOB estimate of  error rate: 2.95%
## Confusion matrix:
##           benign malignant class.error
## benign       294         8     0.02649
## malignant      6       166     0.03488</code></pre>
<p>에러가 0로 기존에 비해 감소하였습니다. 이제 해당 모델을 테스트 셋에 적용합니다.</p>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb215-1" data-line-number="1">rf.biop.test =<span class="st"> </span><span class="kw">predict</span>(rf.biop<span class="fl">.2</span>, <span class="dt">newdata =</span> biop.test, <span class="dt">type =</span> <span class="st">&#39;class&#39;</span>)</a>
<a class="sourceLine" id="cb215-2" data-line-number="2">caret<span class="op">::</span><span class="kw">confusionMatrix</span>(rf.biop.test, biop.test<span class="op">$</span>class)</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##            Reference
## Prediction  benign malignant
##   benign       138         0
##   malignant      4        67
##                                         
##                Accuracy : 0.981         
##                  95% CI : (0.952, 0.995)
##     No Information Rate : 0.679         
##     P-Value [Acc &gt; NIR] : &lt;2e-16        
##                                         
##                   Kappa : 0.957         
##                                         
##  Mcnemar&#39;s Test P-Value : 0.134         
##                                         
##             Sensitivity : 0.972         
##             Specificity : 1.000         
##          Pos Pred Value : 1.000         
##          Neg Pred Value : 0.944         
##              Prevalence : 0.679         
##          Detection Rate : 0.660         
##    Detection Prevalence : 0.660         
##       Balanced Accuracy : 0.986         
##                                         
##        &#39;Positive&#39; Class : benign        
## </code></pre>
<p>test 데이터를 상대로 98% 이상의 정확도를 나타내며, 단일 트리에 훨씬 개선된 성과를 보입니다. 이처럼 랜덤 포레스트는 회귀 트리 보다는 분류 트리에서 더욱 뛰어난 성능 개선을 보입니다.</p>
<p>변수의 중요도를 살펴보도록 합니다.</p>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb217-1" data-line-number="1"><span class="kw">varImpPlot</span>(rf.biop<span class="fl">.2</span>, <span class="dt">scale =</span> T, </a>
<a class="sourceLine" id="cb217-2" data-line-number="2">           <span class="dt">main =</span> <span class="st">&#39;Variance Importance Plot - PSA Score&#39;</span>)</a>
<a class="sourceLine" id="cb217-3" data-line-number="3"></a>
<a class="sourceLine" id="cb217-4" data-line-number="4"><span class="kw">importance</span>(rf.biop<span class="fl">.2</span>)</a></code></pre></div>
<pre><code>##         MeanDecreaseGini
## thick             10.301
## u.size            57.177
## u.shape           46.932
## adhsn              4.425
## s.size            25.199
## nucl              34.220
## chrom             18.064
## n.nuc             21.229
## mit                1.103</code></pre>
<p><img src="06-CART_files/figure-html/unnamed-chunk-30-1.png" width="50%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="익스트림-그레디언트-부스트-기법-xgboost-1" class="section level2">
<h2><span class="header-section-number">6.4</span> 익스트림 그레디언트 부스트 기법 (XGboost)</h2>
<div id="데이터-불러오기-및-편집-5" class="section level3">
<h3><span class="header-section-number">6.4.1</span> 데이터 불러오기 및 편집</h3>
<p>먼저 피마 인디어 당뇨병 모형 데이터를 불러온 후, 트레이닝과 테스트 셋으로 나눠주도록 합니다.</p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb219-1" data-line-number="1"><span class="kw">data</span>(Pima.tr)</a>
<a class="sourceLine" id="cb219-2" data-line-number="2"><span class="kw">data</span>(Pima.te)</a>
<a class="sourceLine" id="cb219-3" data-line-number="3">pima =<span class="st"> </span><span class="kw">rbind</span>(Pima.tr, Pima.te)</a>
<a class="sourceLine" id="cb219-4" data-line-number="4"></a>
<a class="sourceLine" id="cb219-5" data-line-number="5"><span class="kw">set.seed</span>(<span class="dv">502</span>)</a>
<a class="sourceLine" id="cb219-6" data-line-number="6">ind =<span class="st"> </span><span class="kw">sample</span>(<span class="dv">2</span>, <span class="kw">nrow</span>(pima), <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">prob =</span> <span class="kw">c</span>(<span class="fl">0.7</span>, <span class="fl">0.3</span>))</a>
<a class="sourceLine" id="cb219-7" data-line-number="7">pima.train =<span class="st"> </span>pima[ind <span class="op">==</span><span class="st"> </span><span class="dv">1</span>, ]</a>
<a class="sourceLine" id="cb219-8" data-line-number="8">pima.test =<span class="st"> </span>pima[ind <span class="op">==</span><span class="st"> </span><span class="dv">2</span>, ]</a></code></pre></div>
</div>
<div id="랜덤-포레스트-1" class="section level3">
<h3><span class="header-section-number">6.4.2</span> 랜덤 포레스트</h3>
<p>먼저 비교를 위해 랜덤포레스트를 이용한 분류 모형을 만들도록 합니다.</p>
<div class="sourceCode" id="cb220"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb220-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">321</span>)</a>
<a class="sourceLine" id="cb220-2" data-line-number="2">rf.pima =<span class="st"> </span><span class="kw">randomForest</span>(type <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> pima.train)</a>
<a class="sourceLine" id="cb220-3" data-line-number="3">rf.pima</a></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = type ~ ., data = pima.train) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 2
## 
##         OOB estimate of  error rate: 20.26%
## Confusion matrix:
##      No Yes class.error
## No  235  27      0.1031
## Yes  51  72      0.4146</code></pre>
<p>20% 가량의 오분류가 발생하였습니다. 트리 크기 최적화를 통한 성능 개선여부를 살펴보도록 합니다.</p>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb222-1" data-line-number="1"><span class="kw">which.min</span>(rf.pima<span class="op">$</span>err.rate[, <span class="dv">1</span>])</a></code></pre></div>
<pre><code>## [1] 88</code></pre>
<div class="sourceCode" id="cb224"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb224-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">321</span>)</a>
<a class="sourceLine" id="cb224-2" data-line-number="2">rf.pima<span class="fl">.2</span> =<span class="st"> </span><span class="kw">randomForest</span>(type <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> pima.train,</a>
<a class="sourceLine" id="cb224-3" data-line-number="3">                         <span class="dt">ntree =</span> <span class="kw">which.min</span>(rf.pima<span class="op">$</span>err.rate[, <span class="dv">1</span>]))</a>
<a class="sourceLine" id="cb224-4" data-line-number="4"></a>
<a class="sourceLine" id="cb224-5" data-line-number="5"><span class="kw">print</span>(rf.pima<span class="fl">.2</span>)</a></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = type ~ ., data = pima.train, ntree = which.min(rf.pima$err.rate[,      1])) 
##                Type of random forest: classification
##                      Number of trees: 88
## No. of variables tried at each split: 2
## 
##         OOB estimate of  error rate: 19.74%
## Confusion matrix:
##      No Yes class.error
## No  236  26     0.09924
## Yes  50  73     0.40650</code></pre>
<p>오류율이 역시 20% 정도로써 모델이 크게 개선되지는 않습니다. 해당 모형을 테스트 셋에 적용해보도록 합니다.</p>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb226-1" data-line-number="1">rf.pima.test =<span class="st"> </span><span class="kw">predict</span>(rf.pima<span class="fl">.2</span>, <span class="dt">newdata =</span> pima.test, <span class="dt">type =</span> <span class="st">&#39;class&#39;</span>)</a>
<a class="sourceLine" id="cb226-2" data-line-number="2">caret<span class="op">::</span><span class="kw">confusionMatrix</span>(rf.pima.test, pima.test<span class="op">$</span>type)</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction No Yes
##        No  74  17
##        Yes 19  37
##                                         
##                Accuracy : 0.755         
##                  95% CI : (0.677, 0.822)
##     No Information Rate : 0.633         
##     P-Value [Acc &gt; NIR] : 0.00105       
##                                         
##                   Kappa : 0.477         
##                                         
##  Mcnemar&#39;s Test P-Value : 0.86763       
##                                         
##             Sensitivity : 0.796         
##             Specificity : 0.685         
##          Pos Pred Value : 0.813         
##          Neg Pred Value : 0.661         
##              Prevalence : 0.633         
##          Detection Rate : 0.503         
##    Detection Prevalence : 0.619         
##       Balanced Accuracy : 0.740         
##                                         
##        &#39;Positive&#39; Class : No            
## </code></pre>
<p>75% 가량의 정확도를 보입니다. 이처럼 해당 데이터에는 랜덤포레스트 기법을 적용하여도 모델이 크게 개선되지 않습니다.</p>
</div>
<div id="xgboost-모형-만들기" class="section level3">
<h3><span class="header-section-number">6.4.3</span> XGboost 모형 만들기</h3>
<p>XGboost 모형 적용을 위해, 먼저 다음과 같이 그리드를 만들도록 합니다.</p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb228-1" data-line-number="1">grid =<span class="st"> </span><span class="kw">expand.grid</span>(</a>
<a class="sourceLine" id="cb228-2" data-line-number="2">  <span class="dt">nrounds =</span> <span class="kw">c</span>(<span class="dv">75</span>, <span class="dv">100</span>),</a>
<a class="sourceLine" id="cb228-3" data-line-number="3">  <span class="dt">colsample_bytree =</span> <span class="dv">1</span>,</a>
<a class="sourceLine" id="cb228-4" data-line-number="4">  <span class="dt">min_child_weight =</span> <span class="dv">1</span>,</a>
<a class="sourceLine" id="cb228-5" data-line-number="5">  <span class="dt">eta =</span> <span class="kw">c</span>(<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="fl">0.3</span>),</a>
<a class="sourceLine" id="cb228-6" data-line-number="6">  <span class="dt">gamma =</span> <span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.25</span>),</a>
<a class="sourceLine" id="cb228-7" data-line-number="7">  <span class="dt">subsample =</span> <span class="fl">0.5</span>,</a>
<a class="sourceLine" id="cb228-8" data-line-number="8">  <span class="dt">max_depth =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb228-9" data-line-number="9">)</a></code></pre></div>
<p>위에서 입력한 인자값의 내용은 다음과 같습니다.</p>
<ul>
<li>nrounds: 최대 반복 횟수(최종 모형에서의 트리 수)</li>
<li>colsample_bytree: 트리를 생성할 때 표본 추출한 피처 수(비율로 표시), 기본값은 1(피처 수의 100%)</li>
<li>min_child_weight: 부스트되는 트리에서 최소 가중값. 기본값은 1</li>
<li>eta: 학습 속도. 해법에 관한 각 트리의 기여도를 의미. 기본값은 0.3</li>
<li>gamma: 트리에서 다른 리프 분할을 하기 위해 필요한 최소 손실 감소(minimum loss reduction)</li>
<li>subsample: 데이터 관찰값의 비율. 기본값은 1(100%)</li>
<li>max_depth: 개별 트리의 최대 깊이</li>
</ul>
<p>다음은 <code>trainControl()</code> 함수를 이용하여 인자를 지정합니다.</p>
<div class="sourceCode" id="cb229"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb229-1" data-line-number="1"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb229-2" data-line-number="2"></a>
<a class="sourceLine" id="cb229-3" data-line-number="3">cntrl =<span class="st"> </span><span class="kw">trainControl</span>(</a>
<a class="sourceLine" id="cb229-4" data-line-number="4">  <span class="dt">method =</span> <span class="st">&#39;cv&#39;</span>,</a>
<a class="sourceLine" id="cb229-5" data-line-number="5">  <span class="dt">number =</span> <span class="dv">5</span>,</a>
<a class="sourceLine" id="cb229-6" data-line-number="6">  <span class="dt">verboseIter =</span> <span class="ot">FALSE</span>, <span class="co"># TRUE 설정시 과정이 보임</span></a>
<a class="sourceLine" id="cb229-7" data-line-number="7">  <span class="dt">returnData =</span> <span class="ot">FALSE</span>,</a>
<a class="sourceLine" id="cb229-8" data-line-number="8">  <span class="dt">returnResamp =</span> <span class="st">&#39;final&#39;</span></a>
<a class="sourceLine" id="cb229-9" data-line-number="9">)</a></code></pre></div>
<p>먼저 최적화된 인자를 구하도록 합니다. trControl와 tuneGrid는 위에서 입력한 인자를 입력한다.</p>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb230-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb230-2" data-line-number="2"></a>
<a class="sourceLine" id="cb230-3" data-line-number="3">train.xgb =<span class="st"> </span><span class="kw">train</span>(</a>
<a class="sourceLine" id="cb230-4" data-line-number="4">  <span class="dt">x =</span> pima.train[, <span class="dv">1</span><span class="op">:</span><span class="dv">7</span>],</a>
<a class="sourceLine" id="cb230-5" data-line-number="5">  <span class="dt">y =</span> pima.train[, <span class="dv">8</span>],</a>
<a class="sourceLine" id="cb230-6" data-line-number="6">  <span class="dt">trControl =</span> cntrl,</a>
<a class="sourceLine" id="cb230-7" data-line-number="7">  <span class="dt">tuneGrid =</span> grid,</a>
<a class="sourceLine" id="cb230-8" data-line-number="8">  <span class="dt">method =</span> <span class="st">&#39;xgbTree&#39;</span></a>
<a class="sourceLine" id="cb230-9" data-line-number="9">)</a>
<a class="sourceLine" id="cb230-10" data-line-number="10"></a>
<a class="sourceLine" id="cb230-11" data-line-number="11"><span class="kw">print</span>(train.xgb)</a></code></pre></div>
<p><img src="images/xgboost.png" width="100%" style="display: block; margin: auto;" /></p>
<p>모형 생성을 위한 최적 인자들의 조합이 출력됩니다.</p>
<p>다음으로 <code>xgb.train()</code> 함수에서 사용할 인자 목록(param)에 위에서 출력된 값을 입력합니다.</p>
<p>그 후 데이터 프레임을 입력 피처의 행렬을 x로, 레이블을 0과 1로 변환한 값을 y로 입력한 후, x와 y를 <code>xgb.Dmatrix()</code> 함수에서 입력값으로 사용합니다.</p>
<div class="sourceCode" id="cb231"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb231-1" data-line-number="1"><span class="kw">library</span>(xgboost)</a>
<a class="sourceLine" id="cb231-2" data-line-number="2"></a>
<a class="sourceLine" id="cb231-3" data-line-number="3">param =<span class="st"> </span><span class="kw">list</span>(</a>
<a class="sourceLine" id="cb231-4" data-line-number="4">  <span class="dt">objective =</span> <span class="st">&#39;binary:logistic&#39;</span>,</a>
<a class="sourceLine" id="cb231-5" data-line-number="5">  <span class="dt">eval_metric =</span> <span class="st">&#39;error&#39;</span>,</a>
<a class="sourceLine" id="cb231-6" data-line-number="6">  <span class="dt">eta =</span> <span class="fl">0.01</span>,</a>
<a class="sourceLine" id="cb231-7" data-line-number="7">  <span class="dt">max_depth =</span> <span class="dv">2</span>,</a>
<a class="sourceLine" id="cb231-8" data-line-number="8">  <span class="dt">subsample =</span> <span class="fl">0.5</span>,</a>
<a class="sourceLine" id="cb231-9" data-line-number="9">  <span class="dt">comsample_byree =</span> <span class="dv">1</span>,</a>
<a class="sourceLine" id="cb231-10" data-line-number="10">  <span class="dt">gamma =</span> <span class="fl">0.5</span></a>
<a class="sourceLine" id="cb231-11" data-line-number="11">)</a>
<a class="sourceLine" id="cb231-12" data-line-number="12"></a>
<a class="sourceLine" id="cb231-13" data-line-number="13">x =<span class="st"> </span><span class="kw">as.matrix</span>(pima.train[, <span class="dv">1</span><span class="op">:</span><span class="dv">7</span>])</a>
<a class="sourceLine" id="cb231-14" data-line-number="14">y =<span class="st"> </span><span class="kw">ifelse</span>(pima.train<span class="op">$</span>type <span class="op">==</span><span class="st"> &#39;Yes&#39;</span>, <span class="dv">1</span>, <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb231-15" data-line-number="15">train.mat =<span class="st"> </span><span class="kw">xgb.DMatrix</span>(<span class="dt">data =</span> x, <span class="dt">label =</span> y)</a></code></pre></div>
<p>다음으로 모형을 만들도록 합니다. param 인자에는 위에서 입력한 param을, data 인자에는 위에서 만든 train.mat을 입력합니다.</p>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb232-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb232-2" data-line-number="2">xgb.fit =<span class="st"> </span><span class="kw">xgb.train</span>(<span class="dt">params =</span> param, <span class="dt">data =</span> train.mat, <span class="dt">nrounds =</span> <span class="dv">75</span>)</a></code></pre></div>
<p><code>xgb.importance()</code> 함수를 이용해 변수의 중요도를 살펴보록 합니다.</p>
<div class="sourceCode" id="cb233"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb233-1" data-line-number="1">impMatrix =<span class="st"> </span><span class="kw">xgb.importance</span>(<span class="dt">feature_names =</span> <span class="kw">dimnames</span>(x)[[<span class="dv">2</span>]],</a>
<a class="sourceLine" id="cb233-2" data-line-number="2">                           <span class="dt">model =</span> xgb.fit)</a>
<a class="sourceLine" id="cb233-3" data-line-number="3"></a>
<a class="sourceLine" id="cb233-4" data-line-number="4"><span class="kw">print</span>(impMatrix)</a></code></pre></div>
<pre><code>##    Feature    Gain   Cover Frequency
## 1:     glu 0.67995 0.52640   0.41553
## 2:     age 0.14210 0.17592   0.19178
## 3:     ped 0.08877 0.14562   0.18721
## 4:     bmi 0.03496 0.04465   0.06393
## 5:   npreg 0.03104 0.05888   0.06849
## 6:      bp 0.01182 0.02899   0.03196
## 7:    skin 0.01136 0.01954   0.04110</code></pre>
<div class="sourceCode" id="cb235"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb235-1" data-line-number="1"><span class="kw">xgb.plot.importance</span>(impMatrix, <span class="dt">main =</span> <span class="st">&#39;Gain by Feature&#39;</span>)</a></code></pre></div>
<p><img src="06-CART_files/figure-html/unnamed-chunk-41-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>다음으로 수행 결과를 살펴보도록 합니다.</p>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb236-1" data-line-number="1">pred =<span class="st"> </span><span class="kw">predict</span>(xgb.fit, x)</a>
<a class="sourceLine" id="cb236-2" data-line-number="2">pred.bi =<span class="st"> </span><span class="kw">ifelse</span>(pred <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb236-3" data-line-number="3"></a>
<a class="sourceLine" id="cb236-4" data-line-number="4">caret<span class="op">::</span><span class="kw">confusionMatrix</span>(<span class="kw">as.factor</span>(y), <span class="kw">as.factor</span>(pred.bi))</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 243  19
##          1  49  74
##                                        
##                Accuracy : 0.823        
##                  95% CI : (0.782, 0.86)
##     No Information Rate : 0.758        
##     P-Value [Acc &gt; NIR] : 0.001319     
##                                        
##                   Kappa : 0.566        
##                                        
##  Mcnemar&#39;s Test P-Value : 0.000437     
##                                        
##             Sensitivity : 0.832        
##             Specificity : 0.796        
##          Pos Pred Value : 0.927        
##          Neg Pred Value : 0.602        
##              Prevalence : 0.758        
##          Detection Rate : 0.631        
##    Detection Prevalence : 0.681        
##       Balanced Accuracy : 0.814        
##                                        
##        &#39;Positive&#39; Class : 0            
## </code></pre>
<p>82%의 정확도를 보입니다.</p>
<div class="sourceCode" id="cb238"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb238-1" data-line-number="1"><span class="kw">library</span>(InformationValue)</a>
<a class="sourceLine" id="cb238-2" data-line-number="2"></a>
<a class="sourceLine" id="cb238-3" data-line-number="3">optim =<span class="st"> </span><span class="kw">optimalCutoff</span>(y, pred)</a>
<a class="sourceLine" id="cb238-4" data-line-number="4"><span class="kw">print</span>(optim)</a></code></pre></div>
<pre><code>## [1] 0.4342</code></pre>
<p><code>optimalCutoff()</code> 함수를 사용하여 로지스틱 함수에서 최적의 cut off 지점을 찾을 수도 있습니다. 즉 0.5를 기준으로 분류하는 것이 아닌, 0.4342을 기준으로 분류할 때 더욱 뛰어난 성과를 보입니다.</p>
<p>해당 모형을 테스트 셋에 적용해봅니다.</p>
<div class="sourceCode" id="cb240"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb240-1" data-line-number="1">pima.testMat =<span class="st"> </span><span class="kw">as.matrix</span>(pima.test[, <span class="dv">1</span><span class="op">:</span><span class="dv">7</span>])</a>
<a class="sourceLine" id="cb240-2" data-line-number="2">xgb.pima.test =<span class="st"> </span><span class="kw">predict</span>(xgb.fit, pima.testMat)</a>
<a class="sourceLine" id="cb240-3" data-line-number="3">y.test =<span class="st"> </span><span class="kw">ifelse</span>(pima.test<span class="op">$</span>type <span class="op">==</span><span class="st"> &#39;Yes&#39;</span>, <span class="dv">1</span>, <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb240-4" data-line-number="4"><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">misClassError</span>(y.test, xgb.pima.test, <span class="dt">threshold =</span> optim)</a></code></pre></div>
<pre><code>## [1] 0.7279</code></pre>
<p>테스트 데이터에 해당 모델을 적용한 결과는 위와 같다. cut off 지점을 0.4342으로 하였을 경우 27% 가량의 오차가 발생하여, 73% 가량의 정확도를 보입니다.</p>
<div class="sourceCode" id="cb242"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb242-1" data-line-number="1"><span class="kw">plotROC</span>(y.test, xgb.pima.test)</a></code></pre></div>
<p><img src="06-CART_files/figure-html/unnamed-chunk-45-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p><code>plotROC()</code> 함수를 이용해 ROC 및 AUC를 확인해보면, AUC가 0.8 정도로 계산됩니다.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="knn과-svm.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="pca.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
